<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    
    
    
    <title>Data mining and Machine Learning</title>
    
        
            <script src="/usr/share/javascript/mathjax/tex-mml-chtml.js" type="text/javascript"></script>
        
    <!-- mine -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.tailwindcss.com?plugins=forms,typography,aspect-ratio,line-clamp"></script>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.css" integrity="sha256-jf6kOo6UQszq3sNgQ0z/3u/IYGjd9MnxN8N0TkTIEc=" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/prismjs/prism.js" integrity="sha256-8Z8V2JjKxFQcx5e5M5vWKjkvzwE5nkUjYjYxD8pWfE4=" crossorigin="anonymous"></script>

</head>
<body>
    <div class="prose prose-sm prose-img:rounded-xl prose-img:max-h-48 prose-img:max-w-xs !container w-full px-4 mx-auto">

                
                    <header id="title-block-header" class="!my-36">
                <h1 class="title">Data mining and Machine Learning</h1>
                            
                            
                            </header>
            
                    <nav class="!mb-36" id="TOC" role="doc-toc">
                                <ul>
                                <li><a href="#data-mining-and-machine-learning"><span class="toc-section-number">1</span> Data Mining and Machine Learning</a>
                                <ul>
                                <li><a href="#data-mining-and-knowledge-discovery"><span class="toc-section-number">1.1</span> Data Mining and Knowledge Discovery</a></li>
                                <li><a href="#business-intelligence-process"><span class="toc-section-number">1.2</span> Business Intelligence Process</a></li>
                                <li><a href="#statistical-and-machine-learning-process"><span class="toc-section-number">1.3</span> Statistical and Machine Learning Process</a></li>
                                <li><a href="#types-of-data-and-patterns"><span class="toc-section-number">1.4</span> Types of Data and Patterns</a>
                                <ul>
                                <li><a href="#patterns-that-can-be-mined"><span class="toc-section-number">1.4.1</span> Patterns that can be mined</a></li>
                                </ul></li>
                                <li><a href="#evaluation-of-results"><span class="toc-section-number">1.5</span> Evaluation of results</a></li>
                                <li><a href="#classification-and-label-prediction"><span class="toc-section-number">1.6</span> Classification and label prediction</a></li>
                                <li><a href="#unsupervised-learning"><span class="toc-section-number">1.7</span> Unsupervised learning</a></li>
                                <li><a href="#outlier-analysis"><span class="toc-section-number">1.8</span> Outlier analysis</a></li>
                                <li><a href="#graph-mining-and-information-network-analysis"><span class="toc-section-number">1.9</span> Graph Mining and Information network analysis</a></li>
                                <li><a href="#post-processing-phase"><span class="toc-section-number">1.10</span> Post-processing phase</a></li>
                                <li><a href="#data-mining-and-multiple-disciplines"><span class="toc-section-number">1.11</span> Data Mining and multiple disciplines</a></li>
                                <li><a href="#high-dimensionality-and-high-complexity-of-data"><span class="toc-section-number">1.12</span> High-dimensionality and high-complexity of data</a></li>
                                <li><a href="#major-issues-in-data-mining"><span class="toc-section-number">1.13</span> Major issues in data mining</a></li>
                                </ul></li>
                                <li><a href="#data"><span class="toc-section-number">2</span> Data</a>
                                <ul>
                                <li><a href="#important-characteristics-of-structured-data"><span class="toc-section-number">2.1</span> Important Characteristics of Structured Data:</a></li>
                                <li><a href="#types-of-attributes"><span class="toc-section-number">2.2</span> Types of attributes</a></li>
                                <li><a href="#basic-statistical-descriptions-of-data"><span class="toc-section-number">2.3</span> Basic Statistical Descriptions of Data</a></li>
                                <li><a href="#measuring-the-central-tendency"><span class="toc-section-number">2.4</span> Measuring the central tendency</a>
                                <ul>
                                <li><a href="#median"><span class="toc-section-number">2.4.1</span> median</a></li>
                                <li><a href="#mode"><span class="toc-section-number">2.4.2</span> mode</a></li>
                                <li><a href="#usage"><span class="toc-section-number">2.4.3</span> usage</a></li>
                                </ul></li>
                                <li><a href="#measuring-the-dispersion-of-data"><span class="toc-section-number">2.5</span> Measuring the Dispersion of Data</a>
                                <ul>
                                <li><a href="#quartiles"><span class="toc-section-number">2.5.1</span> Quartiles</a></li>
                                <li><a href="#boxplot-analysis"><span class="toc-section-number">2.5.2</span> Boxplot analysis</a></li>
                                <li><a href="#standard-deviation"><span class="toc-section-number">2.5.3</span> standard deviation</a></li>
                                </ul></li>
                                <li><a href="#graphic-displays-of-basic-statistical-descriptions"><span class="toc-section-number">2.6</span> Graphic displays of basic statistical descriptions</a>
                                <ul>
                                <li><a href="#histogram-analysis"><span class="toc-section-number">2.6.1</span> Histogram Analysis</a></li>
                                <li><a href="#quantile-plot"><span class="toc-section-number">2.6.2</span> Quantile plot</a></li>
                                <li><a href="#quantile-quantile-q-q-plot"><span class="toc-section-number">2.6.3</span> Quantile-Quantile (Q-Q) Plot</a></li>
                                <li><a href="#scatter-plot"><span class="toc-section-number">2.6.4</span> Scatter Plot</a></li>
                                </ul></li>
                                <li><a href="#data-visualization"><span class="toc-section-number">2.7</span> Data Visualization</a>
                                <ul>
                                <li><a href="#categorization-of-visualization-methods"><span class="toc-section-number">2.7.1</span> Categorization of visualization methods:</a></li>
                                <li><a href="#pixel-oriented-visualization-techniques"><span class="toc-section-number">2.7.2</span> Pixel-Oriented Visualization Techniques</a></li>
                                <li><a href="#geometric-projection-visualization-techniques"><span class="toc-section-number">2.7.3</span> Geometric Projection Visualization Techniques</a></li>
                                <li><a href="#scatterplot-and-scatterplot-matrices"><span class="toc-section-number">2.7.4</span> Scatterplot and scatterplot matrices</a></li>
                                </ul></li>
                                <li><a href="#parallel-coordinates"><span class="toc-section-number">2.8</span> Parallel Coordinates</a>
                                <ul>
                                <li><a href="#icon-based-visualization-techniques"><span class="toc-section-number">2.8.1</span> Icon-based Visualization Techniques</a></li>
                                <li><a href="#hierarchical-visualization-techniques"><span class="toc-section-number">2.8.2</span> Hierarchical Visualization Techniques</a></li>
                                </ul></li>
                                <li><a href="#measuring-data-similarity-and-dissimilarity"><span class="toc-section-number">2.9</span> Measuring Data Similarity and Dissimilarity</a>
                                <ul>
                                <li><a href="#from-the-data-matrix"><span class="toc-section-number">2.9.1</span> From the data matrix…</a></li>
                                <li><a href="#to-the-dissimilarity-matrix"><span class="toc-section-number">2.9.2</span> …to the dissimilarity matrix</a></li>
                                <li><a href="#dissimilarity-matrix-for-nominal-attributes"><span class="toc-section-number">2.9.3</span> dissimilarity matrix for nominal attributes</a></li>
                                <li><a href="#distance-measure-for-symmetric-binary-variables"><span class="toc-section-number">2.9.4</span> Distance measure for symmetric binary variables</a></li>
                                <li><a href="#distance-measure-for-asymmetric-binary-variables"><span class="toc-section-number">2.9.5</span> Distance measure for asymmetric binary variables:</a></li>
                                <li><a href="#jaccard-coefficient"><span class="toc-section-number">2.9.6</span> Jaccard coefficient</a></li>
                                </ul></li>
                                <li><a href="#standardizing-numeric-data"><span class="toc-section-number">2.10</span> Standardizing Numeric Data</a>
                                <ul>
                                <li><a href="#euclidean-distance-l2-norm-is-defined-like"><span class="toc-section-number">2.10.1</span> Euclidean distance (L2 norm) is defined like:</a></li>
                                <li><a href="#the-z-score"><span class="toc-section-number">2.10.2</span> The z-score</a></li>
                                <li><a href="#mean-absolute-deviation"><span class="toc-section-number">2.10.3</span> mean absolute deviation</a></li>
                                <li><a href="#minkowski-distance"><span class="toc-section-number">2.10.4</span> Minkowski distance</a></li>
                                <li><a href="#ordinal-variables"><span class="toc-section-number">2.10.5</span> Ordinal variables</a></li>
                                <li><a href="#attributes-of-mixed-type"><span class="toc-section-number">2.10.6</span> Attributes of Mixed Type</a></li>
                                <li><a href="#cosine-similarity"><span class="toc-section-number">2.10.7</span> Cosine similarity</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#preprocessing"><span class="toc-section-number">3</span> Preprocessing</a>
                                <ul>
                                <li><a href="#data-cleaning"><span class="toc-section-number">3.1</span> Data Cleaning</a>
                                <ul>
                                <li><a href="#missing-data"><span class="toc-section-number">3.1.1</span> Missing data</a></li>
                                <li><a href="#noise"><span class="toc-section-number">3.1.2</span> Noise</a></li>
                                <li><a href="#smoothing-filters"><span class="toc-section-number">3.1.3</span> Smoothing filters</a></li>
                                <li><a href="#binning"><span class="toc-section-number">3.1.4</span> Binning</a></li>
                                <li><a href="#other-smoothing-techniques"><span class="toc-section-number">3.1.5</span> Other smoothing techniques</a></li>
                                </ul></li>
                                <li><a href="#data-cleaning-as-a-process"><span class="toc-section-number">3.2</span> Data cleaning as a process</a>
                                <ul>
                                <li><a href="#data-migration-and-integration"><span class="toc-section-number">3.2.1</span> Data migration and integration</a></li>
                                </ul></li>
                                <li><a href="#data-integration"><span class="toc-section-number">3.3</span> Data Integration</a>
                                <ul>
                                <li><a href="#correlation-analysis"><span class="toc-section-number">3.3.1</span> Correlation Analysis</a></li>
                                <li><a href="#correlation-coefficient"><span class="toc-section-number">3.3.2</span> Correlation coefficient</a></li>
                                </ul></li>
                                <li><a href="#data-reduction"><span class="toc-section-number">3.4</span> Data Reduction</a>
                                <ul>
                                <li><a href="#dimensionality-reduction"><span class="toc-section-number">3.4.1</span> Dimensionality Reduction</a></li>
                                <li><a href="#principal-component-analysis-pca"><span class="toc-section-number">3.4.2</span> Principal Component Analysis (PCA)</a></li>
                                </ul></li>
                                <li><a href="#attribute-subset-selection"><span class="toc-section-number">3.5</span> Attribute subset selection</a>
                                <ul>
                                <li><a href="#mutual-information"><span class="toc-section-number">3.5.1</span> Mutual information</a></li>
                                <li><a href="#ni-normalized-mi-between-f_i-and-f_s"><span class="toc-section-number">3.5.2</span> <span class="math inline">\(NI\)</span> := Normalized MI between <span class="math inline">\(f_i\)</span> and <span class="math inline">\(f_s\)</span>:</a></li>
                                <li><a href="#information-entropy"><span class="toc-section-number">3.5.3</span> Information entropy</a></li>
                                <li><a href="#attribute-creation-feature-generation"><span class="toc-section-number">3.5.4</span> Attribute creation (feature generation)</a></li>
                                <li><a href="#numerosity-reduction"><span class="toc-section-number">3.5.5</span> Numerosity Reduction</a></li>
                                <li><a href="#regression-analysis"><span class="toc-section-number">3.5.6</span> Regression analysis</a></li>
                                <li><a href="#histogram-analysis-1"><span class="toc-section-number">3.5.7</span> Histogram analysis</a></li>
                                <li><a href="#clustering"><span class="toc-section-number">3.5.8</span> Clustering</a></li>
                                <li><a href="#sampling"><span class="toc-section-number">3.5.9</span> Sampling</a></li>
                                <li><a href="#data-cube-aggregation"><span class="toc-section-number">3.5.10</span> Data Cube Aggregation</a></li>
                                </ul></li>
                                <li><a href="#data-compression"><span class="toc-section-number">3.6</span> Data Compression</a></li>
                                <li><a href="#data-transformation-and-data-discretization"><span class="toc-section-number">3.7</span> Data Transformation and Data Discretization</a>
                                <ul>
                                <li><a href="#min-max-normalization"><span class="toc-section-number">3.7.1</span> Min-max normalization</a></li>
                                <li><a href="#z-score-normalization"><span class="toc-section-number">3.7.2</span> Z-score normalization</a></li>
                                <li><a href="#normalization-by-decimal-scaling"><span class="toc-section-number">3.7.3</span> Normalization by decimal scaling</a></li>
                                <li><a href="#discretization"><span class="toc-section-number">3.7.4</span> Discretization</a></li>
                                <li><a href="#chimerge-discretization"><span class="toc-section-number">3.7.5</span> ChiMerge Discretization</a></li>
                                <li><a href="#concept-hierarchy-generation"><span class="toc-section-number">3.7.6</span> Concept Hierarchy Generation</a></li>
                                <li><a href="#automatic-concept-hierarchy-generation"><span class="toc-section-number">3.7.7</span> Automatic Concept Hierarchy Generation</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#association-rule"><span class="toc-section-number">4</span> Association Rule</a>
                                <ul>
                                <li><a href="#mining-frequent-patterns"><span class="toc-section-number">4.1</span> Mining Frequent Patterns</a>
                                <ul>
                                <li><a href="#basic-concepts"><span class="toc-section-number">4.1.1</span> Basic concepts</a></li>
                                <li><a href="#purpose-of-frequent-pattern-analysis"><span class="toc-section-number">4.1.2</span> Purpose of Frequent Pattern Analysis</a></li>
                                </ul></li>
                                <li><a href="#frequent-pattern-matching"><span class="toc-section-number">4.2</span> Frequent Pattern Matching</a>
                                <ul>
                                <li><a href="#transactions"><span class="toc-section-number">4.2.1</span> Transactions</a></li>
                                <li><a href="#association-rule-1"><span class="toc-section-number">4.2.2</span> Association Rule</a></li>
                                <li><a href="#implementation"><span class="toc-section-number">4.2.3</span> Implementation</a></li>
                                </ul></li>
                                <li><a href="#apriori-algorithm"><span class="toc-section-number">4.3</span> Apriori Algorithm</a>
                                <ul>
                                <li><a href="#apriori-pruning-principle"><span class="toc-section-number">4.3.1</span> Apriori Pruning Principle</a></li>
                                <li><a href="#pseudo-code"><span class="toc-section-number">4.3.2</span> Pseudo Code</a></li>
                                <li><a href="#how-is-the-apriori-property-used-in-the-algorithm"><span class="toc-section-number">4.3.3</span> How is the Apriori property used in the algorithm?</a></li>
                                <li><a href="#summary"><span class="toc-section-number">4.3.4</span> Summary</a></li>
                                <li><a href="#improving-the-efficiency-of-apriori"><span class="toc-section-number">4.3.5</span> Improving the efficiency of Apriori</a></li>
                                </ul></li>
                                <li><a href="#fpgrowth-approach-mining-frequent-patterns-without-candidate-generation"><span class="toc-section-number">4.4</span> FPGrowth Approach: Mining Frequent Patterns Without Candidate Generation</a>
                                <ul>
                                <li><a href="#fp-tree-approach"><span class="toc-section-number">4.4.1</span> FP-Tree Approach</a></li>
                                <li><a href="#conditional-fp-tree"><span class="toc-section-number">4.4.2</span> Conditional FP-Tree</a></li>
                                <li><a href="#advantages-of-fp-growth"><span class="toc-section-number">4.4.3</span> <strong>Advantages</strong> of FP-Growth:</a></li>
                                <li><a href="#disadvantages-of-fp-growth"><span class="toc-section-number">4.4.4</span> <strong>Disadvantages</strong> of FP-Growth:</a></li>
                                <li><a href="#algorithm-1"><span class="toc-section-number">4.4.5</span> Algorithm</a></li>
                                <li><a href="#benefits-for-the-fp-tree-structure"><span class="toc-section-number">4.4.6</span> <strong>Benefits</strong> for the FP-Tree Structure:</a></li>
                                </ul></li>
                                <li><a href="#the-frequent-pattern-growth-mining-method"><span class="toc-section-number">4.5</span> <strong>The Frequent Pattern Growth Mining Method</strong></a>
                                <ul>
                                <li><a href="#comparison"><span class="toc-section-number">4.5.1</span> Comparison</a></li>
                                <li><a href="#divide-and-conquer"><span class="toc-section-number">4.5.2</span> Divide-and-conquer:</a></li>
                                </ul></li>
                                <li><a href="#eclat-frequent-pattern-mining-with-vertical-data-format"><span class="toc-section-number">4.6</span> ECLAT: Frequent Pattern Mining with Vertical Data Format</a></li>
                                <li><a href="#pattern-evaluation-methods"><span class="toc-section-number">4.7</span> Pattern Evaluation Methods</a>
                                <ul>
                                <li><a href="#lift"><span class="toc-section-number">4.7.1</span> <strong>Lift</strong></a></li>
                                </ul></li>
                                <li><a href="#measures-used-for-comparison"><span class="toc-section-number">4.8</span> Measures used for comparison</a></li>
                                <li><a href="#is-lift-a-good-measure-for-correlation"><span class="toc-section-number">4.9</span> Is <strong>Lift</strong> a good measure for correlation?</a>
                                <ul>
                                <li><a href="#imbalance-ratio-ir"><span class="toc-section-number">4.9.1</span> Imbalance Ratio (IR)</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#classification"><span class="toc-section-number">5</span> Classification</a>
                                <ul>
                                <li><a href="#difference-between-classification-and-clustering"><span class="toc-section-number">5.0.1</span> Difference Between Classification and Clustering</a></li>
                                <li><a href="#preliminary-step-preprocessing"><span class="toc-section-number">5.0.2</span> Preliminary Step: Preprocessing</a></li>
                                <li><a href="#comparing-classification-methods"><span class="toc-section-number">5.1</span> Comparing Classification Methods</a>
                                <ul>
                                <li><a href="#decision-tree"><span class="toc-section-number">5.1.1</span> Decision Tree</a></li>
                                <li><a href="#comparing-attribute-selection-measures"><span class="toc-section-number">5.1.2</span> Comparing Attribute Selection Measures</a></li>
                                </ul></li>
                                <li><a href="#overfitting-and-tree-pruning"><span class="toc-section-number">5.2</span> Overfitting and Tree Pruning</a>
                                <ul>
                                <li><a href="#overfitting-prevention"><span class="toc-section-number">5.2.1</span> Overfitting Prevention</a></li>
                                </ul></li>
                                <li><a href="#enhancements-to-decision-tree-and-challanges"><span class="toc-section-number">5.3</span> Enhancements to Decision Tree and Challanges</a>
                                <ul>
                                <li><a href="#rainforest"><span class="toc-section-number">5.3.1</span> RainForest</a></li>
                                <li><a href="#boat-bootstrapped-optimistic-algorithm-for-tree-construction"><span class="toc-section-number">5.3.2</span> BOAT (Bootstrapped Optimistic Algorithm for Tree Construction)</a></li>
                                </ul></li>
                                <li><a href="#bayes-classification"><span class="toc-section-number">5.4</span> Bayes Classification</a>
                                <ul>
                                <li><a href="#avoiding-the-zero-probability-problem"><span class="toc-section-number">5.4.1</span> Avoiding the Zero-Probability Problem</a></li>
                                <li><a href="#training-a-bayesian-network"><span class="toc-section-number">5.4.2</span> Training a Bayesian Network</a></li>
                                </ul></li>
                                <li><a href="#rule-based-classification"><span class="toc-section-number">5.5</span> Rule-based Classification</a>
                                <ul>
                                <li><a href="#rule-extraction-from-a-decision-tree-pruning-the-rules"><span class="toc-section-number">5.5.1</span> Rule Extraction from a Decision Tree: Pruning the Rules</a></li>
                                </ul></li>
                                <li><a href="#classification-by-using-frequent-patterns"><span class="toc-section-number">5.6</span> Classification by Using Frequent Patterns</a></li>
                                <li><a href="#lazy-learners"><span class="toc-section-number">5.7</span> Lazy Learners</a>
                                <ul>
                                <li><a href="#k-nearest-neighbor-algorithm"><span class="toc-section-number">5.7.1</span> K-Nearest Neighbor Algorithm</a></li>
                                <li><a href="#case-based-reasoning-cbr"><span class="toc-section-number">5.7.2</span> Case-Based Reasoning (CBR)</a></li>
                                </ul></li>
                                <li><a href="#model-evaluation-and-selection"><span class="toc-section-number">5.8</span> Model Evaluation and Selection</a>
                                <ul>
                                <li><a href="#confusion-matrix"><span class="toc-section-number">5.8.1</span> Confusion Matrix</a></li>
                                <li><a href="#holdout-cross-validation-methods"><span class="toc-section-number">5.8.2</span> Holdout &amp; Cross-Validation Methods</a></li>
                                <li><a href="#bootstrap"><span class="toc-section-number">5.8.3</span> Bootstrap</a></li>
                                <li><a href="#estimating-confidence-intervals"><span class="toc-section-number">5.8.4</span> Estimating Confidence Intervals</a></li>
                                <li><a href="#roc-curve"><span class="toc-section-number">5.8.5</span> ROC Curve</a></li>
                                </ul></li>
                                <li><a href="#techniques-to-improve-classification-accuracy-ensemble-methods"><span class="toc-section-number">5.9</span> Techniques to Improve Classification Accuracy: Ensemble Methods</a>
                                <ul>
                                <li><a href="#bagging-bootstrap-aggregation"><span class="toc-section-number">5.9.1</span> Bagging (Bootstrap Aggregation)</a></li>
                                <li><a href="#boosting"><span class="toc-section-number">5.9.2</span> Boosting</a></li>
                                <li><a href="#random-forest"><span class="toc-section-number">5.9.3</span> Random Forest</a></li>
                                </ul></li>
                                <li><a href="#classification-of-class-imbalanced-data-sets"><span class="toc-section-number">5.10</span> Classification of Class-Imbalanced Data Sets</a></li>
                                </ul></li>
                                <li><a href="#cluster-analysis"><span class="toc-section-number">6</span> Cluster Analysis</a>
                                <ul>
                                <li><a href="#typical-applications"><span class="toc-section-number">6.1</span> Typical applications:</a>
                                <ul>
                                <li><a href="#possible-applications-are"><span class="toc-section-number">6.1.1</span> Possible applications are:</a></li>
                                <li><a href="#we-can-use-clustering-as-a-preprocessing-tool"><span class="toc-section-number">6.1.2</span> We can use clustering as a preprocessing tool:</a></li>
                                <li><a href="#clustering-evalutation"><span class="toc-section-number">6.1.3</span> Clustering evalutation</a></li>
                                <li><a href="#considerations-for-cluster-analysis"><span class="toc-section-number">6.1.4</span> Considerations for Cluster Analysis</a></li>
                                </ul></li>
                                <li><a href="#major-clustering-approaches-are"><span class="toc-section-number">6.2</span> Major clustering approaches are:</a>
                                <ul>
                                <li><a href="#recap"><span class="toc-section-number">6.2.1</span> Recap</a></li>
                                <li><a href="#we-have-other-approaches"><span class="toc-section-number">6.2.2</span> We have other approaches:</a></li>
                                </ul></li>
                                <li><a href="#assessing-clustering-tendency"><span class="toc-section-number">6.3</span> Assessing Clustering Tendency</a></li>
                                <li><a href="#hopkins-statistic"><span class="toc-section-number">6.4</span> Hopkins statistic</a>
                                <ul>
                                <li><a href="#algorithm-2"><span class="toc-section-number">6.4.1</span> Algorithm</a></li>
                                <li><a href="#considerations-3"><span class="toc-section-number">6.4.2</span> Considerations</a></li>
                                </ul></li>
                                <li><a href="#partitioning-methods"><span class="toc-section-number">6.5</span> Partitioning Methods</a>
                                <ul>
                                <li><a href="#fixed-k"><span class="toc-section-number">6.5.1</span> Fixed <span class="math inline">\(k\)</span></a></li>
                                <li><a href="#k-means-clustering-method"><span class="toc-section-number">6.5.2</span> K-means clustering method</a></li>
                                <li><a href="#k-medoids"><span class="toc-section-number">6.5.3</span> K-medoids</a></li>
                                <li><a href="#different-cases"><span class="toc-section-number">6.5.4</span> 4 different cases</a></li>
                                <li><a href="#clara"><span class="toc-section-number">6.5.5</span> CLARA</a></li>
                                <li><a href="#clarans"><span class="toc-section-number">6.5.6</span> CLARANS</a></li>
                                <li><a href="#differences-between-clara-and-clarans"><span class="toc-section-number">6.5.7</span> Differences between <code>CLARA</code> and <code>CLARANS</code></a></li>
                                <li><a href="#conclusions-on-k-medois-methods"><span class="toc-section-number">6.5.8</span> Conclusions on K-medois methods</a></li>
                                </ul></li>
                                <li><a href="#hierarchical-methods"><span class="toc-section-number">6.6</span> Hierarchical methods</a>
                                <ul>
                                <li><a href="#algorithms"><span class="toc-section-number">6.6.1</span> Algorithms</a></li>
                                <li><a href="#agnes-agglomerative-nesting"><span class="toc-section-number">6.6.2</span> AGNES (Agglomerative Nesting)</a></li>
                                <li><a href="#diana-divisive-analysis"><span class="toc-section-number">6.6.3</span> DIANA (Divisive Analysis)</a></li>
                                <li><a href="#birch-balanced-iterative-reducing-and-clustering-using-hierarchies"><span class="toc-section-number">6.6.4</span> BIRCH: Balanced Iterative Reducing and Clustering Using Hierarchies</a></li>
                                <li><a href="#chameleon"><span class="toc-section-number">6.6.5</span> CHAMELEON</a></li>
                                </ul></li>
                                <li><a href="#density-based-methods"><span class="toc-section-number">6.7</span> Density-based methods</a>
                                <ul>
                                <li><a href="#dbscan"><span class="toc-section-number">6.7.1</span> DBSCAN</a></li>
                                <li><a href="#optics-ordering-points-to-identify-the-clustering-structure"><span class="toc-section-number">6.7.2</span> OPTICS: Ordering Points To Identify the Clustering Structure</a></li>
                                <li><a href="#denclue-using-statistical-density-functions"><span class="toc-section-number">6.7.3</span> DENCLUE: Using Statistical Density Functions</a></li>
                                </ul></li>
                                <li><a href="#grid-based-clustering-method"><span class="toc-section-number">6.8</span> Grid-Based Clustering Method</a>
                                <ul>
                                <li><a href="#sting"><span class="toc-section-number">6.8.1</span> STING</a></li>
                                <li><a href="#clique-clustering-in-quest"><span class="toc-section-number">6.8.2</span> CLIQUE (Clustering In QUEst)</a></li>
                                </ul></li>
                                <li><a href="#evaluation-of-clustering"><span class="toc-section-number">6.9</span> Evaluation of Clustering</a>
                                <ul>
                                <li><a href="#determining-the-number-of-clusters"><span class="toc-section-number">6.9.1</span> Determining the number of clusters</a></li>
                                <li><a href="#measuring-clustering-quality"><span class="toc-section-number">6.9.2</span> Measuring Clustering Quality</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#clustering-of-high-dimensional-data"><span class="toc-section-number">7</span> Clustering of high-dimensional data</a>
                                <ul>
                                <li><a href="#the-curse-of-dimensionality"><span class="toc-section-number">7.1</span> The curse of dimensionality</a>
                                <ul>
                                <li><a href="#subspace-clustering"><span class="toc-section-number">7.1.1</span> Subspace Clustering</a></li>
                                </ul></li>
                                <li><a href="#subspace-clustering-methods"><span class="toc-section-number">7.2</span> Subspace Clustering Methods</a>
                                <ul>
                                <li><a href="#subspace-search-methods"><span class="toc-section-number">7.2.1</span> Subspace Search Methods</a></li>
                                <li><a href="#correlation-based-methods"><span class="toc-section-number">7.2.2</span> Correlation-Based Methods</a></li>
                                <li><a href="#bi-clustering-methods"><span class="toc-section-number">7.2.3</span> Bi-Clustering Methods</a></li>
                                </ul></li>
                                <li><a href="#bi-clustering-methods-1"><span class="toc-section-number">7.3</span> Bi-Clustering Methods</a>
                                <ul>
                                <li><a href="#bi-clustering-for-micro-array-data-analysis"><span class="toc-section-number">7.3.1</span> Bi-Clustering for Micro-Array Data Analysis</a></li>
                                <li><a href="#dimensionality-reduction-methods"><span class="toc-section-number">7.3.2</span> Dimensionality-Reduction Methods</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#clustering-with-constraints"><span class="toc-section-number">8</span> Clustering with Constraints</a>
                                <ul>
                                <li><a href="#categorization-of-constraints"><span class="toc-section-number">8.1</span> Categorization of Constraints</a>
                                <ul>
                                <li><a href="#constraints-on-instances"><span class="toc-section-number">8.1.1</span> <strong>Constraints on instances</strong>:</a></li>
                                <li><a href="#constraints-on-clusters"><span class="toc-section-number">8.1.2</span> <strong>Constraints on clusters</strong>:</a></li>
                                <li><a href="#constraints-on-similarity-measurements"><span class="toc-section-number">8.1.3</span> <strong>Constraints on similarity measurements</strong>:</a></li>
                                <li><a href="#hard-vs.-soft-constraints"><span class="toc-section-number">8.1.4</span> <strong>Hard vs. soft constraints</strong>:</a></li>
                                </ul></li>
                                <li><a href="#clustering-with-constraints-1"><span class="toc-section-number">8.2</span> Clustering with constraints</a>
                                <ul>
                                <li><a href="#enforcing-constraints"><span class="toc-section-number">8.2.1</span> Enforcing Constraints:</a></li>
                                <li><a href="#how-can-we-measure-the-quality-and-the-usefulness-of-a-set-of-constraints"><span class="toc-section-number">8.2.2</span> How can we measure the quality and the usefulness of a set of constraints?</a></li>
                                <li><a href="#handling-hard-constraints"><span class="toc-section-number">8.2.3</span> <strong>Handling hard constraints</strong>:</a></li>
                                <li><a href="#handling-soft-constraints"><span class="toc-section-number">8.2.4</span> <strong>Handling Soft Constraints</strong></a></li>
                                </ul></li>
                                <li><a href="#speeding-up-constrained-clustering"><span class="toc-section-number">8.3</span> Speeding Up Constrained Clustering</a></li>
                                </ul></li>
                                <li><a href="#outlier-analysis-1"><span class="toc-section-number">9</span> Outlier Analysis</a>
                                <ul>
                                <li><a href="#types-of-outliers"><span class="toc-section-number">9.1</span> Types of Outliers</a>
                                <ul>
                                <li><a href="#global-outlier-or-point-anomaly"><span class="toc-section-number">9.1.1</span> <strong>Global outlier</strong> (or point anomaly):</a></li>
                                <li><a href="#contextual-outlier-or-conditional-outlier"><span class="toc-section-number">9.1.2</span> <strong>Contextual outlier</strong> (or conditional outlier)</a></li>
                                <li><a href="#collective-outliers"><span class="toc-section-number">9.1.3</span> <strong>Collective Outliers</strong></a></li>
                                </ul></li>
                                <li><a href="#challenges-of-outlier-detection"><span class="toc-section-number">9.2</span> Challenges of Outlier Detection</a></li>
                                <li><a href="#outlier-detection"><span class="toc-section-number">9.3</span> Outlier Detection</a>
                                <ul>
                                <li><a href="#supervised-methods"><span class="toc-section-number">9.3.1</span> <strong>Supervised Methods</strong></a></li>
                                <li><a href="#unsupervised-methods"><span class="toc-section-number">9.3.2</span> <strong>Unsupervised Methods</strong></a></li>
                                <li><a href="#semi-supervised-methods"><span class="toc-section-number">9.3.3</span> <strong>Semi-supervised methods</strong></a></li>
                                <li><a href="#statistical-techniques-also-known-as-model-based-methods"><span class="toc-section-number">9.3.4</span> <strong>Statistical techniques</strong> (also known as model-based methods)</a></li>
                                <li><a href="#proximity-based-methods"><span class="toc-section-number">9.3.5</span> <strong>Proximity-based methods</strong></a></li>
                                <li><a href="#clustering-based-methods"><span class="toc-section-number">9.3.6</span> <strong>Clustering-based methods</strong></a></li>
                                </ul></li>
                                <li><a href="#statistical-approaches"><span class="toc-section-number">9.4</span> Statistical approaches</a>
                                <ul>
                                <li><a href="#univariate-data-dataset-involving-only-one-attribute-or-variable"><span class="toc-section-number">9.4.1</span> <strong>Univariate data</strong> (dataset involving only one attribute or variable)</a></li>
                                <li><a href="#multivariate-outliers"><span class="toc-section-number">9.4.2</span> <strong>Multivariate outliers</strong></a></li>
                                <li><a href="#non-parametric-methods"><span class="toc-section-number">9.4.3</span> <strong>Non-parametric methods</strong></a></li>
                                </ul></li>
                                <li><a href="#proximity-based-approaches"><span class="toc-section-number">9.5</span> Proximity-Based Approaches</a>
                                <ul>
                                <li><a href="#distance-based-outlier-detection"><span class="toc-section-number">9.5.1</span> Distance-Based Outlier Detection</a></li>
                                <li><a href="#density-based-outlier-detection"><span class="toc-section-number">9.5.2</span> Density-based Outlier Detection</a></li>
                                </ul></li>
                                <li><a href="#clustering-based-outlier-detection"><span class="toc-section-number">9.6</span> <strong>Clustering-Based Outlier Detection</strong></a>
                                <ul>
                                <li><a href="#strengths-2"><span class="toc-section-number">9.6.1</span> <strong>Strengths</strong></a></li>
                                <li><a href="#weaknesses"><span class="toc-section-number">9.6.2</span> <strong>Weaknesses</strong></a></li>
                                </ul></li>
                                <li><a href="#classification-approaches"><span class="toc-section-number">9.7</span> <strong>Classification Approaches</strong></a>
                                <ul>
                                <li><a href="#one-class-model"><span class="toc-section-number">9.7.1</span> <strong>One-class model</strong></a></li>
                                <li><a href="#semi-supervised-learning"><span class="toc-section-number">9.7.2</span> <strong>Semi-supervised learning</strong></a></li>
                                </ul></li>
                                <li><a href="#mining-contextual-and-collective-outliers"><span class="toc-section-number">9.8</span> <strong>Mining Contextual and Collective Outliers</strong></a>
                                <ul>
                                <li><a href="#transformation-into-conventional-outlier-detection"><span class="toc-section-number">9.8.1</span> Transformation into Conventional Outlier Detection</a></li>
                                <li><a href="#modeling-normal-behavior-considering-contexts"><span class="toc-section-number">9.8.2</span> Modeling Normal Behavior considering Contexts</a></li>
                                <li><a href="#structured-objects"><span class="toc-section-number">9.8.3</span> Structured Objects</a></li>
                                </ul></li>
                                <li><a href="#outlier-detection-in-high-dimensional-data"><span class="toc-section-number">9.9</span> Outlier Detection in <strong>High Dimensional Data</strong></a>
                                <ul>
                                <li><a href="#challenges-1"><span class="toc-section-number">9.9.1</span> Challenges</a></li>
                                <li><a href="#approaches-to-cope-with-the-curse-of-dimensionality"><span class="toc-section-number">9.9.2</span> <strong>Approaches to cope with the curse of dimensionality</strong></a></li>
                                <li><a href="#finding-outliers-in-subspaces"><span class="toc-section-number">9.9.3</span> Finding Outliers in Subspaces</a></li>
                                <li><a href="#modelling-high-dimensional-outliers"><span class="toc-section-number">9.9.4</span> Modelling High-Dimensional Outliers</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#clustering-graphs-and-network-data"><span class="toc-section-number">10</span> Clustering Graphs and Network Data</a>
                                <ul>
                                <li><a href="#network-introduction"><span class="toc-section-number">10.0.1</span> Network Introduction</a></li>
                                <li><a href="#how-can-we-cluster-network-data"><span class="toc-section-number">10.0.2</span> How Can We Cluster Network Data?</a></li>
                                <li><a href="#similarity-measures"><span class="toc-section-number">10.1</span> Similarity Measures</a>
                                <ul>
                                <li><a href="#geodesic-distance"><span class="toc-section-number">10.1.1</span> Geodesic Distance</a></li>
                                <li><a href="#similarity-in-social-networks-introduction"><span class="toc-section-number">10.1.2</span> Similarity in Social Networks Introduction</a></li>
                                <li><a href="#similarity-rank-based-on-structural-context"><span class="toc-section-number">10.1.3</span> Similarity Rank Based on Structural Context</a></li>
                                <li><a href="#similarity-rank-based-on-random-walk"><span class="toc-section-number">10.1.4</span> Similarity Rank Based on Random Walk</a></li>
                                </ul></li>
                                <li><a href="#clustering-in-a-graph"><span class="toc-section-number">10.2</span> Clustering in a Graph</a>
                                <ul>
                                <li><a href="#sparsest-cut"><span class="toc-section-number">10.2.1</span> Sparsest Cut</a></li>
                                <li><a href="#how-to-perform-the-graph-clustering"><span class="toc-section-number">10.2.2</span> How to Perform the Graph Clustering</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#advanced-frequent-pattern-analysis"><span class="toc-section-number">11</span> Advanced Frequent Pattern Analysis</a>
                                <ul>
                                <li><a href="#pattern-mining-in-multi-level-multi-dimensional-space"><span class="toc-section-number">11.1</span> Pattern Mining in Multi-Level, Multi-Dimensional Space</a>
                                <ul>
                                <li><a href="#mining-multiple-level-association-rules"><span class="toc-section-number">11.1.1</span> Mining Multiple-Level Association Rules</a></li>
                                <li><a href="#mining-multi-dimensional-association"><span class="toc-section-number">11.1.2</span> Mining Multi-Dimensional Association</a></li>
                                <li><a href="#mining-quantitative-association"><span class="toc-section-number">11.1.3</span> Mining Quantitative Association</a></li>
                                <li><a href="#mining-rare-patterns-and-negative-patterns"><span class="toc-section-number">11.1.4</span> Mining Rare Patterns and Negative Patterns</a></li>
                                </ul></li>
                                <li><a href="#constraint-based-frequent-pattern-mining"><span class="toc-section-number">11.2</span> Constraint-Based Frequent Pattern Mining</a>
                                <ul>
                                <li><a href="#constraints-in-data-mining"><span class="toc-section-number">11.2.1</span> Constraints in data Mining</a></li>
                                <li><a href="#meta-rule-guided-mining"><span class="toc-section-number">11.2.2</span> Meta-Rule Guided Mining</a></li>
                                <li><a href="#constraint-based-frequent-pattern-mining-1"><span class="toc-section-number">11.2.3</span> Constraint-Based Frequent Pattern Mining</a></li>
                                <li><a href="#convertible-constraints-ordering-data-in-transactions"><span class="toc-section-number">11.2.4</span> <strong>Convertible Constraints: Ordering Data in Transactions</strong></a></li>
                                <li><a href="#data-space-pruning-with-data-anti-monotonicity"><span class="toc-section-number">11.2.5</span> <strong>Data Space Pruning with Data Anti-monotonicity</strong></a></li>
                                </ul></li>
                                <li><a href="#mining-high-dimensional-data-and-colossal-patterns"><span class="toc-section-number">11.3</span> <strong>Mining High-Dimensional Data and Colossal Patterns</strong></a>
                                <ul>
                                <li><a href="#mining-colossal-patterns-motivation-and-philosophy"><span class="toc-section-number">11.3.1</span> Mining Colossal Patterns: Motivation and Philosophy</a></li>
                                <li><a href="#alas-a-show-of-colossal-pattern-mining"><span class="toc-section-number">11.3.2</span> Alas, A Show of Colossal Pattern Mining!</a></li>
                                <li><a href="#pattern-fusion-strategy"><span class="toc-section-number">11.3.3</span> <strong>Pattern-Fusion Strategy</strong></a></li>
                                <li><a href="#idea-of-pattern-fusion-algorithm"><span class="toc-section-number">11.3.4</span> Idea of Pattern-Fusion Algorithm</a></li>
                                </ul></li>
                                <li><a href="#mining-compressed-or-approximate-patterns"><span class="toc-section-number">11.4</span> <strong>Mining Compressed or Approximate Patterns</strong></a>
                                <ul>
                                <li><a href="#redundancy-award-top-k-patterns"><span class="toc-section-number">11.4.1</span> Redundancy-Award Top-k Patterns</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#sequential-pattern-mining"><span class="toc-section-number">12</span> Sequential Pattern Mining</a>
                                <ul>
                                <li><a href="#web-usage-mining"><span class="toc-section-number">12.1</span> Web Usage Mining</a></li>
                                <li><a href="#sequential-pattern-mining-1"><span class="toc-section-number">12.2</span> Sequential Pattern Mining</a>
                                <ul>
                                <li><a href="#problem-definition"><span class="toc-section-number">12.2.1</span> Problem definition:</a></li>
                                <li><a href="#lexicographic-order"><span class="toc-section-number">12.2.2</span> <strong>Lexicographic Order</strong></a></li>
                                </ul></li>
                                <li><a href="#the-algorithms"><span class="toc-section-number">12.3</span> <strong>The Algorithms</strong></a>
                                <ul>
                                <li><a href="#sort-phase"><span class="toc-section-number">12.3.1</span> <strong>Sort Phase</strong></a></li>
                                <li><a href="#litemset-large-itemset-phase"><span class="toc-section-number">12.3.2</span> <strong>Litemset (Large Itemset) Phase</strong></a></li>
                                <li><a href="#transformation-phase"><span class="toc-section-number">12.3.3</span> <strong>Transformation Phase</strong></a></li>
                                <li><a href="#sequence-phase"><span class="toc-section-number">12.3.4</span> <strong>Sequence Phase</strong></a></li>
                                <li><a href="#maximal-phase"><span class="toc-section-number">12.3.5</span> <strong>Maximal Phase</strong></a></li>
                                </ul></li>
                                <li><a href="#aprioriall-count-all"><span class="toc-section-number">12.4</span> <strong>AprioriAll</strong> (Count-All)</a></li>
                                <li><a href="#apriorisome"><span class="toc-section-number">12.5</span> <strong>AprioriSome</strong></a>
                                <ul>
                                <li><a href="#forward-phase"><span class="toc-section-number">12.5.1</span> <strong>Forward Phase</strong></a></li>
                                <li><a href="#backward-phase"><span class="toc-section-number">12.5.2</span> <strong>Backward Phase</strong></a></li>
                                </ul></li>
                                <li><a href="#aprioridynamicsome"><span class="toc-section-number">12.6</span> <strong>AprioriDynamicSome</strong></a>
                                <ul>
                                <li><a href="#initialization-phase"><span class="toc-section-number">12.6.1</span> <strong>Initialization phase</strong></a></li>
                                <li><a href="#forward-phase-1"><span class="toc-section-number">12.6.2</span> <strong>Forward phase</strong></a></li>
                                <li><a href="#backward-phase-1"><span class="toc-section-number">12.6.3</span> <strong>Backward phase</strong></a></li>
                                <li><a href="#why-do-we-need-otf-generate"><span class="toc-section-number">12.6.4</span> Why do we need otf-generate?</a></li>
                                <li><a href="#execution-of-aprioridynamicsome"><span class="toc-section-number">12.6.5</span> Execution of AprioriDynamicSome</a></li>
                                </ul></li>
                                <li><a href="#performance-comparison"><span class="toc-section-number">12.7</span> <strong>Performance Comparison</strong></a>
                                <ul>
                                <li><a href="#bottlenecks-of-apriori-like-methods"><span class="toc-section-number">12.7.1</span> <strong>Bottlenecks of Apriori-like methods</strong></a></li>
                                </ul></li>
                                <li><a href="#freespan-fp-growth-for-sequential-pattern-mining"><span class="toc-section-number">12.8</span> <strong>FreeSpan: FP-growth for sequential pattern mining</strong></a>
                                <ul>
                                <li><a href="#alternative-level-projection"><span class="toc-section-number">12.8.1</span> Alternative-level Projection</a></li>
                                <li><a href="#example-of-database-projection"><span class="toc-section-number">12.8.2</span> <strong>Example of Database Projection</strong></a></li>
                                <li><a href="#mining-by-level-by-level-projected-databases"><span class="toc-section-number">12.8.3</span> Mining by <strong>Level by Level</strong> Projected Databases</a></li>
                                <li><a href="#mining-by-alternative-level-projected-databases"><span class="toc-section-number">12.8.4</span> Mining by <strong>Alternative Level</strong> Projected Databases</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#data-stream-analysis"><span class="toc-section-number">13</span> Data Stream Analysis</a>
                                <ul>
                                <li><a href="#challenges-of-stream-mining"><span class="toc-section-number">13.1</span> Challenges of stream mining</a></li>
                                <li><a href="#motivation"><span class="toc-section-number">13.2</span> Motivation</a></li>
                                <li><a href="#computational-model"><span class="toc-section-number">13.3</span> Computational Model</a>
                                <ul>
                                <li><a href="#stream-processing-requirements"><span class="toc-section-number">13.3.1</span> Stream processing requirements</a></li>
                                </ul></li>
                                <li><a href="#algorithms-1"><span class="toc-section-number">13.4</span> <strong>Algorithms</strong></a>
                                <ul>
                                <li><a href="#concept-drift"><span class="toc-section-number">13.4.1</span> <strong>Concept Drift</strong></a></li>
                                <li><a href="#data-structures"><span class="toc-section-number">13.4.2</span> <strong>Data Structures</strong></a></li>
                                <li><a href="#the-window-model"><span class="toc-section-number">13.4.3</span> <strong>The Window Model</strong></a></li>
                                </ul></li>
                                <li><a href="#data-stream-clustering"><span class="toc-section-number">13.5</span> <strong>Data Stream Clustering</strong></a>
                                <ul>
                                <li><a href="#adaptive-streaming-k-means"><span class="toc-section-number">13.5.1</span> <strong>Adaptive Streaming k-Means</strong></a></li>
                                <li><a href="#mudi-stream"><span class="toc-section-number">13.5.2</span> <strong>MuDi Stream</strong></a></li>
                                <li><a href="#cedas"><span class="toc-section-number">13.5.3</span> <strong>CEDAS</strong></a></li>
                                <li><a href="#improved-data-stream-clustering-algorithm"><span class="toc-section-number">13.5.4</span> <strong>Improved Data Stream Clustering Algorithm</strong></a></li>
                                <li><a href="#dbiecm"><span class="toc-section-number">13.5.5</span> <strong>DBIECM</strong></a></li>
                                <li><a href="#i-hastream-in-my-notes-marcelloni-said-that-we-overlooked-it"><span class="toc-section-number">13.5.6</span> <strong>I-HASTREAM</strong> (IN MY NOTES MARCELLONI SAID THAT WE OVERLOOKED IT)</a></li>
                                <li><a href="#comparison-1"><span class="toc-section-number">13.5.7</span> <strong>Comparison</strong></a></li>
                                </ul></li>
                                <li><a href="#classification-1"><span class="toc-section-number">13.6</span> <strong>Classification</strong></a>
                                <ul>
                                <li><a href="#vfdt-very-fast-decision-tree"><span class="toc-section-number">13.6.1</span> <strong>VFDT</strong> (Very Fast Decision Tree)</a></li>
                                <li><a href="#cvfdt-concept-adapting-very-fast-decision-tree-learner"><span class="toc-section-number">13.6.2</span> <strong>CVFDT</strong> (Concept adapting Very Fast Decision Tree learner)</a></li>
                                </ul></li>
                                </ul></li>
                                <li><a href="#mapreduce-and-hadoop"><span class="toc-section-number">14</span> MapReduce and Hadoop</a>
                                <ul>
                                <li><a href="#mapreduce"><span class="toc-section-number">14.1</span> MapReduce</a>
                                <ul>
                                <li><a href="#divide-and-conquer-1"><span class="toc-section-number">14.1.1</span> Divide and Conquer</a></li>
                                <li><a href="#programming-model"><span class="toc-section-number">14.1.2</span> <strong>Programming Model</strong></a></li>
                                </ul></li>
                                <li><a href="#hadoop"><span class="toc-section-number">14.2</span> <strong>Hadoop</strong></a>
                                <ul>
                                <li><a href="#hadoop-distributed-file-system"><span class="toc-section-number">14.2.1</span> <strong>Hadoop Distributed File System</strong></a></li>
                                <li><a href="#core-hadoop-components-mapreduce"><span class="toc-section-number">14.2.2</span> <strong>Core Hadoop Components: MapReduce</strong></a></li>
                                <li><a href="#physical-architecture"><span class="toc-section-number">14.2.3</span> <strong>Physical Architecture</strong></a></li>
                                <li><a href="#hadoop-limitations"><span class="toc-section-number">14.2.4</span> <strong>Hadoop Limitations</strong></a></li>
                                <li><a href="#mapreduce-execution"><span class="toc-section-number">14.2.5</span> <strong>MapReduce execution</strong></a></li>
                                <li><a href="#parallel-k-means"><span class="toc-section-number">14.2.6</span> <strong>Parallel K-Means</strong></a></li>
                                <li><a href="#parallel-fp-growth-pfp"><span class="toc-section-number">14.2.7</span> <strong>Parallel FP-Growth</strong> (PFP)</a></li>
                                </ul></li>
                                </ul></li>
                                </ul>
            </nav>
                
        <div class="prose-h1:mt-36">
            <h1 data-number="1" id="data-mining-and-machine-learning"><span class="header-section-number">1</span> Data Mining and Machine Learning</h1>
            <p>We have an explosive growth of data, now we talk about petabytes and a nowadays problem is how to store this data and analyze them.</p>
            <p>Most of the algorithms we have today were proposed 40 years ago but today we have a lot of data and the possibility to store and analyze this data, thanks to storage and computational capabilities. This is why today AI is becoming fundamental.</p>
            <p>This data comes from different sources and when we analyze them we want to produce value.</p>
            <h2 data-number="1.1" id="data-mining-and-knowledge-discovery"><span class="header-section-number">1.1</span> Data Mining and Knowledge Discovery</h2>
            <p>Data Mining, from a meaning point of view, is the extraction of interesting (non-trivial, implicit, previously unknown and potentially useful) patterns or knowledge from huge amount of data. Alternatives name are knowledge discovery (mining) in databases (KDD), knowledge extraction, data/pattern analysis, data archeology, data dredging, information harvesting, business intelligence, etc.</p>
            <h2 data-number="1.2" id="business-intelligence-process"><span class="header-section-number">1.2</span> Business Intelligence Process</h2>
            <p>A typical data mining process that can be found in Business Intelligence (analysis in a company) is this one:</p>
            <p><img src="../media/image1.png" /></p>
            <p>We take data from different sources and we preprocess data to remove noise, missing values and we integrate different sources creating the dataset. Results depend on the preprocessing phase.</p>
            <p>We do statistical summary, querying and reporting and then start the real data mining process.</p>
            <p>We have to visualize the result in a way that are understandable for everyone.</p>
            <p>All knowledge extracted are used by management to perform strategical decisions.</p>
            <h2 data-number="1.3" id="statistical-and-machine-learning-process"><span class="header-section-number">1.3</span> Statistical and Machine Learning Process</h2>
            <p>From a statistical and machine learning point of view this is the process:</p>
            <p><img src="../media/image2.png" /></p>
            <p>The decision on choosing the right algorithm depend on the specific application, we have to identify the best algorithm for the particular context.</p>
            <p>We have to define some metrics to give the possibilities to the designer to identify the best solution. We have to compare the results produced by our data mining algorithms and we need a metric to do that.</p>
            <p>When we talk about data we talk about heterogeneous data and we have different types of data that are typically stored in different repositories.</p>
            <p>Most of the data mining algorithms are based on the concept of distance. But if objects are described by numerical or nominal attributes is hard to use distance.</p>
            <h2 data-number="1.4" id="types-of-data-and-patterns"><span class="header-section-number">1.4</span> Types of Data and Patterns</h2>
            <p>We have different kind of knowledge that can be mined, it depends on the specific domain.</p>
            <p>What kind of data we can mine? Data stored in databases (relational databases, data warehouse or transactional databases), data streams and sensor data, time series data, temporal data, sequence data, structure data, graphs, social networks, multi linked data, the www, text databases, multimedia databases and so on.</p>
            <h3 data-number="1.4.1" id="patterns-that-can-be-mined"><span class="header-section-number">1.4.1</span> Patterns that can be mined</h3>
            <p>We have frequent pattern analysis, for example items frequently purchased together by an high number of customers. In this case, the dataset to analyze is the transactional dataset, the bill with product’s purchases. The patter in this case is a subset of products.</p>
            <p>Once we mine the frequent pattern we can produce association rules, that are used to find correlations between data sets. They are ideally used to explain patterns in data from seemingly independent information repositories. We observe that an high number of customers bought together Diaper and Beer, this is a frequent pattern and we can investigate the association rule where an item imply another and what’s the probability that the second item is purchased buying the first item. Diaper -&gt; Beer [0.5%, 75%] (support, confidence) The support is the frequency, express how many transactions support the pattern in all the transactions. The confidence express how much is the probability that purchasing diaper, the customer will purchase beer.</p>
            <h2 data-number="1.5" id="evaluation-of-results"><span class="header-section-number">1.5</span> Evaluation of results</h2>
            <p>Between all association rules we need to find the best rules, having some metrics to evaluate the results. In frequent pattern we want to understand if two items are purchased together. In frequent sequence we want to understand in different transactions if a sequence is repeated.</p>
            <h2 data-number="1.6" id="classification-and-label-prediction"><span class="header-section-number">1.6</span> Classification and label prediction</h2>
            <p>Classification and label prediction is a problem that consists on constructing models based on some training examples, we have to have some samples in which we know the label associated. Starting from this training set, the classification algorithm learn how to classify other records. In classification the output is a label, while in prediction the output is a numeric value.</p>
            <h2 data-number="1.7" id="unsupervised-learning"><span class="header-section-number">1.7</span> Unsupervised learning</h2>
            <p>In unsupervised learning the class label is unknown. In cluster analysis, in particular, we group data to form new categories, cluster houses to find distribution patterns. Principle: Maximizing intra class similarity &amp; minimizing interclass similarity.</p>
            <h2 data-number="1.8" id="outlier-analysis"><span class="header-section-number">1.8</span> Outlier analysis</h2>
            <p>An outlier is a data object that does not comply with the general behavior of the data. Outlier analysis is important to determine outliers. Useful in fraud detection or rare events analysis. Sequence, trend and evolution analysis, but also mining data streams are other possible uses.</p>
            <h2 data-number="1.9" id="graph-mining-and-information-network-analysis"><span class="header-section-number">1.9</span> Graph Mining and Information network analysis</h2>
            <p>Graph Mining consists in finding frequent subgraphs, trees or substructures on data. Information network analysis consists in analyzing social network and web mining consists in analyzing web information networks.</p>
            <h2 data-number="1.10" id="post-processing-phase"><span class="header-section-number">1.10</span> Post-processing phase</h2>
            <p>The post-processing phase is used to understand between all knowledge which are relevant for use, not all mined knowledge are interesting.</p>
            <h2 data-number="1.11" id="data-mining-and-multiple-disciplines"><span class="header-section-number">1.11</span> Data Mining and multiple disciplines</h2>
            <p><img src="../media/image3.png" /></p>
            <h2 data-number="1.12" id="high-dimensionality-and-high-complexity-of-data"><span class="header-section-number">1.12</span> High-dimensionality and high-complexity of data</h2>
            <p>Today we have a tremendous amount of data and we have high-dimensionality data, objects described by a high number of dimensions. With the increase of dimensionality objects appear far from each other. Most of algorithms are based on the concept of distance.</p>
            <p>We can change the algorithm or we can try to reduce the number of dimensions, in high-dimensionality some algorithms do not work properly. We have also high-complexity of data.</p>
            <h2 data-number="1.13" id="major-issues-in-data-mining"><span class="header-section-number">1.13</span> Major issues in data mining</h2>
            <p>The major issues in data mining are mining methodology, efficiency and scalability, diversity of data types, user interaction and impacts on society.</p>
            <h1 data-number="2" id="data"><span class="header-section-number">2</span> Data</h1>
            <p>We have different types of data sets: relational records, data matrix, numerical matrix, crosstabs (that displays the frequency distribution of the variables), document data (a term-frequency vector that displays the frequency of occurrence of terms in the document) transaction data, graph and networks, video data, temporal data, sequential data, spatial data (maps) and image data.</p>
            <h2 data-number="2.1" id="important-characteristics-of-structured-data"><span class="header-section-number">2.1</span> Important Characteristics of Structured Data:</h2>
            <ul>
            <li><p>Dimensionality</p></li>
            <li><p>Sparsity</p></li>
            <li><p>Resolution</p></li>
            <li><p>Distribution</p></li>
            </ul>
            <p>A data object represents an entity. Data objects are described by attributes.</p>
            <p>An attribute (feature, dimension, variable) is a data field, representing a characteristic or feature of a data object.</p>
            <h2 data-number="2.2" id="types-of-attributes"><span class="header-section-number">2.2</span> Types of attributes</h2>
            <p>Attributes can be of different types:</p>
            <ul>
            <li><p><strong>Nominal or categorical</strong> attributes, can have as possible values a finite number of labels.</p>
            <pre><code>Hair_color = {auburn, black, blond, brown, grey, red, white}</code></pre></li>
            <li><p><strong>Binary attributes</strong>, which are nominal attributes with only two states and can be:</p>
            <ul>
            <li><p><strong>symmetric binary</strong>; both outcomes equally important (ex. Gender)</p></li>
            <li><p><strong>asymmetric binary</strong>: outcomes not equally important (ex. Medical test, positive can have a more important outcome, we assign 1 to it for convention)</p></li>
            </ul></li>
            <li><p><strong>Ordinal attributes</strong> are categorical values that have a meaningful order but magnitude between successive values is not known.</p>
            <pre><code>Size = { small, medium, large }</code></pre>
            <blockquote>
            <p>In computing the distance in ordinal attributes it is possible to create a graduation, different than nominal attributes.</p>
            </blockquote></li>
            <li><p>Quantity, integer or real-valued, that can be:</p>
            <ul>
            <li><p>Interval, measured on a scale of equal-size units. Values have order. There’s no zero point and that implies that division makes no sense. e.g. temperature in C° or F°, calendar dates.</p></li>
            <li><p>Ratio, they have a zero point and it is possible to perform division.</p></li>
            </ul></li>
            </ul>
            <p>Another difference is in:</p>
            <ul>
            <li><p><strong>discrete attributes</strong>, that have only a finite or countably infinite set of values.</p></li>
            <li><p><strong>continuous attributes</strong>, that has real numbers as attribute values.</p></li>
            </ul>
            <h2 data-number="2.3" id="basic-statistical-descriptions-of-data"><span class="header-section-number">2.3</span> Basic Statistical Descriptions of Data</h2>
            <p>We can describe in a statistical way our data and visualize the distribution of data to better understand the data, and see if we have some central tendency, some variation and spread.</p>
            <p>If we can see similar attributes we can eliminate one of them because it’s not relevant in the data mining process.</p>
            <p>We have different data dispersion characteristics: median, max, min, quantiles, outliers, variance, etc.</p>
            <p>In a summarized way we can see the statistics with the box plot or quantile analysis.</p>
            <h2 data-number="2.4" id="measuring-the-central-tendency"><span class="header-section-number">2.4</span> Measuring the central tendency</h2>
            <ul>
            <li><p>To measure the central tendency we usually use the <strong>mean</strong>, which algebraic definition is:</p>
            <p><span class="math display">\[ 
                  \bar{x} = \frac{1}{N} \sum_{i=1}^{N}x_i 
              \]</span></p>
            <p>where <span class="math inline">\(N\)</span> is sample size.</p>
            <p>This is the mean along a singole attribute.</p></li>
            <li><p>the <strong>weighted arithmetic mean</strong> where we associate a weight to a specific value:</p>
            <p><span class="math display">\[
                  \bar{x} = \frac{\sum_{i=1}^{N}w_ix_i}{\sum_{i=1}^{N}w_i}
              \]</span></p></li>
            <li><p>we also have the <strong>trimmed mean</strong>, obtained by chopping out extreme values (for instance the top and bottom 2% before computing the mean, they’re outliers).</p></li>
            </ul>
            <h3 data-number="2.4.1" id="median"><span class="header-section-number">2.4.1</span> median</h3>
            <p>Another statistic value is the <strong>median</strong>, which is the middle value if odd number of values, or average of the middle two values otherwise.</p>
            <blockquote>
            <p>It is an holistic measure: it must be computed on the entire dataset as a whole.</p>
            </blockquote>
            <p>It can be heavy from a computational point of view, so sometimes it is estimated by interpolation (for grouped data).</p>
            <p>The estimation can be computed in this way:</p>
            <p><span class="math display">\[
                median = L_1 + \left(\frac{\frac{N}{2}-\sum{freq_i}}{freq_{median}}\right) width
            \]</span></p>
            <ul>
            <li><p><span class="math inline">\(L_1 :=\)</span> lower boundary of the median interval, that is the interval where we expect to have the median value.</p></li>
            <li><p><span class="math inline">\(freq_{median}:=\)</span> is the frequency of the median interval</p></li>
            <li><p><span class="math inline">\(N\)</span> the number of values in each interval on the x-axis.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="procedure">procedure</h4>
            <p>We first identify the interval in which the median value is located by subtracting the total number of values before this interval from N/2.</p>
            <p>Then, we divide this result by the frequency of values in the median interval.</p>
            <p>This estimation is based on the assumption that the values within the median interval are uniformly distributed.</p>
            <h3 data-number="2.4.2" id="mode"><span class="header-section-number">2.4.2</span> mode</h3>
            <p>Another statistical value to measure the central tendency is the <strong>mode</strong>, the value that occurs most frequently in the data. It can be unimodal, bimodal or trimodal.</p>
            <p>The empirical formula is :</p>
            <p><span class="math display">\[
                mean - mode = 3 \times (mean - median)
            \]</span></p>
            <h3 data-number="2.4.3" id="usage"><span class="header-section-number">2.4.3</span> usage</h3>
            <p>Using mean, median and mode we can see something about the distribution of data, we can say if it is symmetric or positively/negative skewed.</p>
            <p>Watching their order we can see if their distribution is symmetric or skewed.</p>
            <table>
            <thead>
            <tr class="header">
            <th>positively skewed</th>
            <th>symmetric</th>
            <th>negatively skewed</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image8.png" /></td>
            <td><img src="../media/image9.png" /></td>
            <td><img src="../media/image10.png" /></td>
            </tr>
            </tbody>
            </table>
            <h2 data-number="2.5" id="measuring-the-dispersion-of-data"><span class="header-section-number">2.5</span> Measuring the Dispersion of Data</h2>
            <h3 data-number="2.5.1" id="quartiles"><span class="header-section-number">2.5.1</span> Quartiles</h3>
            <p>Quantiles are a way to measure the dispersion of data.</p>
            <p>The k-th percentile, such as:</p>
            <ul>
            <li><p>the 25th percentile (Q1)</p></li>
            <li><p>75th percentile (Q3)</p></li>
            <li><p>50th percentile (median)</p></li>
            </ul>
            <p>are used to divide a set of data into equal groups.</p>
            <p>The Interquartile Range (IQR) is calculated as the difference between the third and first quartile (Q3 - Q1) and represents the range in which 50% of the data falls.</p>
            <h3 data-number="2.5.2" id="boxplot-analysis"><span class="header-section-number">2.5.2</span> Boxplot analysis</h3>
            <p>These quartiles are useful because they allow us to represents the distribution of data with boxplots.</p>
            <p>They are a visual representation of the distribution of data and in this we exploit the five number summary of a distribution: Minimum, Q1, Median, Q3, Maximum.</p>
            <p><img src="../media/image11.png" /></p>
            <ul>
            <li><p><span class="math inline">\(IQR:=\)</span> height</p></li>
            <li><p><span class="math inline">\(Whiskers:=\)</span> (the two lines) are the minimun and the maximum</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="outliers">outliers</h4>
            <p>We sometimes have marked points in the representation which are <strong>outliers</strong>, points beyond a specified outlier threshold, plotted individually (usually, a value higher/lower than 1.5 x IQR).</p>
            <h4 class="unnumbered" data-number="" id="boxplots-allow-us-to-compare-distributions.">Boxplots allow us to compare distributions.</h4>
            <p><img src="../media/image12.png" /></p>
            <ul>
            <li><p>The second distribution is very concentrated, we can imagine it with a peak.</p></li>
            <li><p>The third is more sparse.</p></li>
            <li><p>The median value of the third box plot is higher than the second’s.</p></li>
            </ul>
            <h3 data-number="2.5.3" id="standard-deviation"><span class="header-section-number">2.5.3</span> standard deviation</h3>
            <p>To measure the dispersion of data we can also use variance and standard deviation.</p>
            <p><span class="math display">\[
                Standard deviation := \sigma 
            \]</span></p>
            <p><span class="math display">\[
                Variance := \sigma^2 := \frac{1}{N}\sum{x_i -\bar{x}}^2 
            \]</span></p>
            <h2 data-number="2.6" id="graphic-displays-of-basic-statistical-descriptions"><span class="header-section-number">2.6</span> Graphic displays of basic statistical descriptions</h2>
            <p>We have different graphic displays of basic statistical descriptions:</p>
            <ul>
            <li><p><strong>Boxplot</strong> : graphic display of five number summary</p></li>
            <li><p><strong>Histogram</strong> : x axis are values, y axis represents frequencies os that values</p></li>
            <li><p><strong>Quantile plot</strong> : each value xi is paired with fi indicating that approximately 100 fi % of data are lower than or equal to xi (&lt;= xi)</p></li>
            <li><p><strong>Quantile quantile (q-q) plot</strong> : graphs the quantiles of one univariant distribution against the corresponding quantiles of another</p></li>
            <li><p><strong>Scatter plot</strong> : each pair of values is a pair of coordinates and plotted as points in the plane</p></li>
            </ul>
            <h3 data-number="2.6.1" id="histogram-analysis"><span class="header-section-number">2.6.1</span> Histogram Analysis</h3>
            <p>The Histogram is a graph display of tabulated frequencies, shown as bars.</p>
            <p>x axis are values, y axis represents frequencies os that values.</p>
            <p>It shows what proportion of cases fall into each of several categories.</p>
            <p><img src="../media/image15.png" /></p>
            <p>Differs from a bar chart in that it is the area of the bar that denotes the value, the frequency corresponding to that interval, not the height as in bar charts, a crucial distinction when the categories are not of uniform width.</p>
            <p>If we use the same width, the height corresponds to the value.</p>
            <p>The categories are usually specified as non overlapping intervals of some variable. The categories (bars)must be adjacent.</p>
            <p>Two distributions may have the same boxplot representation, because they have same values for: min, Q1, median, Q3, max, but they have rather different data distributions.</p>
            <p>Histograms often tell more than boxplots.</p>
            <h3 data-number="2.6.2" id="quantile-plot"><span class="header-section-number">2.6.2</span> Quantile plot</h3>
            <p>A quantile plot is a graph that shows the distribution of a dataset by plotting quantiles against an axis.</p>
            <p>The x-axis typically represents the quantiles of the data, while the y-axis represents the corresponding observations.</p>
            <p><img src="../media/image16.png" /></p>
            <h3 data-number="2.6.3" id="quantile-quantile-q-q-plot"><span class="header-section-number">2.6.3</span> Quantile-Quantile (Q-Q) Plot</h3>
            <p>A Quantile-Quantile (Q-Q) plot is a type of quantile plot that compares two probability distributions by plotting their quantiles against each other. It is used to check for similarity between two datasets. It is also known as a probability plot.</p>
            <p><img src="../media/image17.png" /></p>
            <h3 data-number="2.6.4" id="scatter-plot"><span class="header-section-number">2.6.4</span> Scatter Plot</h3>
            <p>The scatter plot provides a first look at bivariate data to see clusters of points, outliers, etc. The point will be displayed with two features. Each pair of values is treated as a pair of coordinates and plotted as points in the plane.</p>
            <p><img src="../media/image18.png" /></p>
            <p>We can deduce some conclusion about the scatter plot, the two variables are positively correlated if we can see scatters like this,</p>
            <table>
            <thead>
            <tr class="header">
            <th>positively correlated</th>
            <th>negatively correlated</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image19.png" /></td>
            <td><img src="../media/image20.png" /></td>
            </tr>
            </tbody>
            </table>
            <table>
            <thead>
            <tr class="header">
            <th>non-linear</th>
            <th>uncorrelated</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image21.png" /></td>
            <td><img src="../media/image22.png" /></td>
            </tr>
            </tbody>
            </table>
            <h2 data-number="2.7" id="data-visualization"><span class="header-section-number">2.7</span> Data Visualization</h2>
            <p>It’s important to show in a proper way the results we obtain to:</p>
            <ul>
            <li><p>gain insight into an information space</p></li>
            <li><p>provide qualitative overview of large data sets</p></li>
            <li><p>search for patterns, trends, structure, irregularities, relationships among data,</p></li>
            <li><p>help find interesting regions and suitable parameters for further quantitative analysis</p></li>
            <li><p>provide a visual proof of computer representations derived from data</p></li>
            </ul>
            <p>If we have only 2 or 3 dimensions is easy to visualize the data but if we have more than 3 dimensions it’s really complex.</p>
            <h3 data-number="2.7.1" id="categorization-of-visualization-methods"><span class="header-section-number">2.7.1</span> Categorization of visualization methods:</h3>
            <ul>
            <li><p><a href="#pixel-oriented-visualization-techniques">Pixel oriented</a></p></li>
            <li><p>Geometric projection</p></li>
            <li><p>Icon based</p></li>
            <li><p>Hierarchical</p></li>
            <li><p>Visualizing complex data and relations</p></li>
            </ul>
            <h3 data-number="2.7.2" id="pixel-oriented-visualization-techniques"><span class="header-section-number">2.7.2</span> Pixel-Oriented Visualization Techniques</h3>
            <p>For a data set of <span class="math inline">\(m\)</span> dimensions, create <span class="math inline">\(m\)</span> windows on the screen, one for each dimension. The m dimension values of a record are mapped to m pixels at the corresponding positions in the windows.</p>
            <p>Each pixel corresponds to an object and on each window we see the value for each attribute.</p>
            <p>The colors of the pixels reflect the corresponding values; if the color corresponds to white the value is lower, otherwise it’s greater.</p>
            <p><img src="../media/image23.png" /></p>
            <p>Income and age are not correlated, while income and credit limit have a strong correlation.</p>
            <h3 data-number="2.7.3" id="geometric-projection-visualization-techniques"><span class="header-section-number">2.7.3</span> Geometric Projection Visualization Techniques</h3>
            <p>These are visualization of geometric transformations and projections of the data.</p>
            <h3 data-number="2.7.4" id="scatterplot-and-scatterplot-matrices"><span class="header-section-number">2.7.4</span> Scatterplot and scatterplot matrices</h3>
            <p>We generate a matrix where we consider one attribute in the row of the matrix and we compare it with all others that correspond with the columns of the matrix.</p>
            <p>The matrix is symmetric and in the diagonal we’re comparing the same attributed so we have lines.</p>
            <p>With an high number of attributes this matrix is really complex to analyze.</p>
            <p><img src="../media/image24.png" /></p>
            <h2 data-number="2.8" id="parallel-coordinates"><span class="header-section-number">2.8</span> Parallel Coordinates</h2>
            <p>In Parallel Coordinates we have <span class="math inline">\(n\)</span> equidistant axes which are parallel to one of the screen axes and correspond to the attributes.</p>
            <p>Every data item corresponds to a polygonal line which intersects each of the axes at the point which corresponds to the value for the attribute.</p>
            <p>The axes are scaled to the <span class="math inline">\([minimum, maximum]\)</span>: range of the corresponding attribute.</p>
            <p><img src="../media/image25.png" /></p>
            <p>Depending on the dispersion of these polygonal lines we can understand a lot.</p>
            <h3 data-number="2.8.1" id="icon-based-visualization-techniques"><span class="header-section-number">2.8.1</span> Icon-based Visualization Techniques</h3>
            <p>Typical visualization methods are Chernoff Faces and Stick Figures.</p>
            <p>General techniques are:</p>
            <ul>
            <li><p><strong>Shape coding</strong> : Use shape to represent certain information encoding</p></li>
            <li><p><strong>Color icons</strong> : Use color icons to encode more information</p></li>
            <li><p><strong>Tile bars</strong> : Use small icons to represent the relevant feature vectors in document retrieval</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="chernoff-faces">Chernoff Faces</h4>
            <p>It’s a way to display variables on a two-dimensional surface, e.g., let x be eyebrow slant, y be eye size, z be nose length, etc.</p>
            <p><img src="../media/image26.png" /></p>
            <h4 class="unnumbered" data-number="" id="stick-figure">Stick Figure</h4>
            <p>It’s a very simple type of drawing made of lines and dots, often of the human form or other animals.</p>
            <p>Two attributes of the data are mapped to the display axes and the remaining attributes are mapped to the angle and/or length of the limbs and texture patterns in the visualization how certain data characteristics.</p>
            <p><img src="../media/image28.png" /></p>
            <h4 class="unnumbered" data-number="" id="tile-bar">Tile Bar</h4>
            <p>uses rectangles corresponding to documents.</p>
            <p>The query is specified in terms of k topics, one topic per line, called term sets.</p>
            <p>Columns in rectangles correspond to document segments.</p>
            <p>A square corresponds to a specific term set in a specific text segment.</p>
            <p>The darkness of a square indicates the frequency of terms in the segment from the corresponding TermSet. If the color is white the frequency is zero, while, if it’s dark we have an high frequency.</p>
            <p><img src="../media/image29.png" /></p>
            <h3 data-number="2.8.2" id="hierarchical-visualization-techniques"><span class="header-section-number">2.8.2</span> Hierarchical Visualization Techniques</h3>
            <p>Visualization of the data using a hierarchical partitioning into subspaces. We have different methods.</p>
            <h4 class="unnumbered" data-number="" id="dimensional-stacking">Dimensional Stacking</h4>
            <p>Partitioning of the n-dimensional attribute space in 2-D subspaces, which are ‘stacked’ into each other.</p>
            <p>Partitioning of the attribute value ranges into classes. The important attributes should be used on the outer levels.</p>
            <p>Adequate for data with ordinal attributes of low cardinality, but difficult to display more than nine dimensions.</p>
            <p><img src="../media/image30.png" /></p>
            <p>The idea is to start with the most important attribute and just split the attribute value ranges, identified by bold lines, and inside bold rectangles and doing the same with attribute 2 we have two more attributes inside. The same with the ones inside.</p>
            <p>For example the circled one has the highest value for attribute 1, almost the middle value for attribute 2 and then we can go inside and continue the stack.</p>
            <p><img src="../media/image31.png" /></p>
            <p>This is an example of visualization with longitude and latitude mapped to the outer x-, y-axes and ore grade and depth mapped to the inner x-, y-axes.</p>
            <h4 class="unnumbered" data-number="" id="tree-map">Tree-Map</h4>
            <p>This is another hierarchical approach and uses a hierarchical partitioning of the screen into regions depending on the attribute values.</p>
            <p>The x and y dimension of the screen are partitioned alternately according to the attribute values classes.</p>
            <p>The area represents the portion we have. Example: an overview of the organization of file and directory</p>
            <p><img src="../media/image32.png" /></p>
            <h4 class="unnumbered" data-number="" id="infocube">InfoCube</h4>
            <p>This is a 3D visualization technique where hierarchical information is displayed as nested semi-transparent cubes.</p>
            <p>The outermost cubes correspond to the top level data, while the sub-nodes or the lower level data are represented as smaller cubes inside the outermost cubes, and so on.</p>
            <p><img src="../media/image33.png" /></p>
            <h4 class="unnumbered" data-number="" id="tag-cloud">Tag Cloud</h4>
            <p>With non-numerical data, for example text and social networks, we usually use tag cloud.</p>
            <p>Depending on the size and color of this tag we can express the frequency of the tag in the document.</p>
            <p><img src="../media/image34.png" /></p>
            <h2 data-number="2.9" id="measuring-data-similarity-and-dissimilarity"><span class="header-section-number">2.9</span> Measuring Data Similarity and Dissimilarity</h2>
            <ul>
            <li><p><strong>Similarity</strong> is a numerical measure of how alike two data objects are.</p>
            <p>Value is higher when objects are more alike and often falls in the range [0,1].</p></li>
            <li><p><strong>Dissimilarity</strong> (e.g., distance) is a numerical measure of how different two data objects are.</p>
            <p>The value is lower when objects are more alike.</p>
            <p>Minimum dissimilarity is often 0, while upper limit varies. Distance has like un unlimited maximum value.</p></li>
            <li><p><strong>Proximity</strong> refers to a similarity or dissimilarity.</p>
            <p>Data could be represented with objects with several features.</p></li>
            </ul>
            <h3 data-number="2.9.1" id="from-the-data-matrix"><span class="header-section-number">2.9.1</span> From the data matrix…</h3>
            <p>The matrix is called <strong>data matrix</strong> and it is composed by n data points with p dimensions.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!-- --></th>
            <th><!-- --></th>
            <th><!-- --></th>
            <th><!-- --></th>
            <th><!-- --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><span class="math inline">\(x_{11}\)</span></td>
            <td></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td></td>
            <td><span class="math inline">\(x_{1p}\)</span></td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(\vdots\)</span></td>
            <td></td>
            <td><span class="math inline">\(\ddots\)</span></td>
            <td></td>
            <td><span class="math inline">\(\vdots\)</span></td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(x_{n1}\)</span></td>
            <td></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td></td>
            <td><span class="math inline">\(x_{np}\)</span></td>
            </tr>
            </tbody>
            </table>
            <h3 data-number="2.9.2" id="to-the-dissimilarity-matrix"><span class="header-section-number">2.9.2</span> …to the dissimilarity matrix</h3>
            <p>In a dissimilarity matrix, each cell in the matrix represents the dissimilarity between the two objects corresponding to the row and column of that cell.</p>
            <p>The matrix is symmetric. Additionally, the diagonal of the matrix is filled with 0s as the dissimilarity between an object and itself is always 0.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!-- --></th>
            <th><!-- --></th>
            <th><!-- --></th>
            <th><!-- --></th>
            <th><!-- --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>0</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(d(2,1)\)</span></td>
            <td>0</td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(\vdots\)</span></td>
            <td><span class="math inline">\(\vdots\)</span></td>
            <td>0</td>
            <td></td>
            <td></td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(d(n,1)\)</span></td>
            <td><span class="math inline">\(d(n,2)\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p>It’s easy to compute distance with objects represented by numerical attributes.</p>
            <p>We also have to be able to measure proximity for nominal attributes.</p>
            <h3 data-number="2.9.3" id="dissimilarity-matrix-for-nominal-attributes"><span class="header-section-number">2.9.3</span> dissimilarity matrix for nominal attributes</h3>
            <p>Nominal attributes can take 2 or more states, without any order.</p>
            <ul>
            <li><p><strong>simple matching</strong></p>
            <blockquote>
            <p>Let’s suppose that <span class="math inline">\(p\)</span> is the total number of attributes and <span class="math inline">\(m\)</span> is the number of matches <span class="math display">\[
            d(i,j) = \frac{p-m}{p}
            \]</span></p>
            </blockquote>
            <p>If we don’t have matches the distance is always 1, when we have all matches is 0.</p></li>
            <li><p>A second method is to <strong>use many binary attributes</strong> instead of nominal attributes. The number used will be the number of levels of the nominal attributes.</p>
            <div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co"># object described by eye color and hair color</span></span>
            <span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
            <span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>eyeColor <span class="op">=</span> {<span class="st">&quot;black&quot;</span>, <span class="st">&quot;green&quot;</span>, <span class="st">&quot;blue&quot;</span>}</span>
            <span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>hairColor <span class="op">=</span> {<span class="st">&quot;auburn&quot;</span>, <span class="st">&quot;black&quot;</span>, ...}</span>
            <span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a></span>
            <span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>i <span class="op">=</span> {<span class="st">&quot;eyeBlack&quot;</span>, <span class="st">&quot;eyeGreen&quot;</span>, <span class="st">&quot;eyeBlue&quot;</span>, <span class="st">&quot;hairAuburn&quot;</span>, <span class="st">&quot;hairBlack&quot;</span> ...}</span></code></pre></div>
            <p>Then we apply the distance between objects represented by binary attributes.</p></li>
            <li><p>You can <strong>use integers</strong>, but you have to be use to not use the integer distance, because it doesn’t mean 0 is close to 1 if you assign integers randomly.</p>
            <p>But now we must define how to compute the distance between objects with binary attributes.</p>
            <p>When we have to do with binary attributes, we usually use a <strong>contingency table</strong>, we have two objects, I and j, described with p binary attributes.</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th></th>
            <th><span class="math inline">\(object \ j\)</span></th>
            <th></th>
            <th></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td></td>
            <td></td>
            <td>1</td>
            <td>0</td>
            <td>sum</td>
            </tr>
            <tr class="even">
            <td></td>
            <td>1</td>
            <td>q</td>
            <td>r</td>
            <td>q+r</td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(object \ i\)</span></td>
            <td>0</td>
            <td>s</td>
            <td>t</td>
            <td>s+t</td>
            </tr>
            <tr class="even">
            <td></td>
            <td>sum</td>
            <td>q+s</td>
            <td>r+t</td>
            <td>p</td>
            </tr>
            </tbody>
            </table>
            <p>Label q is the number of attributes in which we have 1 in the object I and the object j. It is the number of matches of value 1. R represents the number of attributes in which I has value 1 and j has value 0.</p>
            <p>S represents the number of attributes in which I has value 0 and j has value 1.</p>
            <p>T represents the number of attributes in which I and j has value 0.</p>
            <p>If objects are equal the matrix has to be diagonal.</p></li>
            </ul>
            <h3 data-number="2.9.4" id="distance-measure-for-symmetric-binary-variables"><span class="header-section-number">2.9.4</span> Distance measure for symmetric binary variables</h3>
            <p><span class="math display">\[
                d(i,j) = \frac{r+s}{q+r+s+t}
            \]</span></p>
            <p>A binary variable is symmetric if both of its states are equally valuable and carry the same weight, if 0s and 1s are frequently equal, the probability to have them is the same.</p>
            <p>The numerator of this ratio contains the number of attributes in which we have different values for binary attributes. It is higher more the two objects are distant.</p>
            <h3 data-number="2.9.5" id="distance-measure-for-asymmetric-binary-variables"><span class="header-section-number">2.9.5</span> Distance measure for asymmetric binary variables:</h3>
            <p><span class="math display">\[
                d(i,j) = \frac{r+s}{q+r+s}
            \]</span></p>
            <p>In this case we have that a value is more probably than another value.</p>
            <p>T represents the number of attributes in which we have in I and j value 0, but this is the most probably value, so the probability of t is very high.</p>
            <p>In case of asymmetric binary value is very high I respect of q, r and s.</p>
            <p>The distance could be very small if we use t and we avoid this phenomenon not considering t.</p>
            <h3 data-number="2.9.6" id="jaccard-coefficient"><span class="header-section-number">2.9.6</span> Jaccard coefficient</h3>
            <p>a meause for symmetric binary variables</p>
            <p><span class="math display">\[
                sim_{Jaccard}(i,j) = \frac{q}{q+r+s}
            \]</span></p>
            <p>This is the same as <strong>coherence</strong>, that can be calculated in this way.</p>
            <p>The support on I and j is the number of attributes in I and j in which we have value 1, and the same for other supports.</p>
            <p><span class="math display">\[
                coherence(i,j) = \frac{sup(i,j)}{sup(i)+sup(j)-sup(i,j)} = \frac{q}{(q+r)+(q+s)-q}
            \]</span></p>
            <p>For example:</p>
            <table>
            <thead>
            <tr class="header">
            <th>Name</th>
            <th>Gender</th>
            <th>Fever</th>
            <th>Cough</th>
            <th>Test-1</th>
            <th>Test-2</th>
            <th>Test-3</th>
            <th>Test-4</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>Jack</td>
            <td>M</td>
            <td>Y</td>
            <td>N</td>
            <td>P</td>
            <td>N</td>
            <td>N</td>
            <td>N</td>
            </tr>
            <tr class="even">
            <td>Mary</td>
            <td>F</td>
            <td>Y</td>
            <td>N</td>
            <td>P</td>
            <td>N</td>
            <td>P</td>
            <td>N</td>
            </tr>
            <tr class="odd">
            <td>Jim</td>
            <td>M</td>
            <td>Y</td>
            <td>P</td>
            <td>N</td>
            <td>N</td>
            <td>N</td>
            <td>N</td>
            </tr>
            </tbody>
            </table>
            <p>Gender is a symmetric attribute, and the remaining attributes are asymmetric binary. Let the values Y and P be 1, and the value N be 0 (use only asymmetric values). We don’t consider t.</p>
            <p><span class="math display">\[
                D(Jack, Mary) = \frac{0+1}{2+0+1} = 0.33
            \]</span></p>
            <p><span class="math display">\[
                D(Jack, Jim) = \frac{1+1}{1+1+1} = 0.67
            \]</span></p>
            <p><span class="math display">\[
                D(Jack, Mary) = \frac{1+2}{1+1+2} = 0.75     
            \]</span></p>
            <h2 data-number="2.10" id="standardizing-numeric-data"><span class="header-section-number">2.10</span> Standardizing Numeric Data</h2>
            <p>Before computing dissimilarity between numerical data we sometimes try to normalize data. We would like the attributes to vary between similar intervals.</p>
            <h3 data-number="2.10.1" id="euclidean-distance-l2-norm-is-defined-like"><span class="header-section-number">2.10.1</span> Euclidean distance (L2 norm) is defined like:</h3>
            <p><span class="math display">\[
                d(i,j) = \sqrt{(x_{i1}-x_{j1})^2 + \dots (x_{iN}-x_{jN})^2}
            \]</span></p>
            <p>It can be applied only on numerical data.</p>
            <p>If one feature is in a large interval it will dominate others in the computation of the Euclidian distance.</p>
            <h3 data-number="2.10.2" id="the-z-score"><span class="header-section-number">2.10.2</span> The z-score</h3>
            <p><span class="math display">\[
                z = \frac{x-\mu}{\sigma}
            \]</span></p>
            <ul>
            <li><p><span class="math inline">\(x:=\)</span> raw score to be standardized</p></li>
            <li><p><span class="math inline">\(\mu:=\)</span> mean of the population</p></li>
            <li><p><span class="math inline">\(\sigma:=\)</span> standard deviation</p></li>
            </ul>
            <p>We subtract the mean value and we divide by the standard deviation, so that if data is dispersed the standard deviation is large.</p>
            <p>It is negative when the raw score is below the mean, “+” when above</p>
            <h3 data-number="2.10.3" id="mean-absolute-deviation"><span class="header-section-number">2.10.3</span> mean absolute deviation</h3>
            <p><span class="math display">\[
                s_f = \frac{1}{N}(|x_{1f} - m_f| + |x_{2f} - m_f| + \dots + |x_{Nf} - m_f|) 
            \]</span></p>
            <p><span class="math display">\[
                m_f = \frac{1}{N} (x_{1f} + \dots + x_{Nf})    
            \]</span></p>
            <p>And in that case we calculate the standardized measure (z-score):</p>
            <p><span class="math display">\[
                z_f = \frac{x_{if}-m_f}{s_f}
            \]</span></p>
            <blockquote>
            <p>Using mean absolute deviation is more robust to outliers than using standard deviation.</p>
            </blockquote>
            <h3 data-number="2.10.4" id="minkowski-distance"><span class="header-section-number">2.10.4</span> Minkowski distance</h3>
            <p><span class="math display">\[
                d(i,j) = \sqrt[h]{|x_{i1}-x_{j1}|^h + \dots |x_{iN}-x_{jN}|^h}
            \]</span></p>
            <p>Properties:</p>
            <ul>
            <li><p><span class="math inline">\(d(i,j) \gt 0\)</span> if <span class="math inline">\(i \neq j\)</span>, and <span class="math inline">\(d(i,i) = 0\)</span> (Positive definiteness)</p></li>
            <li><p><span class="math inline">\(d(i,j) = d(j,i)\)</span> (Simmetry)</p></li>
            <li><p><span class="math inline">\(d(i,j) \leq d(i, k) + d(k, j)\)</span> (Triangle Inequality)</p></li>
            </ul>
            <p>A distance that satisfies these properties is a metric from a mathematical point of view.</p>
            <p>From it we can derive the</p>
            <ul>
            <li><p><strong>Manhattan distance</strong> with <span class="math inline">\(h = 1\)</span> (L1 norm):</p></li>
            <li><p><strong>Euclidean distance</strong> with <span class="math inline">\(h = 2\)</span> (L2 norm):</p></li>
            <li><p><strong>Supremum distance</strong> with <span class="math inline">\(h \to \infty\)</span> (Lmax norm):</p></li>
            </ul>
            <p>Taking the following example we can see that values change but the relations remain similar.</p>
            <p><img src="../media/image55.png" /></p>
            <h3 data-number="2.10.5" id="ordinal-variables"><span class="header-section-number">2.10.5</span> Ordinal variables</h3>
            <p>An <strong>ordinal variable</strong> is similar to a categorical variable. The difference between the two is that there is a clear ordering of the variables.</p>
            <pre><code>    { small, medium, large } = { 1 , 2 , 3 }</code></pre>
            <p>It can be treated like interval scaled.</p>
            <ul>
            <li><p>we replace each value <span class="math inline">\(x_{if}\)</span> with their rank</p></li>
            <li><p>map the range of each variable onto <span class="math inline">\([0, 1]\)</span> by replacing <span class="math inline">\(i-th\)</span> object in the <span class="math inline">\(f-th\)</span> variable by</p></li>
            </ul>
            <p><span class="math display">\[
                z_{if} = \frac{r_{if}-1}{M_f-1}
            \]</span></p>
            <p>where <span class="math inline">\(M_f\)</span> is the maximum number of rank levels.</p>
            <pre><code>{ small, medium, large } = { 0 , 0.5 , 1 }</code></pre>
            <p>And now we can compute the dissimilarity using methods for interval scaled variable.</p>
            <h3 data-number="2.10.6" id="attributes-of-mixed-type"><span class="header-section-number">2.10.6</span> Attributes of Mixed Type</h3>
            <p>We discussed how to compute the dissimilarity between objects described by attributes of the same type, where these types may be either <em>nominal</em>, <em>sym- metric binary, asymmetric binary</em>, <em>numeric</em>, or <em>ordinal</em>. However, in many real databases, objects are described by a <em>mixture</em> of attribute types.</p>
            <p>One approach to compute the dissimilarity between objects of mixed attribute types is to group each type of attribute together, performing separate data mining.</p>
            <p>However, in real applications, it is unlikely that a separate analysis per attribute type will generate compatible results.</p>
            <p>A more preferable approach is to process all attribute types together, performing a single analysis.</p>
            <p>Bringing all of the meaningful attributes onto a common scale of the interval <span class="math inline">\([0.0, 1.0]\)</span>.</p>
            <ul>
            <li><p>Suppose that the data set contains <span class="math inline">\(N\)</span> attributes of mixed type.</p></li>
            <li><p>The dissimilarity <span class="math inline">\(d(i,j)\)</span> between objects is defined as:</p></li>
            </ul>
            <p><span class="math display">\[
                d(i,j) = \frac{\sum_{f=1}^N \delta_{ij}{(f)} \delta_{ij}{(f)}}{\sum_{f=1}^N \delta_{ij}{(f)}}
            \]</span></p>
            <p>where the indicator <span class="math inline">\(\delta_{ij}^{(N)} = 0\)</span> if:</p>
            <ul>
            <li><p><span class="math inline">\(x_{if}\)</span> or <span class="math inline">\(x_{jf}\)</span> is missing or</p></li>
            <li><p><span class="math inline">\(x_{if} = x_{jf} = 0\)</span> and attribute <span class="math inline">\(f\)</span> is asymmetric binary</p></li>
            </ul>
            <p>otherwise <span class="math inline">\(\delta_{ij}^{(N)} = 1\)</span></p>
            <h3 data-number="2.10.7" id="cosine-similarity"><span class="header-section-number">2.10.7</span> Cosine similarity</h3>
            <p><span class="math display">\[
                cos(d_1, d_2) = \frac{d_1 \cdot d_2}{||d_1|| \ ||d_2||} 
            \]</span></p>
            <p>ex:</p>
            <div class="sourceCode" id="cb6"><pre class="sourceCode js"><code class="sourceCode javascript"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a></span>
            <span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a>d1 <span class="op">=</span> [<span class="dv">5</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">3</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">2</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">2</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">0</span>]</span>
            <span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a>d2 <span class="op">=</span> [<span class="dv">3</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">2</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">1</span><span class="op">,</span><span class="dv">1</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">1</span><span class="op">,</span><span class="dv">0</span><span class="op">,</span><span class="dv">1</span>]</span>
            <span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a></span>
            <span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a>d1 <span class="op">*</span> d2 <span class="op">=</span> <span class="dv">25</span></span>
            <span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a><span class="op">||</span>d1<span class="op">||</span> <span class="op">=</span> <span class="fl">6.481</span> </span>
            <span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a><span class="op">||</span>d2<span class="op">||</span> <span class="op">=</span> <span class="fl">4.12</span></span>
            <span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a></span>
            <span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a><span class="fu">cos</span>(d1<span class="op">,</span> d2) <span class="op">=</span> <span class="fl">0.94</span></span></code></pre></div>
            <h1 data-number="3" id="preprocessing"><span class="header-section-number">3</span> Preprocessing</h1>
            <p>Measures for <strong>data quality</strong>: A multidimensional view</p>
            <ol type="1">
            <li><p>Accuracy: correct or wrong, accurate or not</p></li>
            <li><p>Completeness: not recorded, unavailable, ...</p></li>
            <li><p>Consistency: some modified but some not, dangling, ….</p></li>
            </ol>
            <ul>
            <li><p>Timeliness: timely update?</p></li>
            <li><p>Believability: how trustable the data are correct?</p></li>
            <li><p>Interpretability: how easily the data can be understood?</p></li>
            </ul>
            <p>Major Tasks in <strong>Data Preprocessing</strong> are:</p>
            <ul>
            <li><p><strong>Data cleaning</strong>: fill in missing values, smooth noisy data, identify or remove outliers, and resolve inconsistencies</p></li>
            <li><p><strong>Data integration</strong>: integration of multiple sources of data to create a unique dataset.</p></li>
            <li><p><strong>Data reduction</strong>: reduce the size of data set and the number of dimensions without jeopardizing the data mining results?</p>
            <ul>
            <li><p><strong>Dimensionality reduction</strong>: data encoding schemes are applied to obtain a compressed representation of the original data. It can be done with different techniques, like Compression techniques, Attribute subset selection or Attribute construction.</p></li>
            <li><p><strong>Numerosity reduction</strong>: smaller representations using parametric models (regression models) or nonparametric models (cluster, sampling, ecc.) Reducing the number of objects helps us to reduce the complexity of the problem and speed-up the execution.</p></li>
            </ul></li>
            <li><p><strong>Data transformation and data discretization</strong>: Data transformation includes normalization to make attributes value vary on the same interval.Discretizion is needed to transform a numerical value into a nominal value for applying some algorithms proposed in the lecterature. e.g. Age</p></li>
            </ul>
            <h2 data-number="3.1" id="data-cleaning"><span class="header-section-number">3.1</span> Data Cleaning</h2>
            <p>Data in real world is dirty.</p>
            <ul>
            <li><p><strong>Incompleteness</strong>: some values are missing. If we compute a distance with a missing value, the calculation is not reliable, we have a random value.</p></li>
            <li><p><strong>Noise</strong>: containing noise, errors, outlier. For example signals generated by sensor affected by noise. We get a signal with a ripple.</p></li>
            <li><p><strong>Inconsistency</strong>: In some datasets we can have inconsistency, for example we can have an age attribute that is not consistent with the birthday value.</p></li>
            <li><p><strong>Intentional</strong>: we can also have intentional missing data.</p></li>
            </ul>
            <h3 data-number="3.1.1" id="missing-data"><span class="header-section-number">3.1.1</span> Missing data</h3>
            <p>For handling <strong>missing data</strong> we can:</p>
            <ul>
            <li><p><strong>ignore the tuple</strong>. This solution is possible only if the % of missing values per attributes is low.</p></li>
            <li><p><strong>fill the missing values manually</strong>, but not possible if you have a big amount of data.</p></li>
            <li><p><strong>fill in it automatically</strong> with:</p>
            <ul>
            <li>a global constant, e.g. unknown</li>
            <li>the attribute mean of all samples</li>
            <li>the attribute mean for all samples belonging to the same class used when we have to work with classification.</li>
            <li>the most probable value: to get this, one approach is to infers the missing value.</li>
            </ul></li>
            </ul>
            <blockquote>
            <p>But in any case, we need to handle the missing value.</p>
            </blockquote>
            <h3 data-number="3.1.2" id="noise"><span class="header-section-number">3.1.2</span> Noise</h3>
            <p>Another problem is <strong>noise</strong>: random error or variance in a measured variable. The noise affects the result we can produce with our model, so we have to work with it.</p>
            <p>We need to be careful to not remove information when applying techniques.</p>
            <h3 data-number="3.1.3" id="smoothing-filters"><span class="header-section-number">3.1.3</span> Smoothing filters</h3>
            <p>It performs a smoothing action to the signal. With smoothing the data points of a signal are modified so that individual points that are higher than the immediately adjacent points (presumably because of noise) are reduced, and points that are lower than the adjacent points are increased.</p>
            <p>The most popular smoothing algorithm is the <strong>rectangular or unweighted sliding-average smooth</strong>.</p>
            <p>We replace each point in the signal with the average of <span class="math inline">\(m\)</span> adjacent points, where <span class="math inline">\(m\)</span> is a positive integer called the smooth width.</p>
            <p>ex:</p>
            <p><span class="math display">\[
                S_j = \frac{Y_{j-1}+Y_{j}+Y_{j+1}}{3}
            \]</span></p>
            <p>We can use another value and the value of the width affects the result. If I consider a very high width, we have a big smooth but the signal becomes flatter, we may change the information in the signal even if we eliminate the noise.</p>
            <table>
            <thead>
            <tr class="header">
            <th>original</th>
            <th>11 points moving</th>
            <th>51 points</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image68.png" /></td>
            <td><img src="../media/image67.png" /></td>
            <td><img src="../media/image66.png" /></td>
            </tr>
            </tbody>
            </table>
            <h4 class="unnumbered" data-number="" id="considerations">considerations</h4>
            <ul>
            <li><p>With 11 we consider 5 points in the left and 5 on the right and the function is smoothed. A first consideration is that we are considering that the signal’s frequency is lower of the noise.</p></li>
            <li><p>we have to be careful to the width of the rectangular filter.</p></li>
            <li><p>if the underlying function is a constant, or is changing linearly with time (increasing or decreasing), then no bias is introduced into the result. A bias is introduced, however, if the underlying function has a nonzero second derivative. At a local maximum, for example, moving window averaging always reduces the function value.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="savitzky-golay-smoothing-filter">Savitzky-Golay Smoothing Filter</h4>
            <p>That is a generalization of the rectangular filter. Instead of using a constant as we made with the rectangular filter we use coefficients.</p>
            <p>In general, the simplest type of digital filter (the non-recursive or finite impulse response filter) replaces each data value <span class="math inline">\(Y_j\)</span> by a linear combination <span class="math inline">\(S_j\)</span> of itself and some number of nearby neighbors</p>
            <p><span class="math display">\[
                S_j = \sum_{n=-n_L}^{n_R}{c_n Y_{j+n}}
            \]</span></p>
            <p>where <span class="math inline">\(_L\)</span> is the number of points used “to the left” of a data point i, i.e., earlier than it, while <span class="math inline">\(n_R\)</span> is the number used to the right, i.e., later.</p>
            <p>We compute this weighted sum where n is negative or positive depending on the samples we are considering.</p>
            <p>We consider some samples of the right and some on the right and compute this linear combination.If you replace every coefficient with the following, we get exactly the rectangular filter.</p>
            <p><span class="math display">\[
                c_n = \frac{1}{n_L + n_R + 1}
            \]</span></p>
            <p>where <span class="math inline">\(n_L\)</span> is the number of points used “to the left” of a data point i, i.e., earlier than it, while <span class="math inline">\(n_R\)</span> is the number used to the right, i.e., later.</p>
            <p>Using a generic <span class="math inline">\(c_n\)</span> gives us more flexibility.</p>
            <h5 class="unnumbered" data-number="" id="diffences-between-savitzky-golay-and-rectangular-filters">Diffences between Savitzky-Golay and rectangular filters</h5>
            <p>With the rectangular filter samples inside the window are approximated using a line parallel to the horizontal line.</p>
            <p>With the Savitzky-Golay approach we don’t use a line but a polynomial function. Inside the window we consider a more complex function.</p>
            <p>The idea of Savitzky-Golay filtering is to find filter coefficients cn to approximate the underlying function within the moving window not by a constant (whose estimate is the average), but by a polynomial of higher order, typically quadratic or quartic.</p>
            <h4 class="unnumbered" data-number="" id="how-can-we-estimate-c_n-considering-the-function-we-selected">How can we estimate <span class="math inline">\(c_n\)</span> considering the function we selected?</h4>
            <p>A typical approach is to apply a <strong>least-square method</strong>, in which we define the function that approximates better the points inside the window.</p>
            <p>It’s computational heavy to compute for each sample the least-square to find the approximation function.</p>
            <p>We have to work in real-time because we receive samples and we have to apply it. We don’t have much time to apply the smoothing filters.</p>
            <p>But Savitzky-Golay demonstrated that <strong>there’s no need to compute the least-square for each sample</strong> but <strong>coefficients can be computed offline</strong>.</p>
            <p>For each point <span class="math inline">\(Y_i\)</span>, we least-squares fit a polynomial to all <span class="math inline">\(n_L +n_R + 1\)</span> points in the moving window, and then set <span class="math inline">\(S_i\)</span> to be the value of that polynomial at position <span class="math inline">\(i\)</span>.</p>
            <p>In the original paper we can find this table:</p>
            <p><img src="../media/image71.png" /></p>
            <p>In this table we see that once fixed <span class="math inline">\(M\)</span> (the degree of the polynomial) <span class="math inline">\(n_L\)</span> and <span class="math inline">\(n_R\)</span>, we have the coefficients to be used in the formula.</p>
            <p>We just pick up coefficients and apply the filter.</p>
            <p><img src="../media/image72.png" /></p>
            <h5 class="unnumbered" data-number="" id="results">results</h5>
            <ul>
            <li><p>this filter reduces the height of the peak.</p></li>
            <li><p>the width we use in the window is lower than the width in peak in the first case. In the second case the width of the window is approximately the same as the width of the peak.</p></li>
            <li><p>we need to decide the width of the window dependently on the width of the peak.</p></li>
            </ul>
            <p>In the second sample the height is not reduced because we don’t use an approximation with a line like in the first example, but we use polynomial and that means we can approximate more the samples inside.</p>
            <p>But we have more ripple inside the signal.</p>
            <p>The larger the smooth width, the greater the noise reduction, but also the greater the possibility that the signal will be distorted by the smoothing operation.</p>
            <p>The optimum choice of smooth width depends upon the width and shape of the signal and the digitization interval.</p>
            <h5 class="unnumbered" data-number="" id="smoothing-ratio">smoothing ratio</h5>
            <p>For peak-type signals, the critical factor is the <em>smoothing ratio</em>, the ratio between the smooth width and the number of points in the half-width of the peak. In general, increasing the smoothing ratio improves the signal-to-noise ratio but causes a reduction in amplitude and an increase in the bandwidth of the peak.</p>
            <ul>
            <li><p>ex:</p>
            <p><img src="../media/image73.png" /></p>
            <p>We can apply the smoothing filter with a width of 7, where we have a reduction of the noise but the height of the peak is reduced and it is larger.</p>
            <p>We can see that if the ratio increases the amplitude of the peak decreases and the width of the peak increase.</p>
            <p>In general, if the objective of the measurement is to measure the true peak height and width, then smooth ratios below 0.2 should be used.</p>
            <p>While, if the objective of the measurement is to measure the peak position (x-axis value of the peak), much larger smoothing ratios can be employed if desired, because smoothing has no effect at all on the peak position (unless the increase in peak width is so much that it causes adjacent peaks to overlap).</p>
            <p>The position of the peak does not change in fact.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="when-should-you-smooth-a-signal">When should you smooth a signal?</h4>
            <ol type="1">
            <li><p>for cosmetic reasons, to prepare a nicer-looking graphic of a signal for visual inspection or publication.</p></li>
            <li><p>if the signal will be subsequently processed by an algorithm that would be adversely affected by the presence of too much high-frequency noise in the signal</p>
            <blockquote>
            <p>We need to be careful when we have some peaks.</p>
            </blockquote></li>
            <li><p>In outlier detection it can be used to remove outliers and let them become normal points.</p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="when-should-not-you-smooth-a-signal">When should NOT you smooth a signal?</h4>
            <ol type="1">
            <li><p>Smoothing will not significantly improve the accuracy of parameter measurement by least-squares measurements between separate independent signal samples</p></li>
            <li><p>All smoothing algorithms are at least slightly <em>lossy</em>, entailing some change in signal shape and amplitude</p></li>
            <li><p>It is harder to evaluate the fit by inspecting the residuals if the data are smoothed, because smoothed noise may be mistaken for an actual signal</p></li>
            <li><p>Smoothing the signal will seriously underestimate the parameters errors predicted by propagation-of-error calculations and the bootstrap method.</p></li>
            </ol>
            <h3 data-number="3.1.4" id="binning"><span class="header-section-number">3.1.4</span> Binning</h3>
            <p>Another way to handle noisy data is to use another technique which is <strong>binning</strong>.</p>
            <p>It consists in smooth a sorted data value by consulting its neighborhood.</p>
            <p>We have to first sort data eand partition into (equal-frequency) bins, intervals with the same number of values, then one can smooth by bin means, smooth by bin median, smooth by bin boundaries, etc.</p>
            <div class="sourceCode" id="cb7"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a>Sorted_data_for_price <span class="op">=</span> [<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">15</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">24</span>,<span class="dv">25</span>,<span class="dv">28</span>,<span class="dv">34</span>]</span>
            <span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a></span>
            <span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="co">## Partition into (equal frequency) bins:</span></span>
            <span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a>bin_1: [<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">15</span>]</span>
            <span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>bin_2: [<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">24</span>]</span>
            <span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a>bin_3: [<span class="dv">25</span>,<span class="dv">28</span>,<span class="dv">34</span>]</span>
            <span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a></span>
            <span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a><span class="co">## Smoothing by bin means:</span></span>
            <span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>bin_1: [<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>]</span>
            <span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a>bin_1: [<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">22</span>]</span>
            <span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a>bin_1: [<span class="dv">29</span>,<span class="dv">29</span>,<span class="dv">29</span>]</span>
            <span id="cb7-12"><a href="#cb7-12" aria-hidden="true"></a></span>
            <span id="cb7-13"><a href="#cb7-13" aria-hidden="true"></a><span class="co">## Smoothing by bin boundaries:</span></span>
            <span id="cb7-14"><a href="#cb7-14" aria-hidden="true"></a>bin_1: [<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">15</span>]</span>
            <span id="cb7-15"><a href="#cb7-15" aria-hidden="true"></a>bin_2: [<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">24</span>]</span>
            <span id="cb7-16"><a href="#cb7-16" aria-hidden="true"></a>bin_3: [<span class="dv">25</span>,<span class="dv">25</span>,<span class="dv">34</span>]</span></code></pre></div>
            <p>By using bin boundaries, we approximate values to the closest boundary.</p>
            <p>The effect we obtain is like the effect of smoothing.</p>
            <h3 data-number="3.1.5" id="other-smoothing-techniques"><span class="header-section-number">3.1.5</span> Other smoothing techniques</h3>
            <ul>
            <li><strong>Regression</strong>: smooth by fitting the data into regression functions. Re replace the original values with values along the line that approximate these values.
            <ul>
            <li>Linear regression (two attributes)</li>
            <li>Multiple linear regression (multiple attributes)</li>
            </ul></li>
            <li><strong>Clustering</strong>, but we need to first detect and remove outliers because if they get inside clusters they can change dramatically the result. We need to apply clustering imposing constraints.</li>
            </ul>
            <p><img src="../media/image75.png" /></p>
            <p>Today most of the data are collected with sensors and it’s important to approach the noise reduction.</p>
            <p>We assume that our signal is characterized by a low frequency and our noise by a high frequency and that’s why we can use smoothing filters.</p>
            <h2 data-number="3.2" id="data-cleaning-as-a-process"><span class="header-section-number">3.2</span> Data cleaning as a process</h2>
            <p>Tipically we have different sources of data and some of them are real databases.</p>
            <p>We have to clean data otherwise using machine learning techniques we find ourself in trouble, otherwise we have strange results and we don’t know the reason.</p>
            <h4 class="unnumbered" data-number="" id="detect-discrepancies-in-the-data">Detect discrepancies in the data</h4>
            <ul>
            <li><p><strong>metadata</strong>, that describe the domain, the range, the dependency and the distribution of the data. We can associate at the value of the attribute these information, in this way we can analyze if we have some values that do not correspond to the domain, for example.</p></li>
            <li><p><strong>check field overloading</strong>, performed by using one of the bits in the representation to get information about other bits. It typically results when developers squeeze new attribute definitions into unused portions of already defined attributes.</p></li>
            <li><p><strong>check</strong> some rules:</p>
            <ul>
            <li><em>uniqueness rules</em>, where each value of a given attribute must be different from all other values for that attribute</li>
            <li><em>consecutive rules</em>, where there can be no missing values between the lowest and the highest values</li>
            <li><em>null rule,</em> that specifies the use of blanks, question mark, and so on.</li>
            </ul></li>
            </ul>
            <p>We typically have commercial tools:</p>
            <ul>
            <li><p>Data scrubbing: use simple domain knowledge (e.g., postal code, spell-check) to detect errors and make corrections</p></li>
            <li><p>Data auditing: by analyzing data to discover rules and relationship to detect violators (e.g., correlation and clustering to find outliers)</p></li>
            </ul>
            <h3 data-number="3.2.1" id="data-migration-and-integration"><span class="header-section-number">3.2.1</span> Data migration and integration</h3>
            <p>When we have to integrate different sources of data to create a data warehouse, that try to describe the data is organized in such a way to perform operations efficiently in data.</p>
            <p><img src="../media/image76.png" /></p>
            <p>To create this data warehouse we typically use <strong>ETL</strong> (Extraction/Transformation/Loading) tools, that allow users to specify transformations through a graphical user interface.</p>
            <ul>
            <li><p><strong>Extraction</strong>: data is extracted from the source system into the staging area. Transformations if any are done in staging area so that performance of source system in not degraded. We have the need to extract data from the DB when the it is still functional, we can not stop it.</p></li>
            <li><p><strong>Transformation</strong>: Data extracted from source server is raw and not usable in its original form. Therefore it needs to be cleansed, mapped and transformed.</p>
            <p>An example of possible data integration is the following:</p>
            <p><img src="../media/image77.png" /></p>
            <p><strong>Data integrations problems</strong></p>
            <p>We can have same data with different names, different data with same name, data only found in a place and different keys for data.</p>
            <p>Some possible data integrity problems</p>
            <ol type="1">
            <li><p>Different spelling of the same person like Jon, John, etc.</p></li>
            <li><p>There are multiple ways to denote company name like Google, Google Inc.</p></li>
            <li><p>Use of different names like Cleaveland, Cleveland.</p></li>
            <li><p>There may be a case that different account numbers are generated by various applications for the same customer.</p></li>
            <li><p>In some data required files remains blank</p></li>
            <li><p>Invalid product collected at POS as manual entry can lead to mistakes.</p></li>
            </ol>
            <p>Validations are performed during this stage:</p>
            <ul>
            <li><strong>filtering</strong> (select only certain columns to load), character set conversion, conversion of units of measurements (some can be expressed with the english systems while in others you can use inch), data threshold validation check, required fields not blank, and so on.</li>
            </ul></li>
            <li><p><strong>Loading</strong>: We typically have huge volume of data needs to be loaded in a relatively short period. Load process has to be optimized.</p>
            <p><strong>Types of loading</strong> :</p>
            <ol type="1">
            <li><p>Initial load - where we populate all the Data Warehouse tables</p></li>
            <li><p>Incremental load - applying ongoing changes as when needed periodically</p></li>
            <li><p>Full refresh - Sometimes we have it, erasing the contents of one or more tables and reloading with fresh data</p></li>
            </ol>
            <p>After loading we have to perform <em>load verification</em>, verifying that key field data is neither missing nor null, data checks in dimension table as well as history table, and so on.</p>
            <p>We have different several commercial softwares to perform ETL and all the check we need in the process.</p></li>
            </ul>
            <h2 data-number="3.3" id="data-integration"><span class="header-section-number">3.3</span> Data Integration</h2>
            <p>When we perform data integration we can find ourself in <strong>redundancy</strong>.</p>
            <p>Redundant data occur often when integration of multiple databases.</p>
            <ul>
            <li><p><em>Object identification</em>: The same attribute or object may have different names in different databases</p></li>
            <li><p><em>Derivable data:</em> One attribute may be a “derived” attribute in another table, e.g., annual revenue</p></li>
            </ul>
            <p>Redundant attributes may be detected by correlation analysis and covariance analysis.</p>
            <h3 data-number="3.3.1" id="correlation-analysis"><span class="header-section-number">3.3.1</span> Correlation Analysis</h3>
            <p>The first analysis of correlation we see is <strong><span class="math inline">\(Χ^2\)</span> (chi-square) test</strong> and can be applied to nominal data.</p>
            <h4 class="unnumbered" data-number="" id="χ2-chi-square-test"><span class="math inline">\(Χ^2\)</span> (chi-square) test</h4>
            <p>The chi-square independence test is a procedure for testing if two categorical variables A and B are related in some population.</p>
            <p>If they are not independent, we assume there must be a correlation.</p>
            <p>Suppose we have:</p>
            <p><span class="math display">\[
                A = \{a_1 ..., a_c\}
            \]</span></p>
            <p><span class="math display">\[
                B = \{b_1 ..., b_r\}
            \]</span></p>
            <p>The data tuples can be represented by a contingency table, that express the frequencies of occurrences in pair of values. The cell corresponds to the number of instances in which we have for a value ai and for b value bj.</p>
            <p><img src="../media/image78.png" /></p>
            <p>If we just consider the last column, we find the sum of instances where we have value <span class="math inline">\(a_i\)</span>. The last row allows us to find the number of instances in which we have the value <span class="math inline">\(b_j\)</span>.</p>
            <p>In the last column and last row, we have the number of all instances we have in our database.</p>
            <p>The idea is to start from this table to understand if <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent.</p>
            <p>We have to compute <code>chi-square</code>, obtained in this way:</p>
            <p><span class="math display">\[
                X^2 = \sum{\frac{(Observed-Expected)^2}{Expected}}
            \]</span></p>
            <ul>
            <li><p>We subtract from the observed value the expected value and we divide the square for the expected value.</p></li>
            <li><p>The observed value is what we find in the contingency table, is the number of couple in which we have the pair of two values.</p></li>
            <li><p>The expected value is the frequency we expect for that combination value with the assumption that the two variables a and b are independent.</p></li>
            </ul>
            <p><span class="math display">\[
                X^2 = \sum_{i=1}^c\sum_{j=1}^r{\frac{(o_{ij}-e_{ij})^2}{e_{ij}}} 
            \]</span></p>
            <p><span class="math display">\[
                e_{ij} = \frac{count(A=a_i) \times count(B=b_j)}{n}
            \]</span></p>
            <ul>
            <li><p>Counting <span class="math inline">\(A = a_i\)</span> and divided by <span class="math inline">\(n\)</span> is the probability to have <span class="math inline">\(a_i\)</span></p></li>
            <li><p>count <span class="math inline">\(B = b_j\)</span> divided by <span class="math inline">\(n\)</span> is the probability to have <span class="math inline">\(b_j\)</span>.</p></li>
            </ul>
            <p>The product corresponds the product of probability to have <span class="math inline">\(a_i\)</span> and <span class="math inline">\(b_j\)</span>. We multiply them with the supposing that they are independent.</p>
            <p>If I have that <span class="math inline">\(o_{ij}\)</span> is approximately equal to <span class="math inline">\(e_{ij}\)</span>, I can deduce that <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are independent. This imply that the value of <span class="math inline">\(X^2\)</span> is closed to <span class="math inline">\(0\)</span></p>
            <blockquote>
            <p>Lower the value higher our confidence in deducing that the two variables are independent.</p>
            </blockquote>
            <h5 class="unnumbered" data-number="" id="example">example</h5>
            <p>Let’s suppose to have a database and we want to understand if there’s correlation between reading and playing chess.</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>Play chess</th>
            <th>Not play chess</th>
            <th>Sum</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>Likes science</td>
            <td>250(90)</td>
            <td>200(360)</td>
            <td>450</td>
            </tr>
            <tr class="even">
            <td>Doesn’t like science</td>
            <td>50(210)</td>
            <td>1000(840)</td>
            <td>1050</td>
            </tr>
            <tr class="odd">
            <td>Sum</td>
            <td>300</td>
            <td>1200</td>
            <td>1500</td>
            </tr>
            </tbody>
            </table>
            <p>Values not in parenthesis are observed values, while numbers in parenthesis are expected counts calculated based on the data distribution in the two categories.</p>
            <p>To compute the expected value we have, for example, to count the number of instances in which we have people who like science fiction, multiply for the number of instances that play chess and divide by the total number of instances.</p>
            <p><span class="math display">\[
                e_{ij} = \frac{count(A=a_i) \times count(B=b_j)}{n}
            \]</span></p>
            <p><span class="math display">\[
                x_2 = 300 \cdot 450 / 1500 = 90.
            \]</span></p>
            <p>Then we use chi-square formula:</p>
            <p><span class="math display">\[
                X^2 = \frac{(250-90)^2}{90} + \frac{(50-210)^2}{210} + \frac{(200-360)^2}{360} +\frac{(1000-840)^2}{840} = 507.93
            \]</span></p>
            <blockquote>
            <p>Is this value high or low? What kind of information provide us?</p>
            </blockquote>
            <p>When we have to work with statistical test we have two possibilities:</p>
            <ul>
            <li><p><strong>Parametric test</strong>: hypothesis test based on the assumption that observed data are distributed according to some distributions of well- known form (normal, Bernoulli, and so on) up to some unknown parameter on which we want to make inference (say the mean, or the success probability). We know the shape but we need to determine the parametric, for example in normal distribution we need to compute the mean and the variance.</p></li>
            <li><p><strong>Nonparametric test</strong>: hypothesis test where it is not necessary (or not possible) to specify the parametric form of the distribution(s) of the underlying population(s).</p></li>
            </ul>
            <p><img src="../media/image84.png" /></p>
            <h4 class="unnumbered" data-number="" id="we-want-to-verify-if-two-variables-are-independent.">We want to verify if two variables are independent.</h4>
            <ul>
            <li><p>We start from the <strong>Null Hypothesis</strong> (H0), that states that no association exists between the two cross-tabulated variables in the population and therefore the variables are statistically independent.</p></li>
            <li><p>If we reject the null hypothesis we have to assume that the two variables are correlated, we have the <strong>Alternative Hypothesis</strong> (H1) that proposes that the two variables are related in the population.</p>
            <blockquote>
            <p>We need to be careful that correlation does not imply casualty.</p>
            </blockquote></li>
            <li><p>The <strong>degrees of Freedom</strong> are the number of cells in the two-way table of the categorical variables that can vary, given the constraints of the row and column marginal totals. So each "observation" in this case is a frequency in a cell.</p>
            <ul>
            <li><p><span class="math inline">\(d_f := 1\)</span> because if I fix the value in <span class="math inline">\(?\)</span>, I have also fix automatically the other values.</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>Cat</th>
            <th>A</th>
            <th>Total</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><strong>Cat</strong></td>
            <td>?</td>
            <td></td>
            <td>6</td>
            </tr>
            <tr class="even">
            <td><strong>B</strong></td>
            <td></td>
            <td></td>
            <td>15</td>
            </tr>
            <tr class="odd">
            <td><strong>Total</strong></td>
            <td>10</td>
            <td>11</td>
            <td>21</td>
            </tr>
            </tbody>
            </table></li>
            <li><p><span class="math inline">\(d_f = 2\)</span></p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>Cat</th>
            <th>A</th>
            <th></th>
            <th>Total</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><strong>Cat</strong></td>
            <td>?</td>
            <td></td>
            <td></td>
            <td>15</td>
            </tr>
            <tr class="even">
            <td><strong>B</strong></td>
            <td></td>
            <td></td>
            <td></td>
            <td>15</td>
            </tr>
            <tr class="odd">
            <td><strong>Total</strong></td>
            <td>10</td>
            <td>11</td>
            <td>9</td>
            <td>30</td>
            </tr>
            </tbody>
            </table></li>
            </ul></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="general-rule-to-determine-the-degree-of-freedom.">General rule to determine the degree of freedom.</h5>
            <p>The number of degrees of freedom df is equal to:</p>
            <p><span class="math display">\[ 
                d_f = (r-1) (c-1)
            \]</span></p>
            <p>where <span class="math inline">\(r\)</span> is the number of rows and <span class="math inline">\(c\)</span> is the number of columns.</p>
            <blockquote>
            <p>We determine the degree of freedom because we have different chi-square distributions of our problem depending on the degree of freedom.</p>
            </blockquote>
            <p><img src="../media/image87.png" /></p>
            <p>Probability distributions provide the probability of every possible value that may occur. When we have the value the probability of this value is given of area under the curve starting from the value to the end of the curve.</p>
            <p><img src="../media/image88.png" /></p>
            <p>The <span class="math inline">\(Χ^2\)</span> distribution is a type of probability distribution.</p>
            <p>We can determinate the probability calculating the area starting from the chi-square value until the end. To find the probability of a particular value, we find the area under the curve from the value, and it is called the p-value.</p>
            <p>This probability indicates also the probability of the null hypothesis.</p>
            <p>The shape is always skewed right.</p>
            <blockquote>
            <p>Many statistical analyses involve using the p-value. However, calculating a portion of the area under the curve can be difficult.</p>
            </blockquote>
            <p>Alternatively, we can use another <strong>curve</strong>, where we have the <span class="math inline">\(X^2\)</span> on an axis and the p-value in another.</p>
            <p><img src="../media/image89.png" /></p>
            <p>If we compute and set the degree of freedom, it picks the curve we must use.</p>
            <p>Then we can compute the <span class="math inline">\(X^2\)</span> and we can find the point in the <span class="math inline">\(Χ^2\)</span> table corresponding to our <span class="math inline">\(d_f\)</span> and we can determine the value of <span class="math inline">\(p\)</span>.</p>
            <p>The probability of the null hypothesis in the example is <span class="math inline">\(0.3\)</span>.</p>
            <h4 class="unnumbered" data-number="" id="x2-table"><span class="math inline">\(X^2\)</span> table</h4>
            <p>In this table, each row represents a different degree of freedom along with several <span class="math inline">\(X^2\)</span> values. The corresponding p-values are listed at the top of each column</p>
            <p><img src="../media/image90.png" /></p>
            <p>The <strong>null hypothesis</strong> is rejected when the probability of a larger value of <span class="math inline">\(X^2\)</span> is lower than the significance level <span class="math inline">\(\alpha\)</span>.</p>
            <h5 class="unnumbered" data-number="" id="steps">Steps</h5>
            <ul>
            <li><p>compute <span class="math inline">\(X^2\)</span> and with this value we go to this table</p></li>
            <li><p>select our degree of freedom</p></li>
            <li><p>check the value we obtained for <span class="math inline">\(X^2\)</span> corresponding to the probability we want to have to accept/reject the null hypothesis.</p></li>
            </ul>
            <p>If <span class="math inline">\(X^2\)</span> is higher than the value in the table, then we know that the probability to have a true <strong>null hypothesis</strong> is lower than the probability we read in that column.</p>
            <p>If the probability of the null hypothesis is low then the probability of two variables being correlated to each other is high.</p>
            <ul>
            <li><p>ex:</p>
            <p>A scientist wants to know if education level and marital status are related for all people in some country. He collects data on a simple random sample of n = 300 people. He produced this contingency table.</p>
            <p><img src="../media/image91.png" /></p>
            <p>We compute the expected frequencies, and we obtain this contingency table with expected value:</p>
            <p><img src="../media/image92.png" /></p>
            <p>Now we compute the <code>chi-square</code>:</p>
            <p><span class="math display">\[
                  X^2 = \frac{(18-11.7)^2}{11.7} + \frac{(36-27)^2}{27} + \dots + \frac{(6-5.4)^2}{6-5.4} = 23.57  
              \]</span></p>
            <p>Now we have to compute the probability that we obtain this specific value, or we fix the probability, and we see if the null hypothesis can be rejected.</p>
            <p>To compute the probability, we compute the df, which in this case is 12.</p>
            <p><img src="../media/image94.png" /></p>
            <p>We find the probability is <span class="math inline">\(0.023\)</span>, it is very low and we can conclude that marital status and education are related in our population. We could find the same conclusion using the table we saw.</p></li>
            </ul>
            <p>We need statistical tests to assume if that the value of <code>chi-square</code> is relevant or not.</p>
            <p>A limitation is that the <code>chi-square</code> is applicable only on nominal attributes, because it is complicated to compute the contingency table and the probability.</p>
            <h3 data-number="3.3.2" id="correlation-coefficient"><span class="header-section-number">3.3.2</span> Correlation coefficient</h3>
            <p>For numeric data we can compute correlation using <strong>correlation coefficient</strong> or Pearson’s product moment coefficient.</p>
            <p><span class="math display">\[
                r_{A,B} = \frac{\sum_{i=1}^{n}{(a_i-\bar{A})(b_i -\bar{B})}}{n\sigma_A\sigma_B}
            \]</span></p>
            <ul>
            <li><p><span class="math inline">\(r_{A,B} \gt 0\)</span>: A and B are positively correlated (A’s values increase as B’s). The higher, the stronger correlation.</p></li>
            <li><p><span class="math inline">\(r_{A,B} \gt 0\)</span>: independent</p></li>
            <li><p><span class="math inline">\(r_{A,B} \lt 0\)</span>: negatively correlated</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="implications">implications</h4>
            <ul>
            <li>If the correlation coefficient is <span class="math inline">\(&gt;&gt;&gt;\)</span> or <span class="math inline">\(&lt;&lt;&lt; 0\)</span>, an high value in magnitude, the correlation is strong, and higher the value in magnitude the stronger the correlation.</li>
            </ul>
            <blockquote>
            <p>We must be careful, if the correlation is equal to 0 it doesn’t imply that the two variables are independent, and that’s because the Pearson’s product moment coefficient only detects linear relationships.</p>
            </blockquote>
            <ul>
            <li><p>If variables are independent the correlation will be equal to 0.</p>
            <p><img src="../media/image96.png" /></p></li>
            <li><p>We can visually evaluate correlation with the scatter plot.</p>
            <p><img src="../media/image97.png" /></p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="correlation">Correlation</h4>
            <p>it measures the linear relationship between objects.</p>
            <p>To compute correlation, we standardize data objects, A and B, and then take their dot product:</p>
            <p><span class="math display">\[
                a_k^{&#39;} = (a_k-mean(A)) / \sigma_A
            \]</span></p>
            <p><span class="math display">\[
                b_k^{&#39;} = (b_k-mean(B)) / \sigma_B
            \]</span></p>
            <p><span class="math display">\[
                correlation(A,B) = A^{&#39;} \cdot B^{&#39;}
            \]</span></p>
            <h4 class="unnumbered" data-number="" id="covariance">Covariance</h4>
            <p>is similar to correlation:</p>
            <p><span class="math display">\[
                Cov(A,B) = E((A-\bar{A})(B-\bar{B})) = \frac{\sum_{i=1}^{n}(a_i-\bar{A})(b_i-\bar{B})}{n}
            \]</span></p>
            <p>They relation is given by this, the correlation coefficient:</p>
            <p><span class="math display">\[
                r_{A,B} = \frac{Cov(A,B)}{\sigma_A\sigma_B}
            \]</span></p>
            <ul>
            <li><p><strong>Positive covariance</strong>: If <span class="math inline">\(Cov(A,B) &gt; 0\)</span>, then <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> both tend to be larger than their expected values.</p></li>
            <li><p><strong>Negative covariance</strong>: If <span class="math inline">\(Cov(A,B) &lt; 0\)</span> then if A is larger than its expected value, B is likely to be smaller than its expected value.</p></li>
            <li><p><strong>Independence</strong> : <span class="math inline">\(Cov(A,B) = 0\)</span> but the converse is not true</p></li>
            </ul>
            <p>The covariance can be simplified in this way:</p>
            <p><span class="math display">\[
                Cov(A,B) = E(A\cdot B) - \bar{A}\bar{B}
            \]</span></p>
            <h2 data-number="3.4" id="data-reduction"><span class="header-section-number">3.4</span> Data Reduction</h2>
            <p>Data reduction consists in obtaining a reduced representation of the data set that is much smaller in volume but yet produces the same (or almost the same) analytical results.</p>
            <p>Data reduction strategies:</p>
            <ul>
            <li><p><strong>Dimensionality reduction</strong>, e.g., remove unimportant attributes: Wavelet transforms, Principal Components Analysis (PCA), feature subset selection and feature creation</p></li>
            <li><p><strong>Numerosity reduction</strong> (some simply call it: Data Reduction): Regression and Log-Linear Models, Histograms, clustering, sampling and data cube aggregation</p></li>
            <li><p><strong>Data compression</strong></p></li>
            </ul>
            <h3 data-number="3.4.1" id="dimensionality-reduction"><span class="header-section-number">3.4.1</span> Dimensionality Reduction</h3>
            <p>When dimensionality increases, data becomes increasingly sparse.</p>
            <p>Density and distance between points, which is critical to clustering, outlier analysis, becomes less meaningful and this obstacle the application of different algorithms.</p>
            <h5 class="unnumbered" data-number="" id="problems">problems</h5>
            <ul>
            <li><p>The possible combinations of subspaces will grow exponentially.</p></li>
            <li><p>We will avoid the curse of dimensionality, help eliminate irrelevant features and reduce noise and reduce time and space required in data mining.</p></li>
            <li><p>Dimensionality reduction also allow easier visualization.</p></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="dimensionality-reduction-techniques-are">Dimensionality reduction techniques are:</h5>
            <ul>
            <li><p>Wavelet transforms</p></li>
            <li><p>Principal Component Analysis</p></li>
            <li><p>Supervised and non-linear techniques (e.g. feature selection)</p></li>
            </ul>
            <h3 data-number="3.4.2" id="principal-component-analysis-pca"><span class="header-section-number">3.4.2</span> Principal Component Analysis (PCA)</h3>
            <p>PCA transforms the original space in a transformed space, find a projection that capture the largest amount of variation in data. We find this transformed space such that the variation of the projection of the data in the transformed axis is higher than what we obtain in the original axis.</p>
            <p><img src="../media/image103.png" /></p>
            <p>Newly projected red points are more widely spread out than in the original dataset, i.e. more variance.</p>
            <p>PCA tries to find axes such that the variation is higher.</p>
            <p>If I have features in which data vary very much, probably these features contain a lot of information, this is the idea.</p>
            <p>If I apply PCA I find that the transformed space has the same number of dimensions of the original space.</p>
            <p>Actually, we can select the dimensions in the transformed space because we can have some idea of how much exists variation along transformed dimension.</p>
            <h4 class="unnumbered" data-number="" id="eigenvectors">Eigenvectors</h4>
            <p>To exploit this we use eigenvector of the covariance matrix and we can reduce the size of the transformed space because we can eliminate eigenvectors associated with low eigenvalues.</p>
            <p>The original data are projected into a much smaller space, resulting in dimensionality reduction. We find the eigenvectors of the covariance matrix, and these eigenvectors define the new space.</p>
            <h5 class="unnumbered" data-number="" id="steps-1">steps</h5>
            <p>Given <span class="math inline">\(N\)</span> data vectors from <span class="math inline">\(f\)</span> dimensions, find <span class="math inline">\(k \leq f\)</span> orthogonal vectors that can be best used to represent data.</p>
            <blockquote>
            <p>This work for numeric data only.</p>
            </blockquote>
            <ol type="1">
            <li><p>Normalize input data: Each attribute falls within the same range</p></li>
            <li><p>Compute k orthonormal (unit) vectors, i.e., principal components.</p></li>
            <li><p>Each input data (vector) is a linear combination of the k principal component vectors</p></li>
            </ol>
            <p>The principal components are sorted in order of decreasing “significance” or strength</p>
            <p>Since the components are sorted, the size of the data can be reduced by eliminating the weak components (those with low variance).</p>
            <p>Given <span class="math inline">\(N\)</span> data vectors from <span class="math inline">\(f\)</span>-dimensions, find <span class="math inline">\(k\leq f\)</span> orthogonal vectors (principal components) that can be best used to represent data.</p>
            <h4 class="unnumbered" data-number="" id="pca-process">PCA process</h4>
            <p>We start from these data vectors, in the row we present the dimension and in the column we represents the samples. We subtract the mean values features by features, because we want to compute the covariance matrix from our dataset.</p>
            <ol type="1">
            <li><p>Calculate <code>adjusted_data_set</code> by removing the mean from each feature</p>
            <div class="sourceCode" id="cb8"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="co"># A := adjusted_data_set</span></span>
            <span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a><span class="cf">for</span> f <span class="kw">in</span> data.features:</span>
            <span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a>    A[f] <span class="op">=</span> data_set[f] <span class="op">-</span> np.mean(data_set[f])</span></code></pre></div></li>
            <li><p>Calculate the co-variance matrix (<span class="math inline">\(X,Y\)</span> are just two aliases for <span class="math inline">\(A\)</span>)</p>
            <p>in theory <span class="math inline">\(C = Cov(X,Y)\)</span> is equal to:</p>
            <p><span class="math display">\[
                 Cov(X,Y) = \frac{\sum_{i=1}^{n}(x_i-\bar{X})(y_i-\bar{Y})}{N-1}
             \]</span></p>
            <p>but since the adjusted_data_set has mean equal to <span class="math inline">\(0\)</span></p>
            <p><span class="math display">\[
                 Cov(X,Y) = \frac{(AA^{T})}{N-1}
             \]</span></p>
            <div class="sourceCode" id="cb9"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="co"># C := coverage_matrix </span></span>
            <span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a></span>
            <span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>C <span class="op">=</span> np.cov(A, rowvar<span class="op">=</span><span class="va">False</span>)</span></code></pre></div></li>
            <li><p>Calculate the eigenvectors</p>
            <p>this is done with some <code>witchcraft</code> that finds some <span class="math inline">\(f\)</span> <code>eigenvectors</code> where <span class="math inline">\(f \leq N\)</span> and <span class="math inline">\(N\)</span> is the number of original features</p>
            <div class="sourceCode" id="cb10"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a>eigenvalues, eigenvectors <span class="op">=</span> np.linalg.eig(C)</span></code></pre></div>
            <p>For each <code>eigenvectors</code> there is an <code>eigenvalues</code>, we can remove <code>eigenvectors</code> with a low <code>eigenvalue</code>, by doing this we create a mtrix <code>E</code> that contain a subset of eigenvectors.</p>
            <p>We then use the matrix E to transform our data contained in the matrix A.</p></li>
            <li><p>Transform data_set to the new basis</p>
            <p><span class="math display">\[
                 F = E^{T} A
             \]</span></p>
            <div class="sourceCode" id="cb11"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="co"># F := new_data_set </span></span>
            <span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a>F <span class="op">=</span> np.dot(data_set, eigenvectors)</span></code></pre></div>
            <p>The F matrix contain the transformed dataset where each object is defined on the transformed space but using only the dimensions fixed by the eigenvectors we selected.</p>
            <p>We project the data along new axis where we have a high variance of data and we eliminate dimensions where we have few variation.</p></li>
            </ol>
            <h5 class="unnumbered" data-number="" id="conclusions">conclusions</h5>
            <p>the dimensions of <span class="math inline">\(F\)</span> are less than the original dataset, to recover <span class="math inline">\(A\)</span> from <span class="math inline">\(F\)</span></p>
            <p><span class="math display">\[
                (E^T)^{-1}F = (E^T)^{-1}E^TA
            \]</span></p>
            <p><span class="math display">\[
                (E^T)^TF = A
            \]</span></p>
            <p><span class="math display">\[
                EF = A
            \]</span></p>
            <blockquote>
            <p><span class="math inline">\(E\)</span> is orthogonal therefore <span class="math inline">\(E^{-1} = E^T\)</span></p>
            </blockquote>
            <blockquote>
            <p>In the PCA we use the distribution of data, therefore the PCA is unsupervised.</p>
            </blockquote>
            <h5 class="unnumbered" data-number="" id="usage-1">usage</h5>
            <p>In any type of data mining process this can be used for dimensional reduction, and it is independent of any kind of application.</p>
            <p>The reduction of the number of dimensions is performed in the transformed space.</p>
            <p>The transformed space and the new axes are obtained as linear combination of old axes, it’s not easy to understand the impact in original features.</p>
            <h2 data-number="3.5" id="attribute-subset-selection"><span class="header-section-number">3.5</span> Attribute subset selection</h2>
            <p>The discussion is different if I use attribute subset selection.</p>
            <p>I can work on the original space applying some heuristics.</p>
            <p>I can discover to have:</p>
            <ul>
            <li><p><strong>Redundant attributes</strong>: like duplicate much or all of the information contained in one or more other attributes</p></li>
            <li><p><strong>Irrelevant attributes</strong>: contain no information that is useful for the data mining task at hand</p></li>
            </ul>
            <p>It’s not simple to perform attribute selection because the number possible combination of attributes grows exponentially.</p>
            <p>We need to perform this analysis using a greedy approach:</p>
            <p>make what looks to be the best choice at the time. When we apply this algorithm noone can guarantee that the subset we will select is a global optimum, but it’s close to the optimum.</p>
            <h4 class="unnumbered" data-number="" id="typical-heuristic-attribute-selection-methods">Typical heuristic attribute selection methods:</h4>
            <ul>
            <li><p><strong>Best single attribute</strong> under the attribute independence assumption: choose by significance tests</p></li>
            <li><p><strong>Best stepwise feature selection</strong>: The best single-attribute is picked first, then next best attribute condition to the first</p></li>
            <li><p><strong>Step-wise attribute elimination</strong>: Repeatedly eliminate the worst attribute</p></li>
            <li><p><strong>Best combined attribute selection and elimination</strong></p></li>
            <li><p><strong>Optimal branch and bound</strong>: use attribute elimination and backtracking</p>
            <p><img src="../media/image111.png" /></p>
            <p>In the forward selection the attribute is the best when we select it, but at the end no one can guarantee that.</p>
            <p>In the backward elimination we can make similar considerations.</p>
            <p>A typical problem is the <em>stopping condition</em>, sometimes in most approaches we fix the number of dimensions we want to obtain, otherwise we can use specific metrics for selecting the attributes and different metrics as termination condition.</p>
            <p>In the slide we have another approach, the decision tree induction. We use decision tree learning algorithm to perform feature selection.</p></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="example-of-heuristic-approach">Example of heuristic approach</h5>
            <p>This approach assumes that we know the classes of the instances, this is a supervised approach, and it works on the original space.</p>
            <p>We can eliminate features in the original space.</p>
            <p>We’re dealing with a classification problem; we have a set of objects described by features and we want a system that can associate classes to these objects.</p>
            <p>Correlation here means that if we have a value in an attribute, we tend to have some certain classes.</p>
            <p>We want to select features that are strongly correlated with the class.</p>
            <h3 data-number="3.5.1" id="mutual-information"><span class="header-section-number">3.5.1</span> Mutual information</h3>
            <p>If two attributes are equal and strongly correlated with the class, we select both attributes, but this is not a good idea to proceed.</p>
            <p>We should also avoid selecting attributes strongly correlated with each other.</p>
            <p>We need to transmit these intuitions into formula.</p>
            <p>Let’s consider two discrete variables X and Y, with alphabets XX and YY, respectively.</p>
            <p>The <strong>mutual information</strong> (I) between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with a joint probability mass function <span class="math inline">\(p(x,y)\)</span> and marginal probabilities <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y)\)</span> is defined as follows:</p>
            <p><span class="math display">\[
                I(X,Y) = \sum_{x \in XX} \sum_{y \in YY} p(x,y) \cdot \log{\frac{p(x,y)}{p(x)p(y)}}
            \]</span></p>
            <p>This is the sum between the product of the joint probability and the logarithm of the ratio of the joint probability and the product of marginal probabilities, for all values of x and y.</p>
            <p>Alphabets XX and YY contain the possible values for X and Y, respectively.</p>
            <h5 class="unnumbered" data-number="" id="considerations-1">considerations</h5>
            <ul>
            <li><p>If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent the joint probability is the product of marginal probabilities, the ratio is equal to <span class="math inline">\(1\)</span> and the logarithm of one is <span class="math inline">\(0\)</span>.</p>
            <p>The mutual information will be 0.</p></li>
            <li><p>If we are far from the case of independence the joint probability is higher than the product and the mutual information increases.</p></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="properties">properties</h5>
            <ul>
            <li><p>The capacity of measuring any kind of relationship between variables</p></li>
            <li><p>Its invariance under space transformations (translations, rotations and any transformation that preserve the order of the original elements of the variables)</p></li>
            </ul>
            <blockquote>
            <p>Feature selection based on MI is extremely sensitive to the estimation of the pdfs.</p>
            </blockquote>
            <p>If I want to measure if attribute <span class="math inline">\(X\)</span> is correlated with another attribute and not the output <span class="math inline">\(Y\)</span>, I can still exploit this formula.</p>
            <p>Given an initial set <span class="math inline">\(F\)</span> with <span class="math inline">\(n\)</span> features, find subset <span class="math inline">\(S ⊂ F\)</span> with <span class="math inline">\(k\)</span> features that maximizes the <span class="math inline">\(MI I(C;S)\)</span> between the class variable <span class="math inline">\(C\)</span>, and the subset of selected features <span class="math inline">\(S\)</span>.</p>
            <h3 data-number="3.5.2" id="ni-normalized-mi-between-f_i-and-f_s"><span class="header-section-number">3.5.2</span> <span class="math inline">\(NI\)</span> := Normalized MI between <span class="math inline">\(f_i\)</span> and <span class="math inline">\(f_s\)</span>:</h3>
            <p><span class="math display">\[
                NI(f_i, f_s) = \frac{I(f_i;f_s)}{min\{H(f_i), H(f_s)\}}
            \]</span></p>
            <ul>
            <li><p>We divide by the minimum of the entropy of the two values, to reduce the effect of variables with an high number of values.</p></li>
            <li><p>it to varies between 0 and 1.</p></li>
            </ul>
            <p>The selection criterion used in NMIFS consists in selecting the feature that maximizes the measure G.</p>
            <p><span class="math display">\[
                G = I(C,f_i)-\frac{1}{S}\sum_{f_s \in S}{NI(f_i;f_s)}
            \]</span></p>
            <p><span class="math display">\[
                \text{where } I(C,f_i) = \sum_{c \in CC}\sum_{f_i \in FF} p(c, f_i) \cdot \log{\frac{p(c,f_i)}{p(c)p(f_i)}}
            \]</span></p>
            <ul>
            <li><p>We want high mutual information between feature and output but also low mutual information between the feature and the already selected feature.</p></li>
            <li><p><span class="math inline">\(f_i\)</span> is the features we are analyzing, and fs is a feature we already selected.</p></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="algorithm">algorithm</h5>
            <p>So, we want to maximize <span class="math inline">\(G\)</span>, it implements the intuitions we talked about before. This is the algorithm we can use:</p>
            <ol type="1">
            <li><p>Initialization: Set <span class="math inline">\(F = \{f_i \text{with} i = 1,...,N \}\)</span> initial set of <span class="math inline">\(N\)</span> features, and <span class="math inline">\(S = \{\emptyset\}\)</span>, empty set of selected features.</p></li>
            <li><p>Calculate the <span class="math inline">\(MI\)</span> with respect to the classes: calculate <span class="math inline">\(I(f_i ;C)\)</span>, for each <span class="math inline">\(f_i \in F\)</span>, each feature and the output.</p></li>
            <li><p>Select the first feature: Find <span class="math inline">\(i | f_i = \max_{i=1,...,N}{I(f_i; C)}\)</span>, maximum <span class="math inline">\(MI\)</span> between the feature and the output.</p>
            <p>move <span class="math inline">\(f_i\)</span> from <span class="math inline">\(F\)</span> to <span class="math inline">\(S\)</span></p></li>
            <li><p>Greedy Selection: Repeat until <span class="math inline">\(\#S = k\)</span>, a prefixed value before the algorithm starts.</p>
            <ul>
            <li><p>Calculate the <span class="math inline">\(MI\)</span> between features: Calculate <span class="math inline">\(I(f_i ;f_s )\)</span> for all pairs <span class="math inline">\((f_i ; f_s)\)</span>, with <span class="math inline">\(f_i \in F\)</span> and <span class="math inline">\(f_s \in S\)</span></p></li>
            <li><p>Select next feature: Select feature <span class="math inline">\(f_i \in F\)</span> that maximizes <span class="math inline">\(G\)</span>.</p>
            <p>move <span class="math inline">\(f_i\)</span> from <span class="math inline">\(F\)</span> to <span class="math inline">\(S\)</span></p></li>
            </ul></li>
            <li><p>Output the set <span class="math inline">\(S\)</span> containing the selected features.</p></li>
            </ol>
            <p>We don’t have metrics to determine the termination condition but we fix the number of features at the beginning.</p>
            <h4 class="unnumbered" data-number="" id="why-do-we-use-ni-that-is-fracmientropy">Why do we use <span class="math inline">\(NI\)</span> that is <span class="math inline">\(\frac{MI}{entropy}\)</span> ?</h4>
            <p>Suppose we want to encode and transmit a long sequence of symbols from the set <code>{a, c, e, g}</code> using the <strong>least number of bits</strong>, drawn randomly according to the following probability distribution D:</p>
            <table>
            <thead>
            <tr class="header">
            <th>Symbol</th>
            <th>a</th>
            <th>c</th>
            <th>e</th>
            <th>g</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><strong>Prob</strong></td>
            <td>1/8</td>
            <td>1/8</td>
            <td>1/4</td>
            <td>1/2</td>
            </tr>
            </tbody>
            </table>
            <p>Since there are 4 symbols, one possibility is to use 2 bits per symbol:</p>
            <table>
            <thead>
            <tr class="header">
            <th>Symbol</th>
            <th>Encoding</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>a</td>
            <td>00</td>
            </tr>
            <tr class="even">
            <td>c</td>
            <td>01</td>
            </tr>
            <tr class="odd">
            <td>e</td>
            <td>10</td>
            </tr>
            <tr class="even">
            <td>g</td>
            <td>11</td>
            </tr>
            </tbody>
            </table>
            <p>Because it’s possible to use 1.75 bit on average</p>
            <table>
            <thead>
            <tr class="header">
            <th>Symbol</th>
            <th>Encoding</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>a</td>
            <td>000</td>
            </tr>
            <tr class="even">
            <td>c</td>
            <td>001</td>
            </tr>
            <tr class="odd">
            <td>e</td>
            <td>01</td>
            </tr>
            <tr class="even">
            <td>g</td>
            <td>1</td>
            </tr>
            </tbody>
            </table>
            <blockquote>
            <p>We must select the shorting encoding for high probably symbols.</p>
            </blockquote>
            <p><span class="math display">\[
                \text{Average_number_of_bits_per_symbol} = \frac{1}{8} \cdot 3 + \frac{1}{8} \cdot 3 + \frac{1}{4} \cdot 2 + \frac{1}{2} \cdot 1 = 1.75
            \]</span></p>
            <h3 data-number="3.5.3" id="information-entropy"><span class="header-section-number">3.5.3</span> Information entropy</h3>
            <p>In the information theory they introduced the concept that <strong>optimal length code</strong> assigns <span class="math inline">\(\log_{2}{1/p} = -\log_2{p}\)</span> bits to a message having probability <span class="math inline">\(p\)</span>.</p>
            <p>Given a distribution <span class="math inline">\(D\)</span> over a finite set, where <span class="math inline">\(&lt;p_1, p_2, \dots, p_n&gt;\)</span> are the corresponding probabilities, define the <strong>information entropy</strong>, or information, of D by:</p>
            <p><span class="math display">\[
                H(D) = -\sum_{i=1}^{n}{p_i \log_2{p_i}}
            \]</span></p>
            <ul>
            <li><p>For example</p>
            <p>the entropy of the distribution we just examined, <span class="math inline">\(&lt;\frac{1}{8},\frac{1}{8}, \frac{1}{4}, \frac{1}{2}&gt;\)</span>, is 1.75 (bits)</p>
            <p>If all symbols have the same probability to be transmitted, the number of bits we must use in average is 2, our initial solution.</p></li>
            </ul>
            <blockquote>
            <p>The maximum value of entropy can be achieved when we assume all the symbols are equally probable.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="considerations-2">considerations</h4>
            <ul>
            <li><p>We have the lowest value of entropy when just one symbol is characterized by probability 1 and all others are characterized by probability 0.</p>
            <p>This corresponds, though, to the assumption that we transmit only one symbol and in that case, entropy is 0, we don’t actually transmit anything.</p></li>
            <li><p>If we have 5 symbols and the first has probability 1 and all the others 0, the first symbol contributes 0 for the logarithm and all others contributes 0 for the probability.</p></li>
            <li><p>If we have a group of objects that belong to the same class, if we want to analyze the entropy of this object referring to the class, we can compute the probability of having the classes inside the group of objects.</p>
            <p>In this case the entropy of the group of objects in term of probability of classes is equal to 0.</p>
            <p>The probability of one class is 1 and others are 0.</p></li>
            </ul>
            <h3 data-number="3.5.4" id="attribute-creation-feature-generation"><span class="header-section-number">3.5.4</span> Attribute creation (feature generation)</h3>
            <p>We can consider the creation of new attributes (features) that can capture the important information in a data set more effectively than the original ones.</p>
            <p>There are three general methodologies:</p>
            <ul>
            <li><p><strong>Attribute extraction</strong> that can be domain-specific. Feature selection can be automated, thing that can’t be done with feature extraction.</p></li>
            <li><p><strong>Mapping data to new space</strong> (see: data reduction), for example with Fourier transformation, wavelet transformation, manifold approaches (not covered).</p></li>
            <li><p><strong>Attribute construction</strong>: combining features and data discretization.</p></li>
            </ul>
            <h3 data-number="3.5.5" id="numerosity-reduction"><span class="header-section-number">3.5.5</span> Numerosity Reduction</h3>
            <p>It helps us to reduce complexity. The problem is not about computational power but it’s about storage.</p>
            <ul>
            <li><p><strong>Parametric</strong> methods (e.g., regression): in which we assume the data fits some model, estimate model parameters, we store only the parameters and discard the data (except possible outliers)</p></li>
            <li><p><strong>Non-parametric</strong> method, in which we do not assume models and try to summarize data using an approach. For instance, histograms, clustering, sampling.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="regression-and-log-linear-models">Regression and log-linear models</h4>
            <ul>
            <li><p><strong>Linear regression</strong>: in which data is modeled to fit a straight line and it often uses the least-square method to fit the line</p></li>
            <li><p><strong>Multiple regression</strong>: it allows a response variable Y to be modeled as a linear function of multidimensional feature vector</p></li>
            <li><p><strong>Log-linear model:</strong> in which we approximates discrete multidimensional probability distributions</p></li>
            </ul>
            <p>Used to estimate the probability of each point in a multi- dimensional space for a set of discretized attributes, based on a smaller subset of dimensional combinations.</p>
            <h3 data-number="3.5.6" id="regression-analysis"><span class="header-section-number">3.5.6</span> Regression analysis</h3>
            <p>A collective name for techniques for the modeling and analysis of numerical data consisting of values of a dependent variable and of one or more independent variables.</p>
            <p>We compute the parameters of the straight line in such a way the distance between the points and the line is minimum.</p>
            <p><img src="../media/image118.png" /></p>
            <h4 class="unnumbered" data-number="" id="linear-regression">Linear regression</h4>
            <p>Two regression coefficients, <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_0\)</span>, specify the line and are to be estimated by using the data at hand.</p>
            <p>We can use the least squares criterion to the known values of <span class="math inline">\(&lt;y_1, y_2, \dots&gt;, &lt;x1, x2, \dots&gt;\)</span></p>
            <p><span class="math display">\[
                w_i = \frac{\sum_{i=1}^{\#D}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^{\#D}{(x_i-\bar{x})^2}}
            \]</span></p>
            <p>where <span class="math inline">\(\#D\)</span> is the size of the dataset</p>
            <ul>
            <li><p>ex: the <span class="math inline">\(x_i\)</span> column shows scores on the aptitude test. Similarly, the <span class="math inline">\(y_i\)</span> column shows statistics grades.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image120.png" /></td>
            <td><img src="../media/image121.png" /></td>
            <td><img src="../media/image122.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>We want to find a straight line to approximate these values.</p>
            <ul>
            <li><p>Computation of <span class="math inline">\(w_1\)</span>: <code>w1 = 470 / 730 = 0.644</code></p></li>
            <li><p>Computation of <span class="math inline">\(w_0\)</span> : <code>w0 = 77 - 0.644 * 78 = 26.768</code></p></li>
            <li><p>Linear regression equation: <code>y = 0.644 x + 26.768</code></p></li>
            </ul></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="multiple-linear-regression">Multiple linear regression</h4>
            <p><span class="math display">\[
                y = w_0 + w_1 x_1 + w_2 x_2
            \]</span></p>
            <p>Many nonlinear functions can be transformed into the above</p>
            <p><span class="math display">\[
                W = (X^T X)^{-1}\  X^T \ Y
            \]</span></p>
            <h4 class="unnumbered" data-number="" id="polynomial-regression">Polynomial regression</h4>
            <p><span class="math display">\[
                y = w_0 + w_1 x + w_2 x^2 + w_3 x^3
            \]</span></p>
            <p>To convert this equation to a linear regression problem, we apply the following transformation:</p>
            <ul>
            <li><p><span class="math inline">\(x_1=x\)</span></p></li>
            <li><p><span class="math inline">\(x_2=x^2\)</span></p></li>
            <li><p><span class="math inline">\(x_3=x^3\)</span></p></li>
            </ul>
            <p>create a variable for each polynomial degree</p>
            <h4 class="unnumbered" data-number="" id="log-linear-models">Log-linear models</h4>
            <p>Mathematical models that take the form of functions whose logarithm is a first-degree polynomial function of the parameters of the model.</p>
            <p>Estimate the probability of each point (tuple) in a multi-dimensional space for a set of discretized attributes, based on a smaller subset of dimensional combinations.</p>
            <p>Useful for dimensionality reduction and data smoothing.</p>
            <p><span class="math display">\[
                exp(c+ \sum_{i}{w_if_i(X)})
            \]</span></p>
            <h3 data-number="3.5.7" id="histogram-analysis-1"><span class="header-section-number">3.5.7</span> Histogram analysis</h3>
            <p>In non-parametric case we use other analysis, for example <strong>histograms</strong>.</p>
            <p>We divide data into buckets and store average (sum) for each bucket.</p>
            <p>We can have different partitioning rules:</p>
            <ul>
            <li><p>Equal-width: equal bucket range</p></li>
            <li><p>Equal-frequency (or equal-depth)</p></li>
            </ul>
            <p><strong>histogran</strong></p>
            <div class="sourceCode" id="cb12"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="co">## the following data are a list of AllEletronics prices for commonly sold items (sorted)</span></span>
            <span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a></span>
            <span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a>prrices <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">8</span>, <span class="dv">8</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">14</span>, <span class="dv">14</span>, <span class="dv">14</span>, <span class="dv">15</span>, <span class="dv">15</span>, <span class="dv">15</span>, <span class="dv">15</span>, <span class="dv">15</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">18</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">21</span>, <span class="dv">21</span>, <span class="dv">21</span>, <span class="dv">21</span>, <span class="dv">25</span>, <span class="dv">25</span>, <span class="dv">25</span>, <span class="dv">25</span>, <span class="dv">25</span>, <span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">30</span>, <span class="dv">30</span>, <span class="dv">30</span>]</span></code></pre></div>
            <p><img src="../media/image126.png" /></p>
            <ul>
            <li><p>The first histogram is obtained using single buckets.</p></li>
            <li><p>In the second case we have equal-width histogram for price. We discretize the domain of the prize.</p></li>
            </ul>
            <h3 data-number="3.5.8" id="clustering"><span class="header-section-number">3.5.8</span> Clustering</h3>
            <p>Another way is <strong>clustering</strong>, in which we partition data set into clusters based on similarity, and store cluster representation (e.g., centroid and diameter) only.</p>
            <p>We can have <strong>hierarchical clustering</strong> and be stored in multi-dimensional index tree structures.</p>
            <h3 data-number="3.5.9" id="sampling"><span class="header-section-number">3.5.9</span> Sampling</h3>
            <p>Obtaining a small samples to represent the whole data set.</p>
            <p>We want to choose a representative subset of the data. It allows a mining algorithm to run in complexity that is potentially sub-linear to the size of the data.</p>
            <p>Key principle: Choose a representative subset of the data.</p>
            <p>Sampling techniques can be devided into:</p>
            <ul>
            <li><p><strong>Simple random sampling</strong>: there is an equal probability of selecting any particular item</p>
            <ul>
            <li><p><strong>Sampling without replacement</strong>: once an object is selected, it is removed from the population. We put the object in the selected set and we eliminate the object from the original repository. In this case we change the probability, each time I extract an object and eliminate it from the repository I change the probability in the original repository.</p></li>
            <li><p><strong>Sampling with replacement</strong>: a selected object is not removed from the population. We can see that like making a copy of the object.</p>
            <p>Each time that we perform sampling we extract an instance with equal probability, we con’t change the probability of extracting objects. This has the advantage that in my repository</p></li>
            </ul></li>
            <li><p><strong>Stratified sampling</strong>: partition the data set, and draw samples from each partition.</p>
            <p>Used in conjunction with skewed data (also the smaller group of items will be sure to be represented) I generate this stratification and extract data from each partition I have.</p>
            <p>If I have some skew, I’ll dedicate a partition for the skew and be sure that I have representative for the skew.</p>
            <p>In stratified sampling we don’t have the risk to have only data from a majority class, if we have majority and minority class. We can be sure that also instances from the minority class will be selected.</p></li>
            </ul>
            <p>When we sample with replacement, the two sample values are independent.</p>
            <p>What we get on the first one doesn’t affect what we get on the second. The covariance between the two is zero.</p>
            <p>In sampling without replacement, the two sample values aren’t independent. What we got on for the first one affects what we can get for the second one.</p>
            <p>The covariance between the two, from a population with variance <span class="math inline">\(\sigma^2\)</span>, is:</p>
            <p><span class="math display">\[
                \frac{-\sigma^2}{N-1}
            \]</span></p>
            <p>In case of SRSWOR we extract three items and we have just the probability to extract the same objects, we don’t eliminate the same objects, so we have the probability to extract the same objects.</p>
            <p><img src="../media/image128.png" /> <img src="../media/image129.png" /></p>
            <p>In stratifies sampling we can partition someway the raw data and we are sure that we extract data from each partition.</p>
            <p><img src="../media/image130.png" /></p>
            <p>In SRSWOR all records are different, in SRSWR in the subset we can have two records equal because we don’t eliminate records from the repository.</p>
            <p>In stratified sample we have an organization in partition, and we guarantee that we extract an item, at least, from each partition. We try to preserve the distribution that we have for the partition.</p>
            <h3 data-number="3.5.10" id="data-cube-aggregation"><span class="header-section-number">3.5.10</span> Data Cube Aggregation</h3>
            <p>A typical representation we have in a data warehouse, useful to perform analysis, is <strong>data cube</strong>.</p>
            <p>In a cube of data we can represents different attributes with different granularity.</p>
            <p><img src="../media/image131.png" /></p>
            <p>Instead of representing data in terms of months or weeks or days we can represent them in form of year, to have a summarized version of data.</p>
            <p>It’s like we have an hierarchy, we can have multiple levels of aggregation in data cubes.</p>
            <h2 data-number="3.6" id="data-compression"><span class="header-section-number">3.6</span> Data Compression</h2>
            <p>We can consider them in two groups:</p>
            <ul>
            <li><p><strong>lossless</strong> data compression, we compress data but when we uncompress we obtain the perfect original copy</p></li>
            <li><p><strong>lossy</strong> data compression, when we uncompress the data we don’t recover the same data.</p></li>
            </ul>
            <p><img src="../media/image132.png" /></p>
            <blockquote>
            <p>If we have lossless algorithms, why don’t we use always them? &gt; When we use lossy compression, we achieve an higher compression ratio, we compress more the data.</p>
            </blockquote>
            <h2 data-number="3.7" id="data-transformation-and-data-discretization"><span class="header-section-number">3.7</span> Data Transformation and Data Discretization</h2>
            <p><strong>Data transformation</strong> is a function that maps the entire set of values of a given attribute to a new set of replacement values such that each old value can be identified with one of the new values.</p>
            <p>There are different methods:</p>
            <ul>
            <li><p><strong>Smoothing</strong>: Remove noise from data</p></li>
            <li><p><strong>Attribute/feature construction</strong></p></li>
            <li><p><strong>Aggregation</strong>: Summarization, data cube construction</p></li>
            <li><p><strong>Normalization</strong>: Scaled to fall within a smaller, specified range (min-max normalization, z-score normalization, normalization by decimal scaling)</p></li>
            <li><p><strong>Discretization</strong>: Concept hierarchy climbing</p></li>
            </ul>
            <h3 data-number="3.7.1" id="min-max-normalization"><span class="header-section-number">3.7.1</span> Min-max normalization</h3>
            <p>We transform all data in a way that they will vary from <span class="math inline">\([new\_minA, new\_maxA]\)</span>.</p>
            <p><span class="math inline">\(v^{&#39;}\)</span> is the new data.</p>
            <p><span class="math display">\[
                v^{&#39;} = \frac{v-min_A}{max_A-min_A} (new\_max_A -new\_min_A) + new\_min_A
            \]</span></p>
            <p>This normalization suffers for a type of problem.</p>
            <blockquote>
            <p>If we have an outlier which is a minimum and it is far away from the others, all points will be mapped in a narrow zone between 0 and 1.</p>
            </blockquote>
            <p>To avoid this problem, we typically exploit another normalization.</p>
            <h3 data-number="3.7.2" id="z-score-normalization"><span class="header-section-number">3.7.2</span> Z-score normalization</h3>
            <p>We don’t use minimum and maximum, but we use μ (mean) and σ (standard deviation).</p>
            <p><span class="math display">\[
                v^{&#39;}= \frac{v-\mu}{\sigma_A}
            \]</span></p>
            <p>In this normalization values will not vary in a fixed range, but they will vary in similar ranges, thanks to the denominator, which is the std and depends how much data are spread in the original domain.</p>
            <p>The <code>min/max normalization</code> guarantee us that all features vary in the same range but suffers for the problem of outliers, while <code>z-score</code> normalization does not suffer from this problem.</p>
            <h3 data-number="3.7.3" id="normalization-by-decimal-scaling"><span class="header-section-number">3.7.3</span> Normalization by decimal scaling</h3>
            <p>The new value is obtained by the ratio of the old value and this power of <span class="math inline">\(j\)</span>:</p>
            <p><span class="math display">\[
                v^{&#39;} = \frac{v}{10^j}
            \]</span></p>
            <p>where <span class="math inline">\(j\)</span> is the smallest integer such that <span class="math inline">\(max(|v^{&#39;}|) &lt; 1\)</span>.</p>
            <h3 data-number="3.7.4" id="discretization"><span class="header-section-number">3.7.4</span> Discretization</h3>
            <p>We apply discretization when we have a continuous value, and we want to discretize it. It consists in dividing the range of a continuous attribute into intervals.</p>
            <p>Some <code>ML</code> techniques are applicable only on nominal data and not on continuous data. Interval labels can then be used to replace actual data values.</p>
            <blockquote>
            <p>Finding correct intervals is important and we need to have approaches to discretize in such a way to guarantee good results in term of application of ML techniques.</p>
            </blockquote>
            <p>We also reduce data size by discretization.</p>
            <h4 class="unnumbered" data-number="" id="discretization-techniques">discretization techniques</h4>
            <ul>
            <li><p><strong>Supervised</strong>, uses information on the class. We try to divide them in such a way that points belonging to the same class are in the same interval.</p></li>
            <li><p><strong>Unsupervised</strong></p></li>
            </ul>
            <p>It can also be:</p>
            <ul>
            <li><p><strong>Split (top-down)</strong>, we start from a interval and then we split</p></li>
            <li><p><strong>Merge (bottom-up)</strong>, where we try to merge intervals</p></li>
            </ul>
            <p>Discretization can be performed recursively on an attribute.</p>
            <blockquote>
            <p>Discretization can be applied recursevly</p>
            </blockquote>
            <p>Typical methods (All the methods can be applied recursively):</p>
            <ul>
            <li><p><strong>Binning</strong>: top-down split, unsupervised</p></li>
            <li><p><strong>Histogram analysis</strong>: top-down split, unsupervised</p></li>
            <li><p><strong>Clustering analysis</strong>: unsupervised, top-down split or bottom-up merge</p></li>
            <li><p><strong>Decision-tree analysis</strong>: supervised, top-down split</p></li>
            <li><p><strong>Correlation analysis</strong>: unsupervised, bottom-up merge</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="binning-1">Binning</h4>
            <p>We have two types of binning:</p>
            <ul>
            <li><p><strong>Equal-width (distance)</strong> partitioning</p>
            <p>We divide the range into <span class="math inline">\(N\)</span> intervals of equal size: uniform grid.</p>
            <p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are the lowest and highest values of the attribute, the width of intervals will be:</p>
            <p><span class="math display">\[
                W = \frac{(B - A)}{N}. 
            \]</span></p>
            <p>The most straightforward, but outliers may dominate presentation. Skewed data is not handled well because we have intervals of the same size.</p></li>
            <li><p><strong>Equal-depth (frequency)</strong> partitioning</p>
            <p>We divide the range into <span class="math inline">\(N\)</span> intervals, each containing approximately same number of samples.</p>
            <p>Good data scaling is possible.</p>
            <p>Managing categorical attributes can be tricky because we can just put more label together if we consider the same frequency.</p></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="example-1">Example</h5>
            <div class="sourceCode" id="cb13"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a><span class="co">## sorted data for price ($)</span></span>
            <span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a></span>
            <span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a>prices <span class="op">=</span> [<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">15</span>, <span class="dv">21</span>, <span class="dv">21</span>, <span class="dv">24</span>, <span class="dv">25</span>, <span class="dv">26</span>, <span class="dv">26</span>, <span class="dv">29</span>, <span class="dv">34</span>]</span>
            <span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a></span>
            <span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a>eqy_depth_bins <span class="op">=</span> [</span>
            <span id="cb13-6"><a href="#cb13-6" aria-hidden="true"></a>    [<span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">15</span>]</span>
            <span id="cb13-7"><a href="#cb13-7" aria-hidden="true"></a>    [<span class="dv">21</span>, <span class="dv">21</span>, <span class="dv">24</span>, <span class="dv">25</span>]</span>
            <span id="cb13-8"><a href="#cb13-8" aria-hidden="true"></a>    [<span class="dv">26</span>, <span class="dv">18</span>, <span class="dv">29</span>, <span class="dv">34</span>]</span>
            <span id="cb13-9"><a href="#cb13-9" aria-hidden="true"></a>]</span>
            <span id="cb13-10"><a href="#cb13-10" aria-hidden="true"></a></span>
            <span id="cb13-11"><a href="#cb13-11" aria-hidden="true"></a><span class="co">## smoothing by bin means we obtain</span></span>
            <span id="cb13-12"><a href="#cb13-12" aria-hidden="true"></a>bin_means_smoothed_bins <span class="op">=</span> [</span>
            <span id="cb13-13"><a href="#cb13-13" aria-hidden="true"></a>    [<span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">9</span>, <span class="dv">9</span>]</span>
            <span id="cb13-14"><a href="#cb13-14" aria-hidden="true"></a>    [<span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">23</span>, <span class="dv">23</span>]</span>
            <span id="cb13-15"><a href="#cb13-15" aria-hidden="true"></a>    [<span class="dv">29</span>, <span class="dv">29</span>, <span class="dv">29</span>, <span class="dv">29</span>]</span>
            <span id="cb13-16"><a href="#cb13-16" aria-hidden="true"></a>]</span>
            <span id="cb13-17"><a href="#cb13-17" aria-hidden="true"></a></span>
            <span id="cb13-18"><a href="#cb13-18" aria-hidden="true"></a><span class="co">## smoothing by bin boundaries we obtain</span></span>
            <span id="cb13-19"><a href="#cb13-19" aria-hidden="true"></a>bin_means_smoothed_bins <span class="op">=</span> [</span>
            <span id="cb13-20"><a href="#cb13-20" aria-hidden="true"></a>    [<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">15</span>]</span>
            <span id="cb13-21"><a href="#cb13-21" aria-hidden="true"></a>    [<span class="dv">21</span>, <span class="dv">21</span>, <span class="dv">25</span>, <span class="dv">25</span>]</span>
            <span id="cb13-22"><a href="#cb13-22" aria-hidden="true"></a>    [<span class="dv">26</span>, <span class="dv">26</span>, <span class="dv">26</span>, <span class="dv">34</span>]</span>
            <span id="cb13-23"><a href="#cb13-23" aria-hidden="true"></a>]</span></code></pre></div>
            <p>Let’s see a comparison between different approaches of discretization.</p>
            <p><img src="../media/image140.png" /></p>
            <h5 class="unnumbered" data-number="" id="conclusions-1">conclusions</h5>
            <ul>
            <li><p>With equal width binning we just split the domain in equal-width intervals.</p></li>
            <li><p>Points belonging to the same class may be split in different intervals.</p></li>
            <li><p>In the case of equal frequency, the situation is similar, we split points belonging to the same class in different intervals.</p></li>
            </ul>
            <p>This situation can improve if we exploit cluster algorithms, that try to include in the same cluster similar points and the similarity is measured considering the distance between points exploiting the fact they’re close in the space.</p>
            <h4 class="unnumbered" data-number="" id="we-can-have-discretization-performed-in-a-supervised-way">We can have discretization performed in a supervised way:</h4>
            <ul>
            <li><p><strong>Classification</strong></p>
            <ul>
            <li>Using entropy to determine split point (discretization point)</li>
            <li>Top-down, recursive split</li>
            <li>Details to be covered in the following</li>
            </ul></li>
            <li><p><strong>Correlation analysis</strong> (e.g., Chi-merge: χ2-based discretization)</p>
            <ul>
            <li><p>Bottom-up merge: find the best neighboring intervals (those having similar distributions of classes, i.e., low χ2 values) to merge</p></li>
            <li><p>Merge performed recursively, until a predefined stopping condition</p></li>
            </ul></li>
            </ul>
            <h3 data-number="3.7.5" id="chimerge-discretization"><span class="header-section-number">3.7.5</span> ChiMerge Discretization</h3>
            <p>It is a statistical approach to data discretization.</p>
            <p>We want to create intervals in which we have points belonging to the same class.</p>
            <p>It applies the <code>Chi Square</code> method to determine the probability of similarity of data between two intervals. Let’s suppose we have different features <span class="math inline">\(F\)</span> and two classes <span class="math inline">\(K\)</span>.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image141.png" /></td>
            <td><img src="../media/image142.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>We start to sort and order the attributes that we want to group (in this example attribute <span class="math inline">\(F\)</span>).</p>
            <p>We start with having every unique value in the attribute in its own interval.</p>
            <p>This is a bottom-up approach in fact.</p>
            <p>The upper boundary is the point in the middle between a value and the next.</p>
            <p>We begin calculating the Chi Square test on every interval and see if we can merge close intervals. We compute the chi-square considering merging two intervals.</p>
            <p><img src="../media/image143.png" /></p>
            <p>The values we have in the cell are observed values.</p>
            <p><img src="../media/image144.png" /></p>
            <p>The expected value is obtained by considering the frequency of the sample and the frequency of the class.</p>
            <p>We get 0 and 2 for the chi-square. Having 0 means that samples and classes are independent, the value of samples change but the class remain the same.</p>
            <p>In the first class we have correlation, if samples change the class change.</p>
            <p>We consider a threshold <span class="math inline">\(0.1\)</span> with <span class="math inline">\(d_f = 1\)</span> from <code>Chi-square</code> distribution chart and we merge if <span class="math inline">\(X^2 \lt 2.7024\)</span>.</p>
            <p>We calculate chi-values for all intervals and merge all intervals with the smallest chi-values because we are that we have independence between the classes and the samples.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image145.png" /></td>
            <td><img src="../media/image146.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>Now we repeat the computation and we get:</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image147.png" /></td>
            <td><img src="../media/image148.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>And we go on until there’s no interval to satisfy the chi-square test.</p>
            <p>In most intervals almost all samples belong to the same class.</p>
            <p>We merge points inside one interval in a way we have a predominant class of membership of these points. This approach is heavy if we have a large dataset.</p>
            <h3 data-number="3.7.6" id="concept-hierarchy-generation"><span class="header-section-number">3.7.6</span> Concept Hierarchy Generation</h3>
            <p>Concept hierarchy organizes concepts hierarchically and is usually associated with each dimension in a data warehouse.</p>
            <p>The concept hierarchies can be used to transform the data into multiple levels of granulaity.</p>
            <p>Concept hierarchy formation: Recursively reduce the data by collecting and replacing low level concepts by higher level concepts grouping low level concepts.</p>
            <p>Concept hierarchy can be automatically formed for both numeric and nominal data. For numeric data, use discretization methods shown.</p>
            <p>We study four methods for the generation of concept hierarchies for nominal data, as follows.</p>
            <ul>
            <li><p><strong>specification of a partial/total ordering</strong> of attributes explicitly at the schema level by users or experts. A hierarchy can be defined by specifying the total ordering among attributes at the schema level such as:</p>
            <p>street &lt; city &lt; state &lt; country</p>
            <blockquote>
            <p>Country is the highest level of hierarchy, that contains the state, that contains the city, that contains the street.</p>
            </blockquote></li>
            <li><p><strong>specification of a hierarchy</strong> for a set of values by explicit data grouping. In a large database, it is unrealistic to define an entire concept hierarchy by explicit value enumera- tion. On the contrary, we can easily specify explicit groupings for a small portion of intermediate-level data.</p>
            <p>{Urbana, Champaign, Chicago} &lt; Illinois</p></li>
            <li><p><strong>specification of only a partial set of attributes</strong>. Sometimes a user can be careless when defining a hierarchy or have only a vague idea about what should be included in a hierarchy. Consequently, the user may have included only a small subset of the relevant attributes in the hierarchy specification.</p>
            <p>ex: only street &lt; city, not others</p></li>
            <li><p><strong>automatic generation of hierarchies</strong> (or attribute levels) by the analysis of the number of distinct values, considering the numerosity for each of the values we are considering.</p>
            <p>ex: for a set of attributes: {street, city, state, country}</p></li>
            </ul>
            <h3 data-number="3.7.7" id="automatic-concept-hierarchy-generation"><span class="header-section-number">3.7.7</span> Automatic Concept Hierarchy Generation</h3>
            <p>Some hierarchies can be automatically generated based on the analysis of the number of distinct values per attribute in the data set.</p>
            <p>The attribute with the most distinct values is placed at the lowest level of the hierarchy.</p>
            <p>Exceptions, e.g., weekday, month, quarter, year</p>
            <p><img src="../media/image149.png" /></p>
            <p>Depending on specific application domain you need to use the best techniques for preprocessing.</p>
            <h1 data-number="4" id="association-rule"><span class="header-section-number">4</span> Association Rule</h1>
            <h2 data-number="4.1" id="mining-frequent-patterns"><span class="header-section-number">4.1</span> Mining Frequent Patterns</h2>
            <p>A pattern is a set of items (milk and bread which appear together in a transaction data set).A subsequence (buy first a PC, then a digital camera and then a memory card) is also a pattern but with a difference, the order is relevant.A substructure is a pattern that refers to different structural forms (subgraphs, subtrees).</p>
            <p>A pattern is frequent if it occurs frequently in a data set.</p>
            <p>Finding such frequent patterns plays an essential role in mining associations, correlations and other interesting relationships and we talk about frequent pattern analysis.</p>
            <h3 data-number="4.1.1" id="basic-concepts"><span class="header-section-number">4.1.1</span> Basic concepts</h3>
            <p>We define:</p>
            <ul>
            <li><p><strong>Itemset I</strong>: A set of one or more items <span class="math inline">\(\{I_1, I_2, ..., I_m\}\)</span>.</p></li>
            <li><p><strong>k-itemset X</strong> = <span class="math inline">\(\{x_1, x_2, ..., x_k\}\)</span>, set of <span class="math inline">\(k\)</span> items.</p></li>
            <li><p><strong>(absolute) support</strong>, or, <strong>support count of X</strong>: the frequency or occurrence of an itemset X in the database of transactions, number of transactions in which the itemset appears.</p></li>
            <li><p><strong>(relative) support s</strong>: fraction of transactions that contains X (i.e., the probability that a transaction contains X).</p>
            <p><img src="../media/image150.png" /></p>
            <p>The 1-itemset (one element) beer appear 3/5, the absolute support of this set is 3. The relative support is 3/5. The absolute support of Beer, Diaper is 3. The relative support is 3/5.</p>
            <p>The absolute support for Milk is 2. The relative support is 2/5.</p></li>
            <li><p>An itemset X is <strong>frequent</strong> if X’s support is no less than a <strong>minsup threshold</strong>.</p>
            <p>To fix the minsup we need to consider that we are not interested in 1-itemset, we will choose a good number.</p></li>
            </ul>
            <h3 data-number="4.1.2" id="purpose-of-frequent-pattern-analysis"><span class="header-section-number">4.1.2</span> Purpose of Frequent Pattern Analysis</h3>
            <p>The aim of frequent pattern analysis is to mine itemset. Larger is the cardinality of the itemset and more interesting is what we can discover.</p>
            <p>The complexity is exponential, we need specific techniques to reduce complexity.</p>
            <p>After fixing the minsup these techniques will give us all frequent patterns.</p>
            <h2 data-number="4.2" id="frequent-pattern-matching"><span class="header-section-number">4.2</span> Frequent Pattern Matching</h2>
            <p>When we have a set of many items working on all possible combinations is hard.</p>
            <p>We must mine all the possible frequent patterns.</p>
            <h3 data-number="4.2.1" id="transactions"><span class="header-section-number">4.2.1</span> Transactions</h3>
            <p>A transaction T is a set of items such that:</p>
            <p><span class="math display">\[
                T \subseteq I
            \]</span></p>
            <h3 data-number="4.2.2" id="association-rule-1"><span class="header-section-number">4.2.2</span> Association Rule</h3>
            <p>An association rule is an implication of the form X <span class="math inline">\(\Rightarrow\)</span> Y, where</p>
            <ul>
            <li><p><span class="math inline">\(X \subset I\)</span></p></li>
            <li><p><span class="math inline">\(Y \subset I\)</span></p></li>
            <li><p><span class="math inline">\(X \cap Y = \emptyset\)</span></p></li>
            </ul>
            <p>The association rule means that there’s an high probability that X will imply Y.</p>
            <h4 class="unnumbered" data-number="" id="definitions">Definitions</h4>
            <p><strong>The relative support of X <span class="math inline">\(\Rightarrow\)</span> Y in the transaction database D</strong>: percentage of transactions in D that contain <span class="math inline">\(X \cup Y\)</span> (i.e., the union of sets X and Y)</p>
            <p><strong>Support:</strong> <span class="math inline">\(Supp (X \Rightarrow Y) = P(X \cup Y) \rightarrow\)</span> probability that a transaction contains <span class="math inline">\(X \cup Y\)</span></p>
            <p><strong>Confidence:</strong> <span class="math inline">\(Conf (X \Rightarrow Y) = P(Y | X) = \frac{Supp(X \cup Y)}{Supp(X)} \rightarrow\)</span> conditional probability that a transaction having X also contains Y and it is given by this ratio.</p>
            <p>This ratio belongs to the interval [0,1], in fact it is a probability, and that’s because: <span class="math display">\[
                supp(X \cup Y) \Leftarrow supp(X)
            \]</span> If <span class="math inline">\(Conf(X \Rightarrow Y) = 1\)</span>, every time I have X I also have Y.</p>
            <p><strong>All the rules with confidence 1 are optimal for us.</strong></p>
            <p>But a special case there is when Y is always present in the transaction it is not that optimal, it doesn’t mean that when we have X we also have Y, it’s because Y is always present.</p>
            <p>This is the reason why confidence is important, but we need to consider also the probability of Y.</p>
            <h3 data-number="4.2.3" id="implementation"><span class="header-section-number">4.2.3</span> Implementation</h3>
            <p>Each item has a boolean variable representing the presence or absence of that item.</p>
            <p>Each basket can be represented by a Boolean vector of values assigned to these variables.</p>
            <p>We can represent each transaction as boolean vector which is our basket.</p>
            <p>Boolean vectors analyzed for buying patterns that reflect items that are frequently associated or purchased together.</p>
            <p>Computer <span class="math inline">\(\Rightarrow\)</span> antivirus_software [Support=2%,Confidence =60%]</p>
            <p><strong>Support reflects the usefulness</strong> (in the 2% of all transactions, purchased together). <strong>Confidence reflects certainty</strong> (60% of the customers who purchased a computer also bought the software).</p>
            <p>A typical problem we have to solve is that given a database of transactions.</p>
            <p>We need to find all the rules X <span class="math inline">\(\Rightarrow\)</span> U with minimum support and confidence, fixed the minimum values.</p>
            <p>Let’s come back to the previous example.</p>
            <p><img src="../media/image150.png" /></p>
            <p>Let <strong>minsup</strong> = 50%, <strong>minconf</strong> = 50%</p>
            <p>Freq. itemsets: Beer:3, Nuts:3, Diaper:4, Eggs:3, {Beer, Diaper}:3</p>
            <p>They’re frequent because we fixed minsup to 50% (3/5 and 4/5 &gt; 0.5).</p>
            <p>Beer is a 1-itemset frequent, {Beer, Diaper} is a 2-itemset frequent, and so on.</p>
            <p>We can extract now association rules:</p>
            <p>Beer <span class="math inline">\(\Rightarrow\)</span> Diaper (60%, 100%)</p>
            <p>Diaper <span class="math inline">\(\Rightarrow\)</span> Beer (60%, 75%)</p>
            <p>The <strong>support</strong> is the same because it is defined as the number of transactions in which we have the two items together.</p>
            <p>The <strong>confidence</strong> is different because obtained by dividing the support for the support of the antecedent.</p>
            <h4 class="unnumbered" data-number="" id="process">Process</h4>
            <p>In general, association rule mining can be viewed as a two-step process:</p>
            <ul>
            <li><p><strong>Find all frequent itemsets</strong>: By definition, each of these itemsets will occur at least as frequently as a predetermined minimum support count, minsup, once fixed minsup.</p></li>
            <li><p><strong>Generate strong association rules from the frequent itemsets</strong>: By definition, these rules must satisfy minimum support (automatically satisfied because we start from frequent itemsets) and minimum confidence.</p></li>
            </ul>
            <p>A long itemset contains a combinatorial number of sub-itemsets, e.g., we have 100 itemsets <span class="math inline">\(\{a_1, ..., a_{100}\}\)</span>. The number of sub-itemset is:</p>
            <p><span class="math display">\[
                \left(\begin{array}{c}
                    100 \\
                    1
                \end{array}\right)
             + \left(\begin{array}{c}
                    100 \\
                    2
                \end{array}\right) + ... + \left(\begin{array}{c}
                    100 \\
                    100
                \end{array}\right) = 2^{100} - 1 \approx 1.27 \times 10^{30} \hspace{1cm} \left( \dfrac[0pt]{n}{k} \right) = \dfrac{n!}{k!(n-k)!}
            \]</span></p>
            <p>We need to find a solution to show all frequent patterns we can mine to users by summarizing.</p>
            <p>The way to summarize the frequent patterns is showing a subset of frequent pattern we mined.</p>
            <h4 class="unnumbered" data-number="" id="new-problem">New problem</h4>
            <p>A long itemset contains a huge number of sub-items.</p>
            <p><strong>Solution</strong>: Mine closed itemsets and max-itemsets instead</p>
            <ul>
            <li><p>An itemset X is <strong>closed</strong> in a dataset D if X is frequent and there exists no super-itemset <span class="math inline">\(Y \supset X\)</span>, with the same support as X <span class="math inline">\(\rightarrow\)</span> this means that we can give the customer only 1 itemset specifying that it is closed.</p></li>
            <li><p>An itemset X is a <strong>max-itemset</strong> in a dataset D if X is frequent and there exists no frequent super-itemset <span class="math inline">\(Y \supset X\)</span>, but we do not specify nothing about support.</p></li>
            </ul>
            <p>If we have the max-itemset we have all the information we need.</p>
            <p>Closed itemset is a lossless compression of freq. patterns, it allow us to reduce the number of patterns and rules</p>
            <p>For example, we have two subsets one containing 100 items and the other 50 items.</p>
            <p>DB = {&lt;<span class="math inline">\(a_1\)</span>, ..., <span class="math inline">\(a_{100}\)</span>&gt;, &lt; <span class="math inline">\(a_1\)</span>, ..., <span class="math inline">\(a_{50}\)</span>&gt;}</p>
            <p>Min_sup = 1.</p>
            <ul>
            <li><p>What is the set of closed itemset?</p>
            <p>&lt;<span class="math inline">\(a_1\)</span>, ..., <span class="math inline">\(a_{100}\)</span>&gt; closed with support 1</p>
            <p>&lt; <span class="math inline">\(a_1\)</span>, ..., <span class="math inline">\(a_{50}\)</span>&gt; closed with support 2</p></li>
            <li><p>What is the set of max-itemset?</p>
            <p>&lt;<span class="math inline">\(a_1\)</span>, ..., <span class="math inline">\(a_{100}\)</span>&gt;: 1</p></li>
            </ul>
            <p>Giving only the max-itemset we lose information, we cannot know that there exists the other itemset with support 2.</p>
            <p>The set of closed frequent itemsets contains, instead, complete information regarding the frequent itemsets.</p>
            <p>For instance, from the set of closed frequent itemset, we can derive {<span class="math inline">\(a_2\)</span>, <span class="math inline">\(a_{45}\)</span> : 2};</p>
            <p>From the data set, we can only infer that {<span class="math inline">\(a_2\)</span>, <span class="math inline">\(a_{45}\)</span> : 1}</p>
            <h4 class="unnumbered" data-number="" id="computational-complexity">Computational Complexity</h4>
            <p>Let C be the set of closed frequent itemsets for a data set D satisfying a minimum support threshold, min_sup.</p>
            <p>Let M be the set of maximal frequent itemsets for D satisfying min_sup.</p>
            <p>Suppose that we have the support count of each itemset in C and M. Notice that C and its count information can be used to derive the whole set of frequent itemsets. Thus, we say that C contains complete information regarding its corresponding frequent itemsets.</p>
            <p>For the computational complexity it’s impossible to generate all possible combinations.</p>
            <p>How many itemsets are potentially to be generated in the worst case?</p>
            <p>The number of frequent itemsets to be generated is sensitive to the minsup threshold.</p>
            <p>When minsup is low, there exist potentially an exponential number of frequent itemsets.The worst case: MN where M is the number of distinct items, and N is the max length of transactions.</p>
            <p>Let’s see the worst case complexity vs. the expected probability .</p>
            <p>Ex. Suppose Walmart has <span class="math inline">\(10^4\)</span> kinds of products.The chance to pick up one product is <span class="math inline">\(10^{-4}\)</span>.The chance to pick up a particular set of 10 products is <span class="math inline">\(\sim 10^{-40}\)</span> because we consider independent products.</p>
            <p>We can see that with the increase of M in the transaction the probability to have this itemset frequent decreases.</p>
            <p>This is the basis of the most popular algorithm used for frequent pattern mining.</p>
            <h2 data-number="4.3" id="apriori-algorithm"><span class="header-section-number">4.3</span> Apriori Algorithm</h2>
            <p><strong>Apriori algorithm</strong> is an approach in which we have candidate generation-and-test.</p>
            <p>The <em>downward closure property</em> of frequent patterns tells us that any nonempty subset of a frequent itemset must also be frequent. (Apriori property) It tells us that if we have that {beer, diaper, nuts} is frequent, so is {beer, diaper} i.e., every transaction having {beer, diaper, nuts} also contains {beer, diaper} .</p>
            <p>If we add another item to a frequent itemset we at most have the same support but cannot increase it.This property belongs to a special category of properties called <em>antimonotonicity</em> in the sense that if a set cannot pass a test, all of its supersets will fail the same test as well. Antimonotonicity because the property is monotonic in the context of failing a test.</p>
            <p>If a 2-itemset is not frequent it’s useless to test a 3-itemset containing the 2-itemset.The reason is the apriori property.</p>
            <h3 data-number="4.3.1" id="apriori-pruning-principle"><span class="header-section-number">4.3.1</span> Apriori Pruning Principle</h3>
            <p><strong>Apriori pruning principle</strong>: If there is any itemset which is infrequent, its superset should not be generated/tested!</p>
            <p>In fact, we are sure that this super-test is not frequent.</p>
            <p>Method:</p>
            <ul>
            <li><p>Initially, scan DB once to get frequent 1-itemset, in this way for the apriori property if we discover than 1 item is not frequent, we can remove the item for the generation of the higher order itemset.</p></li>
            <li><p><strong>Generate</strong> length (k+1) <strong>candidate</strong> itemsets from length k <strong>frequent</strong> itemsets.</p></li>
            </ul>
            <p>At the beginning we generate the two itemset by combining the 1-itemset.</p>
            <ul>
            <li><p><strong>Test</strong> the candidates against DB, we check if they’re frequent</p></li>
            <li><p>Terminate when no frequent or candidate set can be generated</p></li>
            </ul>
            <p>We stop certainly because with the increasing of number of items in the itemset, the support decreases.</p>
            <p>At the end we will have a value lower than minsup.</p>
            <p>The minsup affects a lot this process, if we choose an high minsup the process will terminate fastly, while if it is small we’ll generate a lot of itemsets and we have to check a lot of times that they’re frequent.</p>
            <p>With an high-value the complexity decreases but we may not get what we want.</p>
            <p>It has to be chosen not depending on the complexity.</p>
            <p>When we start the method the minsup is fixed.</p>
            <p><img src="../media/image155.png" /></p>
            <p>We start from our db that may contain some transactions.</p>
            <p>We count the occurrence of the single item to establish if it is frequent or not.</p>
            <p>If we assume minsup = 2, D is not a frequent pattern so we can eliminate if from the 1-itemsets frequent. We denote with Lk the frequent k-itemset.</p>
            <p>With Ck we denotate the candidate k-itemset.</p>
            <p>Now we generate all possible candidate 2-itemset and we consider, for the apriori property, all the 1-itemset frequent.</p>
            <p>We consider {A, B} and not {B, A} because we are not interested in the order but only on occurrences.</p>
            <p>We scan the DB to realize if these 2-itemsets are frequent.</p>
            <p>We continue to discover super-items frequent of the 2-itemset, generating the 3 candidate itemsets, combining the two frequent itemset.</p>
            <p>From the first two frequent 2-itemset we can generate {A, B, C} but the support of one subset is equal to 1, and for the apriori property if one of subsets is not frequent also the super-itemset is not frequent. For {A, B, E} and {A, C, E} we can do the same considerations.</p>
            <p>The only combination that we are not able to discard is {B, C, E}, it has 2 as support.</p>
            <h3 data-number="4.3.2" id="pseudo-code"><span class="header-section-number">4.3.2</span> Pseudo Code</h3>
            <p><span class="math inline">\(C_k\)</span>: Candidate itemset of size k</p>
            <p><span class="math inline">\(L_k\)</span> : frequent itemset of size k</p>
            <p><span class="math inline">\(L_1\)</span> = {frequent items};</p>
            <p>for (k = 1; <span class="math inline">\(L_k\)</span> != <span class="math inline">\(\emptyset\)</span>; k++) do begin</p>
            <blockquote>
            <p><span class="math inline">\(C_{k+1}\)</span> = candidates generated from <span class="math inline">\(L_k\)</span>;</p>
            <p>for each transaction t in database do increment the count of all candidates in <span class="math inline">\(C_{k+1}\)</span> that are contained in t</p>
            <p><span class="math inline">\(L_{k+1}\)</span> = candidates in Ck+1 with min_support</p>
            </blockquote>
            <p>end</p>
            <p>return <span class="math inline">\(\cup_k L_k\)</span>;</p>
            <p>This is a summarization of what we analyzed, starting from the set of frequent 1-itemset and generating candidate k-itemset until we have an empty set.</p>
            <p>But how can we generate this candidate <span class="math inline">\(C_{k+1}\)</span> itemset?</p>
            <h3 data-number="4.3.3" id="how-is-the-apriori-property-used-in-the-algorithm"><span class="header-section-number">4.3.3</span> How is the Apriori property used in the algorithm?</h3>
            <p>Let us look at how <span class="math inline">\(L_{k-1}\)</span> is used to find <span class="math inline">\(L_k\)</span> for k <span class="math inline">\(\geq\)</span> 2. A two-step process, consisting of join and prune actions.</p>
            <h4 class="unnumbered" data-number="" id="the-join-step"><strong>The join step</strong>:</h4>
            <p>in order to find <span class="math inline">\(L_k\)</span>, a set of candidate k- itemsets, denoted <span class="math inline">\(C_k\)</span>, is generated by joining <span class="math inline">\(L_{k-1}\)</span> with itself.</p>
            <ul>
            <li><p>The notation <span class="math inline">\(l_i\)</span>[j] refers to the jth item in <span class="math inline">\(l_i\)</span> (<span class="math inline">\(l_1\)</span>[k-2] refers to the second to the last item in <span class="math inline">\(l\)</span>).</p></li>
            <li><p>For efficient implementation, Apriori assumes that items within a transaction or itemset are sorted in lexicographic order.</p></li>
            <li><p>For the (k-1)-itemset, <span class="math inline">\(l_i\)</span> , this means that the items are sorted such that <span class="math inline">\(l_i\)</span>[1] <span class="math inline">\(&lt;\)</span> <span class="math inline">\(l_i\)</span>[2] <span class="math inline">\(&lt;\)</span> ... <span class="math inline">\(&lt;\)</span> <span class="math inline">\(l_i\)</span>[k-1].</p></li>
            </ul>
            <p>We assume that in the transaction dataset the order is this lexicographic order that we impose. We generate the candidate items in this way from this order.</p>
            <p>With this order we generate the candidate itemset.</p>
            <p>The join, <span class="math inline">\(L_{k-1} \bowtie L_{k-1}\)</span>, is performed, where members of <span class="math inline">\(L_{k-1}\)</span> are joinable if their first (k-2) items are in common:</p>
            <p><span class="math display">\[
                (l_1[1] = l_2[1]) \wedge (l_1[2] = l_2[2]) \wedge ... \wedge (l_1[k-2] = l_2[k-2]) \wedge (l_1[k-1] &lt; l_2[k-1])
            \]</span></p>
            <p>The resulting itemset formed by the joining is:</p>
            <p><span class="math display">\[
                \{ l_1[1], l_1[2], ..., l_1[k-2], l_1[k-1], l_2[k-1] \}
            \]</span></p>
            <p>We have the k-2 items of <span class="math inline">\(l_1\)</span> and <span class="math inline">\(l_2\)</span>[k-1] as the k-th item in the k-itemset.</p>
            <p>This join allow us to generate automatically all the possible candidate k-itemsets.</p>
            <h4 class="unnumbered" data-number="" id="the-prune-step"><strong>The prune step</strong>:</h4>
            <p>any (k-1)-itemset that is not frequent cannot be a subset of a frequent k-itemset, for the apriori property. Hence, if any (k-1)-subset of a candidate k-itemset is not in <span class="math inline">\(L_{k-1}\)</span>, then the candidate cannot be frequent either and so can be removed from <span class="math inline">\(C_k\)</span>.</p>
            <p>If we discover that one subset itemset is not frequent we can remove directly the candidate itemset generated, like we did before.</p>
            <p>For example we can suppose the minsup is 2.</p>
            <p><img src="../media/image160.png" /></p>
            <p>We assume that the items are in lexicographic order, useful for us to check.</p>
            <p><img src="../media/image161.png" /></p>
            <p>First we check if the 1-itemsets are frequent. We generate <span class="math inline">\(C_2\)</span> candidates, we don’t apply directly the join because we don’t have any k-1 equal items, we directly apply combinations. We could apply the join but the first part is empty.</p>
            <p>We only consider <span class="math inline">\(l_1\)</span>[k-1] <span class="math inline">\(&lt;\)</span> <span class="math inline">\(l_2\)</span>[k-1] and combine <span class="math inline">\(l_1\)</span>[k-1] and <span class="math inline">\(l_2\)</span>[k-1] in the 2-itemset.</p>
            <p>We check if they’re frequent and now we have to generate the candidate 3-itemsets.</p>
            <p><img src="../media/image162.png" /></p>
            <p>{I1 , I2} and {I1, I3} can be combined, the first element is in common, I2 is lower than I3 and from these two itemsets we generate the 3-itemset {I1, I2, I3}.</p>
            <p>{I1, I2} and {I2, I3} cannot join, and so on.</p>
            <p>Now we have to perform pruning checking if any of these 3-itemsets contain inside subsets not frequent, if this occur, we can remove the itemset without checking the frequency in the transactional database.</p>
            <p>If both 2-itemsets are members of L2 we keep them.</p>
            <p>Then we have to check if they are frequent.</p>
            <p><img src="../media/image163.png" /></p>
            <p><span class="math inline">\(L_1\)</span>, <span class="math inline">\(L_2\)</span>, <span class="math inline">\(L_3\)</span> are the itemset frequents I can generate.</p>
            <p>If we want to return the closed itemsets to the user we should return {I1, I2}, the 2-itemset with support 2, or also {I1} and {I2}, the 1-itemsets with their support.</p>
            <p>For the definition of closed itemsets we have to return all them, otherwise we lose information.</p>
            <h3 data-number="4.3.4" id="summary"><span class="header-section-number">4.3.4</span> Summary</h3>
            <p><img src="../media/image164.png" /></p>
            <p><img src="../media/image165.png" /></p>
            <p><img src="../media/image166.png" /></p>
            <p>The last procedure is the pruning step applied after the generation of the candidate itemset; it allows us to remove itemsets without checking if they’re frequent.</p>
            <p>With a low minsup we have a long process, we stop at a very high value of k.</p>
            <p>We generate several k itemsets and we scan several times the database. The complexity can become larger.</p>
            <p>Each time we must check if an itemset is frequent we must scan the db.</p>
            <p>If the minsup is too high, we have the reduction of generation of candidate itemsets and reduce the number of scans in the db.</p>
            <p>Once we have all the frequent itemsets we need to <strong>generate association rules</strong>.</p>
            <p>Association rules can be generated by combining all the possible combination of items in the frequent itemsets in the antecedent and the consequent part.</p>
            <p>The only constraint is that the itemset in the antecedent and the itemset in the consequent have empty intersection, they don’t have items in common.</p>
            <p>For each frequent itemset l, we generate all nonempty subsets of l. For every nonempty subset s of l, output the rule "s <span class="math inline">\(\Rightarrow\)</span> (l - s) if <span class="math inline">\(\frac{support\_count(l)}{support\_count(s)} \geq min\_conf\)</span>, where min_conf is the minimum confidence threshold.</p>
            <p><span class="math display">\[
                Confidence(A \Rightarrow B) = P(B|A) = \frac{Support\_count(A \cup B)}{Support\_count(A)}
            \]</span></p>
            <p>The support of the rule is always higher than minsup because generating using the frequent patterns.</p>
            <p>The confidence of an association rule is defined as the ratio of the support of the frequent pattern and the support of A.</p>
            <p>We’re looking for an high value of confidence because it tells us that when we have the antecedent we have a high probability to have the consequent.</p>
            <p>But if the consequent is always present in our transactions, of course the confidence is high, but it doesn’t mean A imply B really, that is there because it’s always in the transactions.</p>
            <p>We don’t have a strong real correlation. If we have an high value of confidence, if it doesn’t occur, the imply is real.</p>
            <p>The choice of minsup can be very important: if we choose a high minsup we cannot discover rare item in the transaction.</p>
            <p>The disadvantage of the <strong>support</strong> is the <strong>rare item problem</strong>.</p>
            <p>Items that occur infrequently in the data set are pruned although they would still produce interesting and potentially valuable rules. The rare item problem is important for transaction data which usually have a uneven distribution of support for the individual items.</p>
            <p>We must find a value of minsup to extract the rare items.A problem with <strong>confidence</strong> is that it is <strong>sensitive</strong> to the frequency of the consequent in the database. Caused by the way confidence is calculated, consequents with higher support will automatically produce higher confidence values even if there exists no association between the items.</p>
            <h3 data-number="4.3.5" id="improving-the-efficiency-of-apriori"><span class="header-section-number">4.3.5</span> Improving the efficiency of Apriori</h3>
            <p>The apriori algorithm is quite old, but have some limitations in term of computational effort.</p>
            <p>Any itemset that is potentially frequent in DB must be frequent in at least one of the partitions of DB</p>
            <p>Scan 1: partition database and find local frequent patterns</p>
            <p>Scan 2: consolidate global frequent patterns</p>
            <p><img src="../media/image168.png" /></p>
            <p>We divide the DB in n partitions (with empty intersection) and find frequent itemsets in local partitions.</p>
            <p>We combine all local frequent itemsets and then find global frequent itemsets.</p>
            <p><img src="../media/image169.png" /></p>
            <p>The idea is to apply the apriori algorithm in all partition to discover frequent itemsets.</p>
            <p>If an itemset is not frequent in any database, the itemset can not be frequent in general.</p>
            <p>This is a way to reduce the number of itemsets we need to check.</p>
            <h4 class="unnumbered" data-number="" id="dhp-reduce-the-number-of-candidates"><strong>DHP: Reduce the number of candidates</strong></h4>
            <p>Another approach to increase the efficiency of the algorithm is to use an hash table for the itemset.</p>
            <p>When scanning each transaction in the database to generate the frequent 1-itemsets, <span class="math inline">\(L_1\)</span>, we can generate all the 2-itemsets <span class="math inline">\(C_k\)</span>, for each transaction, hash (i.e., map) them into the different buckets of a hash table structure, and increase the corresponding bucket counts.</p>
            <p><img src="../media/image170.png" /></p>
            <p>If a bucket count is lower than minsup we can say that all the contents of the bucket are not frequent. This is a way to reduce the 2-itemsets to consider in the next step.</p>
            <p>In this example, {I1, I4} = 1 * 10 + 4 = 14 mod 7 = 0.</p>
            <p>A k-itemset whose corresponding hashing bucket count is below the threshold cannot be frequent</p>
            <ul>
            <li><p><strong>Candidates</strong>: I1, I2, I3, I4, I5, I6</p></li>
            <li><p><strong>Hash entries</strong> : {I1I4, I3I5} , {I1I5} ,…</p></li>
            </ul>
            <p>I1I4 is not a candidate 2-itemset if the sum of count of {I1I4, I3I5} is below support threshold</p>
            <h4 class="unnumbered" data-number="" id="sampling-for-frequent-patterns"><strong>Sampling for Frequent Patterns</strong></h4>
            <ul>
            <li><p>Select a sample of the original database</p></li>
            <li><p>Mine frequent patterns within sample using Apriori</p>
            <ul>
            <li>Possibly use a lower support threshold</li>
            </ul></li>
            <li><p>Scan database once to verify frequent itemsets found in sample, only borders of closure of frequent patterns are checked</p>
            <p>Example: check abcd instead of ab, ac, ..., etc.</p></li>
            <li><p>Scan database again to find missed frequent patterns using the concept of negative border. The negative border contains the “closest” itemsets that could be frequent. Given a collection S in P(R) of sets, closed with respect to the set inclusion relation, the negative border of S consists of the minimal itemsets X in R not in S.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="dic-reduce-number-of-scans"><strong>DIC: Reduce Number of Scans</strong></h4>
            <p>The Database is partitioned into blocks.</p>
            <p>Once both A and D are determined frequent, the counting of AD begins (start point)</p>
            <p>Once all length-2 subsets of BCD are determined frequent, the counting of BCD begins</p>
            <p>The idea is to anticipate the scanning of the DB for checking if we have 2-itemsets frequent. During the scan of the database we see that A and D are frequent and we also start to scan also the 2-itemset AD.</p>
            <p><img src="../media/image172.png" /></p>
            <!-- ![](../media/image173.png) -->
            <hr />
            <p>We can try to improve performances but we always have some <strong>bottlenecks of the apriori algorithm</strong>.</p>
            <p>Apriori uses a generate-and-test approach <span class="math inline">\(\rightarrow\)</span> generates candidate itemsets and tests if they are frequent (Breadth-first (i.e., level-wise) search.</p>
            <p>Generation of candidate itemsets is expensive (in both space and time)</p>
            <p>If there are <span class="math inline">\(10^4\)</span> frequent 1-itemsets, the Apriori algorithm will need to generate more than <span class="math inline">\(10^7\)</span> candidate 2-itemsets and it also has to check them all.</p>
            <p>To discover a frequent pattern of size 100, it has to generate at least <span class="math inline">\(2^{100} -1\)</span> candidates in total.</p>
            <p>We need to avoid to generate this large amount of candidate itemsets.</p>
            <p>Support counting is expensive</p>
            <ul>
            <li><p>Subset checking (computationally expensive)</p></li>
            <li><p>Multiple Database scans (I/O)</p></li>
            <li><p>Breadth-first (i.e., level-wise) search: may need to generate a huge number of candidate sets</p></li>
            </ul>
            <h2 data-number="4.4" id="fpgrowth-approach-mining-frequent-patterns-without-candidate-generation"><span class="header-section-number">4.4</span> FPGrowth Approach: Mining Frequent Patterns Without Candidate Generation</h2>
            <p>The FPGrowth Approach allows frequent itemset discovery without candidate itemset generation. It is a two step approach:</p>
            <ul>
            <li><p><strong>Step 1</strong>: Build a compact data structure called the FP-tree. It is built using only 2 scans over the data-set and it will be a summarization of the DB.</p></li>
            <li><p><strong>Step 2</strong>: Extracts frequent itemsets directly from the FP-tree. We do not need to access the DB. If it is compact and can be stored in main memory we save a lot of effort.</p></li>
            </ul>
            <p>It is a depth-first search algorithm.</p>
            <p>The major philosophy is to grow long patterns from short ones using local frequent items only .</p>
            <p>If “abc” is a frequent pattern, we get all transactions having “abc”, i.e., project DB on abc: DB|abc. With this projection we can discover others frequent itemsets.</p>
            <p>If “d” is a local frequent item in DB|abc <span class="math inline">\(\rightarrow\)</span> abcd is a frequent pattern.</p>
            <p>We work with projections of the DB.</p>
            <h3 data-number="4.4.1" id="fp-tree-approach"><span class="header-section-number">4.4.1</span> FP-Tree Approach</h3>
            <ul>
            <li><p>First, it compresses the database representing frequent items into a frequent pattern tree, or FP-tree, which retains the itemset association information. It summarized the DB but without losing information.</p></li>
            <li><p>Then divides the compressed database into a set of conditional databases (a special kind of projected database), each associated with one frequent item or “pattern fragment,” and mines each database separately. We split the problem in these databases starting from frequent items.</p></li>
            <li><p>For each “pattern fragment,” only its associated data sets need to be examined. Therefore, this approach may substantially reduce the size of the data sets to be searched, along with the “growth” of patterns being examined.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="fp-tree-construction"><strong>FP-Tree Construction</strong></h4>
            <p>We need to represent the DB with a FP-Tree, which is constructed using 2 passes over the data-set:</p>
            <p><strong>Pass 1</strong>:</p>
            <ul>
            <li><p>Scan data and find support for each item.</p></li>
            <li><p>Discard infrequent items.</p></li>
            <li><p>Sort frequent items in decreasing order based on their support.</p></li>
            </ul>
            <p>We use this order when building the FP-Tree, so common prefixes can be shared.</p>
            <p><img src="../media/image174.png" /></p>
            <p><strong>Pass 2</strong>:</p>
            <p>We generate the FP-Tree. Nodes correspond to items and have a counter.</p>
            <ul>
            <li><p>FP-Growth reads 1 transaction at a time and maps it to a path in the FP-Tree.</p></li>
            <li><p>Fixed order is used, so paths can overlap when transactions share items (when they have the same prefix ). In this case, when we add shared paths the counters are incremented</p></li>
            <li><p>Pointers are maintained between nodes containing the same item, creating singly linked lists (dotted lines). The more paths that overlap, the higher the compression. FP- tree may fit in memory. It is fundamental because it avoids to have swapping.</p></li>
            <li><p>Frequent itemsets are extracted from the FP-Tree.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="step-1-fp-tree-construction"><strong>Step 1: FP-Tree Construction</strong></h4>
            <p>Suppose we have this transaction dataset. We generate the FP-Tree using this database.</p>
            <p>We start with the first transaction {a,b}.</p>
            <p>When we build FP-Tree the root node is null, just a starting point. The first path consists in a and b and we reproduce this transaction as a path of FP-Tree, that has in sequence the most frequent items of the transaction.</p>
            <p>The second transaction starts with a different item respect to the first transaction, we need to create another path. The dashed line with the arrow is the link that connects nodes with the same item. We cannot use one single node, we use this link to count how many occurencies of the same item we have in the FP-Tree.</p>
            <p><img src="../media/image175.png" /></p>
            <p>In the third transaction we see that the first item is consistent with the first of the first transaction, we already have a path and we can exploit it.</p>
            <p>We generate a path starting from ‘a’, without duplicating it but simply increasing the counter.</p>
            <p>If we navigate the FP-tree we can discover all transactions we have in the original DB.</p>
            <p>If we have transactions with different items we have the problem to reproduce the path without any common nodes, we increase the size of the tree and considering we have to create paths and arrows, the occupation of memory is high.</p>
            <p>Thanks to the fact that a lot of paths share same nodes we get a more compact representation than the DB.</p>
            <p>The FP-Tree usually has a smaller size than the uncompressed data - typically many transactions share items (and hence prefixes).</p>
            <ul>
            <li><p>Best case scenario: all transactions contain the same set of items: 1 path in the FP-tree</p></li>
            <li><p>Worst case scenario: every transaction has a unique set of items (no items in common) In this case the size of the FP-tree is at least as large as the original data. The storage requirements for the FP-tree are higher - need to store the pointers between the nodes and the counters.</p></li>
            </ul>
            <p>The size of the FP-tree depends on how the items are ordered. Ordering by decreasing support, in this way we have most frequent items close to the root, is typically used but it does not always lead to the smallest tree (it's a heuristic).</p>
            <p>We also are using a parameter, we are talking about frequent patterns, so we are fixing the minsup. If we fix an high minsup we have a compressed tree.</p>
            <p>If the value is low we expect that the FP-tree will be larger.</p>
            <h4 class="unnumbered" data-number="" id="step-2-frequent-itemset-generation"><strong>Step 2: Frequent Itemset Generation</strong></h4>
            <p>The FP-Growth algorithm extracts frequent itemsets from the FP-tree, doesn’t work on the DB.</p>
            <p>We need to scan the DB only twice, once to scan frequent items and one to generate the FP-Tree.</p>
            <p>It is a bottom-up algorithm, from the leaves towards the root.</p>
            <p>It uses a divide and conquer approach: first look for frequent itemsets ending in one item and then itemsets consisting of that item and another, first e, we focus on all FP-Trees including e then de, etc. . . then d, then cd, etc. . .</p>
            <p>We first extract prefix path subtress ending in item and then we work directly to discover all frequent patterns including that item locally with the subtree. (use the linked lists)</p>
            <p>Suppose we have extracted this FP-Tree:</p>
            <p><img src="../media/image176.png" /></p>
            <p>We start from the item with the lowest frequency and we want to extract all itemsets including e. We extract subtrees with e in the leaves. That is the subtree with prefix ‘e’.</p>
            <p>We can then extract with prefix d with the same considerations. That will be the subtree for prefix ‘d’, and so on.</p>
            <p>Each prefix path sub-tree is processed recursively to extract the frequent itemsets. Solutions are then merged.</p>
            <p>We look for e and we ensure that all itemsets including e are inside in this subtree.</p>
            <p>If we want to discover all itemsets including e we can focus only on the subtree.</p>
            <p>E.g. we consider first the prefix path sub-tree for e and it will be used to extract frequent itemsets ending in e, then in de, ce, be and ae, then in cde, bde, cde, etc.</p>
            <p><img src="../media/image177.png" /></p>
            <p>We use a divide and conquer approach. We start with the tree including e, we focus then on de and so on.</p>
            <p>When we select the sub-tree ‘de’ it refers all transactions in which we have the pair (d,e).</p>
            <h3 data-number="4.4.2" id="conditional-fp-tree"><span class="header-section-number">4.4.2</span> Conditional FP-Tree</h3>
            <p>The <strong>Conditional FP-Tree</strong> is the FP-Tree that would be built if we only consider transactions containing a particular itemset (and then removing that itemset from all transactions).</p>
            <p>Example: FP-Tree conditional on e.</p>
            <p><img src="../media/image178.png" /></p>
            <p>We consider only transactions in which e appears without seeing that in the tree.To extract a FP-Tree conditional to a specific item consists in extracting the part of FP-Tree corresponding to the transactions in the DB in which the prefix is included.</p>
            <p>Let minSup = 2 and extract all frequent itemsets containing e.</p>
            <p>We want to obtain the prefix path sub-tree for e:</p>
            <p><img src="../media/image179.png" /></p>
            <p>We use the overall complete FP-Tree and we analyze paths ending in ‘e’.</p>
            <ol type="1">
            <li><p>Check if e is a frequent item by adding the counts along the linked list (dotted line). If so, extract it. We have to check if e is a frequent item by adding the counters in the link list (with arrow). Yes, count =3 so {e} is extracted as a frequent itemset.</p></li>
            <li><p>As e is frequent, find frequent itemsets ending in e. i.e. de, ce, be and ae.</p></li>
            <li><p>Use the conditional FP-tree for e to find frequent itemsets ending in de, ce and ae. Note that be is not considered as b is not in the conditional FP-tree for e. We have that the path bce is only one transaction even if b has two as value of the counter, because e has 1 as value of the counter. be is not frequent.</p></li>
            </ol>
            <p>For each of them (e.g. de), find the prefix paths from the conditional tree for e, extract frequent itemsets, generate conditional FP-tree, etc... (recursive)</p>
            <p>Example: e <span class="math inline">\(\rightarrow\)</span> de <span class="math inline">\(\rightarrow\)</span> ade ({d,e}, {a,d,e} are found to be frequent)</p>
            <p><img src="../media/image180.png" /></p>
            <p>We generate the conditional FP-Tree for de and then for ade. We eliminate c because it cannot generate frequent itemsets with d and e.</p>
            <p>We generate conditional FP-Tree only for frequent itemsets.</p>
            <p>Example: e <span class="math inline">\(\rightarrow\)</span> ce ({c,e} is found to be frequent)</p>
            <p><img src="../media/image181.png" /></p>
            <p>We can stop there because we don’t have a conditional FP-tree for ce.</p>
            <p>Frequent itemsets found (ordered by suffix and order in which they are found):</p>
            <p><img src="../media/image182.png" /></p>
            <h3 data-number="4.4.3" id="advantages-of-fp-growth"><span class="header-section-number">4.4.3</span> <strong>Advantages</strong> of FP-Growth:</h3>
            <ul>
            <li><p>only 2 passes over data-set, independently on the value of the minsup</p></li>
            <li><p>“compresses” data-set</p></li>
            <li><p>no candidate generation</p></li>
            <li><p>much faster than Apriori</p></li>
            </ul>
            <h3 data-number="4.4.4" id="disadvantages-of-fp-growth"><span class="header-section-number">4.4.4</span> <strong>Disadvantages</strong> of FP-Growth:</h3>
            <ul>
            <li><p>FP-Tree may not fit in memory, we are obligated to swap-in/swap-out, reducing also the advantage of compression</p></li>
            <li><p>FP-Tree is expensive to build</p></li>
            </ul>
            <p>Let's see another example.</p>
            <p><img src="../media/image183.png" /></p>
            <p>We have a DB of transactions, we generate this FP-tree and in the structure we also have this table with all items in descending order and from the table we have a link to the first node of the tree, which have links to other nodes with the same item.</p>
            <p>We focus on i5 and analyze if we have conditional FP-trees and frequent patterns to generate.</p>
            <p>We can generate the conditional FP-tree for i5.</p>
            <p>We discard i3 because it is included in only one path in which we have i5.</p>
            <p>Then we can generate frequent patterns.</p>
            <p>In I4 conditional FP-Tree we discard I1 because we have only one possible path which include I4.</p>
            <p>We have I2 with support 4 because we have 2 paths with support 2.</p>
            <h3 data-number="4.4.5" id="algorithm-1"><span class="header-section-number">4.4.5</span> Algorithm</h3>
            <p>This is a description of the <strong>algorithm</strong>.</p>
            <p><img src="../media/image184.png" /></p>
            <p><img src="../media/image185.png" /></p>
            <p><img src="../media/image186.png" /></p>
            <h3 data-number="4.4.6" id="benefits-for-the-fp-tree-structure"><span class="header-section-number">4.4.6</span> <strong>Benefits</strong> for the FP-Tree Structure:</h3>
            <ul>
            <li><p>Completeness. It preserves complete information for frequent pattern mining. It never breaks a long pattern of any transaction.</p></li>
            <li><p>Compactness. It reduces irrelevant info—infrequent items are gone. We have items in frequency descending order: the more frequently occurring, the more likely to be shared.It can never be larger than the original database (not count node-links and the count field)</p></li>
            </ul>
            <h2 data-number="4.5" id="the-frequent-pattern-growth-mining-method"><span class="header-section-number">4.5</span> <strong>The Frequent Pattern Growth Mining Method</strong></h2>
            <p>When the database is large, it is sometimes unrealistic to construct a main memory-based FP-tree.</p>
            <p>We have an alternative.</p>
            <ul>
            <li><p>first partition the database into a set of projected databases,</p></li>
            <li><p>then construct an FP-tree and mine it in each projected database</p></li>
            <li><p>This process can be recursively applied to any projected database if its FP-tree still cannot fit in main memory.</p></li>
            </ul>
            <p>A study of the FP-growth method performance shows that it is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm.</p>
            <h3 data-number="4.5.1" id="comparison"><span class="header-section-number">4.5.1</span> Comparison</h3>
            <p>This is the comparison of performance in large datasets:</p>
            <p><img src="../media/image187.png" /></p>
            <p>In x we have the value of the support, and when it increases the run time decreases.</p>
            <p>If it is high, the run-time value is comparable, also the number of scans performed by apriori is in fact low. Iterations stop really soon.</p>
            <p>The apriori algorithm run time with a low value of support is not manageble.</p>
            <p>We have this increase because some items were not really frequent with low values of minsup, we have an increase of items becoming frequent.</p>
            <h3 data-number="4.5.2" id="divide-and-conquer"><span class="header-section-number">4.5.2</span> Divide-and-conquer:</h3>
            <ul>
            <li><p>Decompose both the mining task and DB according to the frequent patterns obtained so far</p></li>
            <li><p>Lead to focused search of smaller databases</p></li>
            </ul>
            <p>Other factors:</p>
            <ul>
            <li><p>No candidate generation, no candidate test</p></li>
            <li><p>Compressed database: FP-tree structure</p></li>
            <li><p>No repeated scan of entire database</p></li>
            <li><p>Basic ops: counting local freq items and building sub FP-tree, no pattern search and matching</p></li>
            </ul>
            <p>A good open-source implementation and refinement of FPGrowth is FPGrowth+.</p>
            <p>We also have further improvements of FPGrowth.</p>
            <h2 data-number="4.6" id="eclat-frequent-pattern-mining-with-vertical-data-format"><span class="header-section-number">4.6</span> ECLAT: Frequent Pattern Mining with Vertical Data Format</h2>
            <p>With the previous algorithms you can find all frequent patterns.</p>
            <p>The last approach work with the vertical format.</p>
            <p>For now we worked with horizontal format, for each transaction we had the item involved.</p>
            <p>In this format for each itemset we store the list of transaction in which we can find the itemset.</p>
            <p>We can use the standard apriori approach, consider 1 itemset, compute the set of transaction with the itemset and compute the 2-itemset.</p>
            <p>If the 2-itemset is frequent, here we don’t have to scan the dataset, we have to compute the cardinality of the intersection between the set of transactions corresponding to the item involved in this itemset and the other, and if is higher than the minsup, the itemset is frequent.</p>
            <p>At the end we have the set of transaction supporting the itemset.</p>
            <p><img src="../media/image188.png" /></p>
            <p>When we want to compute the support of {I1, I2} we compute the intersection and discover that it’s supported by those transactions.</p>
            <p>The cardinality of the itemset decreases with the increase of items in the itemset.</p>
            <p>We have to store a lot of data as disadvantage, we need to explore all transaction, but an advantage is that there is no need to scan the overall dataset.</p>
            <p>Independently of the algorithm we obtain the same result.</p>
            <p>Fixed the minsup we obtain the same frequent patterns.</p>
            <h2 data-number="4.7" id="pattern-evaluation-methods"><span class="header-section-number">4.7</span> Pattern Evaluation Methods</h2>
            <p>Let’s suppose we have 10000 customer transactions analyzed and we realize that:</p>
            <ul>
            <li><p>6000 include computer games</p></li>
            <li><p>7500 include videos</p></li>
            <li><p>4000 include computer games and video</p></li>
            </ul>
            <p><span class="math display">\[
                buys(X, &quot;computer\ games&quot;) \Rightarrow buys(X, &quot;videos&quot;)
            \]</span></p>
            <p><span class="math display">\[
                [support = 40\%, confidence = 66\%]
            \]</span></p>
            <p>The support is quite high, a lot of transactions support this rule.</p>
            <p>This value of confidence says that this rule proves that exists correlation between them.</p>
            <p>The rule is misleading because the probability of purchasing videos is 75%, which is even larger than 66%. Computer games and videos are negatively associated because the purchase of one of these items actually decreases the likelihood of purchasing the other.</p>
            <p>By purchasing together the product we reduce the probability to reduce the single product separately. They are negatively associated, one purchase of one of these items decrease the purchase of the other.</p>
            <p>When the consequent is highly probable, the confidence present an high value but it doesn’t mean a correlation between the antecedent and the consequent.</p>
            <p>To reduce this problem another metric was introduced, the <strong>lift</strong>.</p>
            <h3 data-number="4.7.1" id="lift"><span class="header-section-number">4.7.1</span> <strong>Lift</strong></h3>
            <p><span class="math display">\[
                lift(A\rightarrow B) = lift(B\rightarrow A) = \frac{Conf(A\rightarrow B)}{supp(B)} = \frac{Conf(B\rightarrow A)}{supp(A)} = \frac{P(A \cup B)}{(P(A)P(B))}
            \]</span></p>
            <p>Lift of A <span class="math inline">\(\rightarrow\)</span> B (A imply B) is defined as the ratio between the confidence of A <span class="math inline">\(\rightarrow\)</span> B (A imply B) and the support of B.</p>
            <p>It can be interpreted with that ratio of probabilities.</p>
            <p>Lift measures how many times more often X and Y occur together than expected if they were statistically independent.</p>
            <p>If we consider the definition in terms of probabilities, we have the joint probabilities between A and B divided by the product of probabilities.</p>
            <p>If they are not correlated the lift tend to be 1.</p>
            <p>If I’m far from 1, I can assume there’s a correlation.</p>
            <p>If it’s higher than 1 we have a positive correlation, if lower I’m decreasing the probability.</p>
            <p>Lift is not down-ward closed and does not suffer from the rare item problem.</p>
            <p>Rare itemsets with low counts (low probability) which per chance occur a few times (or only once) together can produce enormous lift values. It is a measure that gives us the possibility to investigate correlation between rare itemsets.</p>
            <p>Let’s see this example:</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>Basketball</th>
            <th>Not basketball</th>
            <th>Sum (row)</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>Cereal</td>
            <td>2000</td>
            <td>1750</td>
            <td>3750</td>
            </tr>
            <tr class="even">
            <td>Not cereal</td>
            <td>1000</td>
            <td>250</td>
            <td>1250</td>
            </tr>
            <tr class="odd">
            <td>Sum (col.)</td>
            <td>3000</td>
            <td>2000</td>
            <td>5000</td>
            </tr>
            </tbody>
            </table>
            <p>Play basketball <span class="math inline">\(\Rightarrow\)</span> Eat cereal [40%, 66.7%] is misleading</p>
            <p>The overall % of students eating cereal is 75% &gt; 66.7%. The probability decreases. Play basketball <span class="math inline">\(\Rightarrow\)</span> Not eat cereal [20%, 33.3%] is more accurate, although with lower support and confidence.</p>
            <p><span class="math display">\[
                lift = \frac{P(A \cup B)}{P(A)P(B)}
            \]</span></p>
            <p><img src="../media/image192.png" /></p>
            <p>The two lifts are close to 1, but the second is more far away than the first.</p>
            <p>These two values tell us that there’s no strong correlation between the 2 because lift is close to 1.</p>
            <p>We can also use another metric, the chi-square metric.</p>
            <p><img src="../media/image193.png" /></p>
            <p>We have a negative correlation because <span class="math inline">\(x^2\)</span> is quite high, negative because we can plot this value and understand that there’s a negative correlation.</p>
            <p>These metrics suffer from a generic problem: support and confidence are not good to indicate correlations.</p>
            <h2 data-number="4.8" id="measures-used-for-comparison"><span class="header-section-number">4.8</span> Measures used for comparison</h2>
            <p>Over 20 interestingness measures have been proposed. The best metrics are:</p>
            <ul>
            <li><strong>All_confidence [0,1]</strong>:</li>
            </ul>
            <p><span class="math display">\[
                All\_conf(A,B) = \frac{sup(A \cup B)}{max\{sup(A), sup(B)\}} = min\{P(A|B),P(B|A)\}
            \]</span></p>
            <ul>
            <li>Max_confidence [0,1]:</li>
            </ul>
            <p><span class="math display">\[
                Max\_conf(A,B) = max \{P(A|B),P(B|A)\}
            \]</span></p>
            <ul>
            <li>Kulczynski [0,1]:</li>
            </ul>
            <p><span class="math display">\[
                Kulc(A,B) = \frac{1}{2}(P(A|B)+P(B|A))
            \]</span></p>
            <ul>
            <li>Cosine [0,1]:</li>
            </ul>
            <p><span class="math display">\[
                cosine(A,B) = \frac{P(A \cup B)}{\sqrt{P(A) \times P(B)}} = \frac{sup(A \cup B)}{\sqrt{sup(A) \times sup(B)}} = \sqrt{P(A|B) \times P(B|A)}
            \]</span></p>
            <p>Let’s see an example with a contingency table:</p>
            <p><img src="../media/image198.png" /></p>
            <p>Let’s take the D1 dataset, <span class="math inline">\(x^2\)</span> is high (strong correlation), lift is high (strong correlation) and the 4 metrics have high value (strong correlation, they vary from 0 and 1). But if we see the transactions with mc, we have 10.000 transactions with milk and coffee of 11.000 with milk. There is a strong correlation.</p>
            <p>In <span class="math inline">\(D_2\)</span> we don’t change mc, <span class="math inline">\(\bar{m}c\)</span> , <span class="math inline">\(m\bar{c}\)</span> but we change <span class="math inline">\(\bar{mc}\)</span>, transactions in which milk and coffee do not appear.</p>
            <p><span class="math inline">\(x^2\)</span> tells us they are independent; lift tells us that they are independent but those values tell us we have a strong correlation.</p>
            <p>The problem is in <span class="math inline">\(\bar{mc}\)</span>, null-transactions as they are called, they affect <span class="math inline">\(x^2\)</span> and lift because they are metrics that are known as null-variant while the others are null-invariant, independently of null transactions the result is the same.</p>
            <p>Looking at Kulczynski, in the definition we are computing P(B|A) which is the probability of transaction in which we have AB divided by the total number of transactions of A, plus P(B!A) which is the probability of transaction in which we have AB divided by the total number of transactions of B. If you consider this probability, the total number of transactions, n, is expressed in the numerator and denominator so you can cancel it. This is the reason why the average is not affected on the total number of transactions but only on the transactions in which we have A, in which we have B and in which we have A and B together.</p>
            <h2 data-number="4.9" id="is-lift-a-good-measure-for-correlation"><span class="header-section-number">4.9</span> Is <strong>Lift</strong> a good measure for correlation?</h2>
            <p>In the lift we have:</p>
            <p><span class="math inline">\(P(A) = \frac{nA}{n}\)</span>, <span class="math inline">\(P(B) = \frac{nB}{n}\)</span>, <span class="math inline">\(P(AB) = \frac{nAB}{n}\)</span>. When we compute the lift, in the denominator we have <span class="math inline">\(2*n\)</span>, this means we cannot simplify it and we have the dependency on n in the lift. This is the reason why the lift is not null-invariant, we have the dependency on the overall number of transactions.</p>
            <p>If we want to analyze the correlation between milk and coffee, we should use null-invariant metrics because not affected by null transactions.</p>
            <p>But in D3, we analyze form <span class="math inline">\(x^2\)</span> and lift a strong correlation but from the other 4 values we see a value close to 0.</p>
            <p>Coffee is present in 1100 transactions but only in 100 transactions they appear together. The correct metrics are the last four.</p>
            <p><span class="math inline">\(x^2\)</span> and lift are not good metrics for understanding the correlation.</p>
            <p>These 4 metrics are similar, or we have a more reliable metric?</p>
            <p>In D4 we have that a strong correlation for <span class="math inline">\(x^2\)</span> and lift, while we have 0.5 for those metrics. Analyzing the dataset, we can deduce 0.5, we in fact have <span class="math inline">\(\frac{1000}{2000}\)</span>.</p>
            <p>In D5 we have that those 4 metrics disagree, <span class="math inline">\(Max\_conf\)</span> tells us strong correlation, <span class="math inline">\(All\_conf\)</span> that tells us not correlation, Kulc tells in the middle and cosine low correlation.</p>
            <p>If we analyze the data, coffee is present in 1100 and milk in 10100 and milk and coffee together are present in 1000. From the coffee point of view, we see a strong correlation, while form the milk point of view we can see a low correlation.</p>
            <p>From the definition of metrics, we have different values. The more reliable index is the Kulc. because give me the information that I cannot decide for correlation.</p>
            <p>If we consider D6, again Kulc. is in an intermediate situation. In fact, milk and coffee are in balance. Just one of 2 variables have an high number of transaction in respect to the other.</p>
            <p>The last 4 metrics offer us a more reliable conclusion, they are <strong>null-invariant</strong>.</p>
            <p>But considering, the last 4 metrics we have just to make consideration when our dataset is imbalanced.</p>
            <h3 data-number="4.9.1" id="imbalance-ratio-ir"><span class="header-section-number">4.9.1</span> Imbalance Ratio (IR)</h3>
            <p>We need to compute the IR (Imbalance Ratio), that measures the imbalance of two itemsets A and B in rule implications:</p>
            <p><span class="math display">\[
                IR(A,B) = \frac{|sup(A) - sup(B)|}{sup(A) + sup(B) - sup(A \cup B)}
            \]</span></p>
            <p>If IR is high, the reliability of the metrics cannot be guaranteed.</p>
            <p>If IR is low and Kulc. indicates 0.5 we really have an intermediate situation, while if IR is high, we cannot really have a reliable conclusion. The IR brings us to not make reliable conclusions.</p>
            <table>
            <thead>
            <tr class="header">
            <th>Data</th>
            <th><span class="math inline">\(mc\)</span></th>
            <th><span class="math inline">\(\bar{m}c\)</span></th>
            <th><span class="math inline">\(m\bar{c}\)</span></th>
            <th><span class="math inline">\(\bar{mc}\)</span></th>
            <th><span class="math inline">\(All\_conf\)</span></th>
            <th><span class="math inline">\(Max\_conf\)</span></th>
            <th><span class="math inline">\(Kulc\)</span></th>
            <th><span class="math inline">\(cosine\)</span></th>
            <th><span class="math inline">\(IR\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><span class="math inline">\(D_1\)</span></td>
            <td>10.000</td>
            <td>1.000</td>
            <td>1.000</td>
            <td>100.000</td>
            <td>0.91</td>
            <td>0.91</td>
            <td>0.91</td>
            <td>0.91</td>
            <td>0.0</td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(D_2\)</span></td>
            <td>10.000</td>
            <td>1.000</td>
            <td>1.000</td>
            <td>100</td>
            <td>0.91</td>
            <td>0.91</td>
            <td>0.91</td>
            <td>0.91</td>
            <td>0.0</td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(D_3\)</span></td>
            <td>100</td>
            <td>1.000</td>
            <td>1.000</td>
            <td>100.000</td>
            <td>0.09</td>
            <td>0.09</td>
            <td>0.09</td>
            <td>0.09</td>
            <td>0.0</td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(D_4\)</span></td>
            <td>1.000</td>
            <td>1.000</td>
            <td>1.000</td>
            <td>100.000</td>
            <td>0.5</td>
            <td>0.5</td>
            <td>0.5</td>
            <td>0.5</td>
            <td>0.0</td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(D_5\)</span></td>
            <td>1.000</td>
            <td>100</td>
            <td>10.000</td>
            <td>100.000</td>
            <td>0.09</td>
            <td>0.91</td>
            <td>0.5</td>
            <td>0.29</td>
            <td>0.89</td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(D_6\)</span></td>
            <td>1.000</td>
            <td>10</td>
            <td>100.000</td>
            <td>100.000</td>
            <td>0.01</td>
            <td>0.99</td>
            <td>0.5</td>
            <td>0.10</td>
            <td>0.89</td>
            </tr>
            </tbody>
            </table>
            <p>Kulczynski and Imbalance Ratio (IR) together present a clear picture for all the three datasets D4 through D6</p>
            <ul>
            <li><p><span class="math inline">\(D_4\)</span> is balanced &amp; neutral</p></li>
            <li><p><span class="math inline">\(D_5\)</span> is imbalanced &amp; neutral</p></li>
            <li><p><span class="math inline">\(D_6\)</span> is very imbalanced &amp; neutral</p></li>
            </ul>
            <p>We can see in fact that when we have an imbalance ratio in Kulczynski we have that one probability tend to be equal to 1 and the other very low and the average is close to 0.5.</p>
            <h1 data-number="5" id="classification"><span class="header-section-number">5</span> Classification</h1>
            <p>Classification is a technique in which we predict categorical class labels (discrete or nominal) and the aim is to generate a model that classifies labeled data (constructs a model) based on the training set (set of object with an associated class) and the values (class labels) in a classifying attribute and uses it in classifying new data, unlabeled objects. We learn the model (learning phase) and then we use the model to classify unlabeled data live (classification phase).</p>
            <p>Typical applications:</p>
            <ul>
            <li><p>Spam/non spam emails: based on the number of occurrencies of each word classify if it’s spam or not spam</p></li>
            <li><p>Credit/loan approval: if a loan applicant is safe or risky</p></li>
            <li><p>Medical diagnosis: if a tumor is cancerous or benign</p></li>
            <li><p>Fraud detection: if a transaction is fraudulent</p></li>
            <li><p>Web page categorization: which category it is</p></li>
            </ul>
            <p>Numeric Prediction models continuous-valued functions, i.e., predicts unknown or missing values.</p>
            <p>Classification is a two-step process.</p>
            <p>The first step is model construction (learning step or training phase), where a classifier is built describing a set of predetermined classes or concepts by exploiting a set of tuples/samples.</p>
            <p>A tuple is represented by an n-dimensional attribute vector (or feature vector) X = (x1, x2, ..., xn).</p>
            <p>Each tuple/sample is assumed to belong to a predefined class, as determined by the class label attribute. The set of tuples used for model construction is denoted as training set. The model is represented as classification rules, decision trees, or mathematical formulae.</p>
            <p>The second step consists in model usage for classifying future or unknown objects.</p>
            <p>We estimate the accuracy of the model, to be sure that it works properly,The known label of test sample is compared with the classified result from the model.</p>
            <p>Accuracy rate is the percentage of test set samples that are correctly classified by the model (valid if the dataset is not imbalanced). It’s important to know about the distribution of the classes. If we have a strong majority class and a classifier that answers always the majority class, in this way we have a high accuracy but it’s not high. We must be able to understand the ability to discriminate between classes. If the dataset is balanced the accuracy, is a good metric. We need to compare the classifiers in an affidable way.</p>
            <p>Test set is independent of the training set. We need it to measure the accuracy and find the best model to use, we known the label even for the test set.</p>
            <p>We need to see if the classifier can generalize.If the accuracy is acceptable, we can use the model to classify data tuples whose class labels are not known.</p>
            <figure>
            <img src="../media/image201.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <figure>
            <img src="../media/image202.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>In this example the classifier is a rule, which has the characteristic to be interpretable.</p>
            <p>Explainable AI is important, people want to understand how the system produces the output. People can believe in AI this way. It’s also important for legal aspects, we must understand for a legal point of view if an accident is a choice or a chance.</p>
            <p>Tenured is the class we want to predict. We use a trainset of 6 objects, described by 3 attributes.</p>
            <p>Typically, the name is not relevant for the classification.</p>
            <p>We can generate the rule we see in the right.</p>
            <p>This is an easy rule because the number of instances is low and so is the number of classes.</p>
            <p>In the classification step we analyze objects consisting of the same description of the training set, but we didn’t use them to train the model.</p>
            <p>Known the test set, we can compare classes in the test set with classes predicted by our model.</p>
            <p>We have different models we can use; we have to test them.</p>
            <p>The classification time is fundamental, a user want to have answers in a very short time.</p>
            <p>It’s preferable to spend shorter time in the second phase rather than the first, in which engineers set parameters.</p>
            <h3 data-number="5.0.1" id="difference-between-classification-and-clustering"><span class="header-section-number">5.0.1</span> Difference Between Classification and Clustering</h3>
            <ul>
            <li><strong>Supervised learning</strong> (classification)</li>
            </ul>
            <blockquote>
            <p>In these approaches we exploit the supervision, which is the training data (observations, measurements, etc.), accompanied by labels indicating the class of the observations</p>
            </blockquote>
            <p>New data is classified based on the training set</p>
            <ul>
            <li><strong>Unsupervised learning</strong> (clustering)</li>
            </ul>
            <p>The class labels of training data is unknown.</p>
            <blockquote>
            <p>Given a set of measurements, observations, etc. the aim is to establish the existence of classes or clusters in the data. We just exploit some characteristics of these data.</p>
            </blockquote>
            <ul>
            <li>We also have semi-supervised learning in which we approach characteristic of both approaches.</li>
            </ul>
            <h3 data-number="5.0.2" id="preliminary-step-preprocessing"><span class="header-section-number">5.0.2</span> Preliminary Step: Preprocessing</h3>
            <p>Before making use of classification, we need to <strong>prepare data</strong>.</p>
            <ul>
            <li><p>Data cleaning: preprocessing of data for removing or reducing noise and treating missing values</p></li>
            <li><p>Relevance Analysis: many of the attributes may be irrelevant for the classification problem, without losing information, without changing our classification problem.</p>
            <ul>
            <li><p>Correlation Analysis</p></li>
            <li><p>Attribute subset selection (or feature subset selection)</p></li>
            </ul></li>
            </ul>
            <blockquote>
            <p>Ideally, the time spent on relevance analysis should be less than the time that would have been spent on learning from the original set of attributes</p>
            </blockquote>
            <ul>
            <li><p>Data Transformation and Reduction</p>
            <ul>
            <li><p>Normalization: scaling the values for a given attribute so that they fall within a small, specified range</p></li>
            <li><p>Reduction</p></li>
            </ul></li>
            <li><p>Generalization to higher level concepts. For instance, numeric values for attribute income can be generalized as low, medium, and high</p>
            <ul>
            <li><p>several methods (wavelet transform, principle component analysis, discretization techniques such as binning, histogram analysis and clustering)</p></li>
            <li><p>Discretization: exploit ground truth for determining an optimal discretization of continuous-valued attributes</p></li>
            </ul></li>
            <li><p>Some learning algorithms proposed in the literature are applicable only to categorical attributed</p></li>
            <li><p>Especially when dealing with big data, performing discretization concurrently with learning can be very computationally expensive</p></li>
            <li><p>Typically use of heuristic approaches based on specific metrics</p></li>
            </ul>
            <blockquote>
            <p>We need to discretize the variables in a way that allow us to improve as much as possible our classification performance.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="discretization-technique">Discretization Technique</h4>
            <p>For discretization we can use the <strong>Fayyad and Irani approach</strong> (1992).</p>
            <ul>
            <li>Objective: to find an optimal partition for each continuous-valued attribute A. Partitions are determined by a set of cut points.</li>
            </ul>
            <p>This approach is supervised, we exploit the knowledge of classes.</p>
            <ul>
            <li>Observation: in the original paper the authors prove that the optimal cut points for the metrics used by them lie always between two examples of different classes in the sequence of sorted values of attribute A.</li>
            </ul>
            <blockquote>
            <figure>
            <img src="../media/image203.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            </blockquote>
            <p>Along this continuous variable we have several values associated to a specific class.</p>
            <p>The optimal cut is in the middle of instances between different classes.</p>
            <p>Definition: the potential cut point T is a boundary point b if in the sequence of examples sorted by the value A, there exist two examples, e1 and e2, having different classes, such that A(e1) &lt; T &lt; A(e2) and there exists no other example e’ such that A(e1) &lt; A(e’) &lt; A(e2). Let us assume that b is the midpoint value between A(e1) and A(e2).</p>
            <p>This potential cut point is in the middle of two points belonging to two different classes.</p>
            <p>Let BA be the set of all candidate boundary points for attribute A.</p>
            <h5 class="unnumbered" data-number="" id="entropy-for-the-best-cutpoint"><strong>Entropy for the best cutpoint</strong></h5>
            <p>The algorithm exploits the entropy as quality measure to find the best boundary cutpoint.</p>
            <p>Let S be the set of examples. Let there be k classes C1, ..., Ck.</p>
            <p>Let P(Ci,S) be the proportion of examples in S that have the class Ci.</p>
            <p>The class entropy of a subset S is defined as:</p>
            <figure>
            <img src="../media/image204.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>P(Ci, S) = Probability of having Ci.</p>
            <p>If in S we have only one class, k = 1, the Entropy is 0.</p>
            <p>For the only class we have the log equals to 0, and for the other the probability is 0.</p>
            <p>The worst case is in equal-probability classes, this is the higher level of confusion.</p>
            <p>If I select an interval and the probability of having classes is the same for all classes, this interval doesn’t allow me to discriminate classes.</p>
            <p>We should reach intervals with low value of entropy.</p>
            <p>The idea is to split the continuous variable in such a way that each interval is characterized with low entropy, I’m going forward my optimal solution, where I have intervals in which I have only one class, in that case we talk of pure partitions.</p>
            <p>Let T a cut points in BA. Set S is partitioned in the subsets S1 and S2, where S1 contains the subset of examples in S with A-values not exceeding T and S2 = S- S1. The class information entropy of the partition induced by T, denoted as EP(A, T; S), is defined as:</p>
            <figure>
            <img src="../media/image205.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>The cut point TA for which EP(A, TA; S) is minimal amongst al the candidate cut points is taken as the best cut point</p>
            <p>We look for the cutpoint for which the entropy is minimal among all possible cutpoints.</p>
            <p>This cutpoint is obtained like in the formula.</p>
            <p>When I cut S with TA, S1 is the set in the left of the cutpoint and S2 is the set in the right.</p>
            <p>The ratio we have in the formula is given by the cardinality of S1 over the number of points, it gives us the probability of having values inside the set S1, and these two probabilities weight the entropy of S1 and S2.</p>
            <p>The algorithm first partition S, find the optimal cutpoint and then repeat the process in S=S1 and S=S2.</p>
            <p>The we split again until a stopping condition is met.</p>
            <p>The stopping condition is defined as:</p>
            <figure>
            <img src="../media/image206.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <figure>
            <img src="../media/image207.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>(Not important to remember)</p>
            <p>This approach has the advantage to have a stopping condition, we don’t have to fix anything.</p>
            <p>At the end of the approach, we have intervals that discretize our continuous variable, which become an ordinal variable.</p>
            <p>Each value in each sub-partition will be replaced with a specific label.</p>
            <p>We try to find best intervals thinking about what we want to reach at the end, it is a supervised approach.</p>
            <h2 data-number="5.1" id="comparing-classification-methods"><span class="header-section-number">5.1</span> Comparing Classification Methods</h2>
            <ul>
            <li><p>Accuracy: to understand how many records are being classified correctly.</p></li>
            <li><p>Speed:</p>
            <ul>
            <li><p>time to construct the model (training time)</p></li>
            <li><p>time to use the model (classification/prediction time)</p></li>
            </ul></li>
            <li><p>Robustness: handling noise and missing values. Machine learning models are under attack, if you use specific training sets you can let the classifier to learn wrong models</p></li>
            <li><p>Scalability: efficiency in disk-resident databases. Expecially in the learning phase it’s important if we have many instances, swapping can be very consuming.</p></li>
            <li><p>Interpretability: understanding and insight provided by the model, to convince people.</p></li>
            <li><p>Other measures, e.g., goodness of rules, such as decision tree size or compactness of classification rules. Measures specific to specific types of classifiers.</p></li>
            </ul>
            <p>We will analyze in detail accuracy and interpretability.</p>
            <h3 data-number="5.1.1" id="decision-tree"><span class="header-section-number">5.1.1</span> Decision Tree</h3>
            <p>The decision tree: learning of decision trees from class-labeled training tuples</p>
            <p>The Decision Tree is a flowchart-like tree structure, where:</p>
            <ul>
            <li><p>each internal node (non-leaf node) denotes a test on an attribute,</p></li>
            <li><p>each branch represents an outcome of the test</p></li>
            <li><p>each leaf node (terminal node) holds a class label.</p></li>
            </ul>
            <p><img src="../media/image208.png" alt="alt" /> <img src="../media/image209.png" alt="alt" /></p>
            <p>During the classification phase, depending on the specific value, we follow one of the branches of the built decision tree.</p>
            <p>How decision tree are used for classification? Given a tuple, the attribute values of the tuple are tested</p>
            <p>against the decision tree.</p>
            <h4 class="unnumbered" data-number="" id="decision-tree-classifier-advantages">Decision Tree Classifier Advantages</h4>
            <ul>
            <li><p>No domain knowledge, we don’t need to know about the specific application</p></li>
            <li><p>No parameter setting</p></li>
            <li><p>Can manage high-dimensional data. Some attributes are not used in the decision tree if not useful. During the building we also perform attribute selection.</p></li>
            <li><p>Representation intuitive and easy to assimilate</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="how-does-decision-tree-work">How does Decision Tree Work?</h4>
            <p>We need to understand why we use a specific attribute on the root and some of the others after.</p>
            <p>The choice of these attributes affect the final result and the size of the decision tree.</p>
            <p>We need to select the optimal attribute for each deicision node.</p>
            <p>The basic algorithm is a <strong>greedy</strong> algorithm - follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding the global optimum. It’s not sure that we achieve it but we may achieve a close solution in short time.</p>
            <ul>
            <li><p>Tree is constructed in a <strong>top-down</strong> recursive divide-and- conquer manner</p></li>
            <li><p>At start, all the training examples are at the root</p></li>
            <li><p><strong>Attributes are categorical</strong> (if continuous-valued, they are discretized in advance, actually in some approaches It happens on the applying)</p></li>
            <li><p>Examples are partitioned recursively based on selected attributes</p></li>
            <li><p>Test attributes are selected on the basis of a heuristic or statistical measure (e.g., information gain)</p></li>
            </ul>
            <p>Let’s see the <strong>conditions for stopping partitioning:</strong></p>
            <ul>
            <li><p>All samples for a given node belong to the same class, we already solved our problem. We do not need to exploit more.</p></li>
            <li><p>There are no remaining attributes for further partitioning – majority voting is employed for classifying the leaf. We explored all attributes, we transform this node in a leaf and we associate the majority class.</p></li>
            <li><p>There are no samples left, we classified all samples in the training set</p></li>
            </ul>
            <p>The strategy for selecting the attributes is to select the attributes that guarantee to create partitions at each branch as pure as possible.</p>
            <p>A partition is pure whether all the tuples in it belong to the same class.</p>
            <p>Let’s see the attribute selection measure.</p>
            <p>Let D be a training set of class-labeled tuples.</p>
            <p>Suppose that the class label attribute has m distinct values defining m distinct classes, Ci (for i=1,..,m).</p>
            <ul>
            <li>Select the attribute with the highest information gain. To define it we need to define the probability (pi) that an arbitrary tuple in D belongs to class Ci, estimated by |Ci, D|/| D |</li>
            </ul>
            <h5 class="unnumbered" data-number="" id="info-gain"><strong>Info Gain</strong></h5>
            <p>The info gain is based on the computation of the information entropy.</p>
            <p>Expected <strong>information</strong> (<strong>entropy</strong>) needed to classify a tuple in D:</p>
            <blockquote>
            <p><img src="../media/image210.png" alt="alt" /><br />
            Observe: pi is the probability of class i in D</p>
            <p>When a probability of a class is 1, info(D) is 0. If the entropy is 0 it means that in D we have only instances belonging to the same class.</p>
            </blockquote>
            <p>This is the plot in a two classes problem of the probability from 0 to 1 of one class, the other is just opposite of course.</p>
            <figure>
            <img src="../media/image211.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>When the probability of one class is 0 the entropy is 0, the same for 1.</p>
            <p>When the probability is 0.5 we have a lot of confusion and the entropy is maximum.</p>
            <p>All the training set at the beginning is in the root. We first compute the entropy of the overall training set, if it is equal 0 all the instances in the training set belongs to just one class and the problem is solved.</p>
            <p>We have to decide the attribute to put in the root, we should choose it in such a way that we go towards pure partitions.</p>
            <p>If we discover that we created only pure partitions we need only this attribute to classify the dataset.</p>
            <p>Suppose that we split D into v partitions {D1, ..., Dv} and we choose the attribute A having v distinct values {a1,...,av}. Information needed (after using A to split D into v partitions) to classify D:</p>
            <figure>
            <img src="../media/image212.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We compute the entropy of Dj where Dj contains all the tuples in D with value aj for A.</p>
            <p>|Dj| is the cardinality of the set of objects in D with value for A equal to aj.</p>
            <p>We compute the entropy of this instances and we weight each Dj with this probability, comuted as number of object in Dj over number of object in D, probability to have value aj for the attribute A.</p>
            <p>If A is the optimum attribute, it must be able to split the training set in pure sets, each set contains only instances belonging to the same class.</p>
            <p>This situation is reached when Info(Dj) = 0, we have then a pure partition.</p>
            <p>We want to select the attribute that split the training set in subsets pure.</p>
            <p>We need to weight it because Info(Dj) can be different than 0, and we can have a pure partition with only 1 instance and all others with a lot of instances.</p>
            <p>If I just adopt the average without weight, the pure partition have only one instance and we get confused.</p>
            <p>We need to test all attributes and we choose the attribute with the highest level of <strong>information gain</strong>:</p>
            <figure>
            <img src="../media/image213.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If Gain(A) = Info(D) we are in the best situation.</p>
            <h5 class="unnumbered" data-number="" id="example-of-info-gain-with-decision-tree"><strong>Example of Info Gain with Decision Tree</strong></h5>
            <p>Let’s suppose to have a db of 14 samples and we want to understand if the customer will buy or not the computer.</p>
            <figure>
            <img src="../media/image214.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We need to compute of the overall training set, all the objects inside.</p>
            <p>The probability of having class yes is the number of instances in which we have yes, and the probability is 9/14.</p>
            <p>We compute the entropy of the overall dataset.</p>
            <figure>
            <img src="../media/image215.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We get a high value of entropy.</p>
            <p>We decide to select one attribute to improve this situation. We must test all 4 attributes to decide which is the best.</p>
            <p>We analyze the attribute age, that can have 3 different values.</p>
            <p>We compute:</p>
            <figure>
            <img src="../media/image216.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We consider the probability for that value * the information related to the subset identified by value of age = youth.</p>
            <p>We have 3 instances with value age = youth with class no and 2 instances with class yes.</p>
            <p>The info related to the subset identified corresponds to this application of the entropy:</p>
            <figure>
            <img src="../media/image217.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We apply the same for age = medium and old, and we get the info for this attribute.</p>
            <p>We select the attribute with the highest value of gain:</p>
            <figure>
            <img src="../media/image218.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Similarly:</p>
            <figure>
            <img src="../media/image219.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>So, we select age as attribute to use in the root.</p>
            <figure>
            <img src="../media/image220.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Age is the best attribute for the root node.</p>
            <p>It splits the training set in 3 subsets characterized by the three values of age: youth, middle_age, and senior.</p>
            <p>We can’t stop here because if we see the distribution of classes, we don’t have pure partitions, we have more than one class.</p>
            <p>We have a pure partition for middle_aged, I have the same class and that’s a termination condition, that subset mustn’t be split more.</p>
            <p>If I discover that value for age, I can conclude that the age is yes.</p>
            <p>We will repeat the process for the others with other attributes, stopping conditions aren’t satisfied.</p>
            <p>We will assume now that D is Dage=youth in the left, we will have a smaller training set.</p>
            <figure>
            <img src="../media/image221.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We will consider only the remaining attributes.</p>
            <p>We will follow the decision tree following the values of the attributes.</p>
            <p>If we don’t have remaining attributes but more classes, we will transform the subset in a leaf associating it the majority class.</p>
            <h4 class="unnumbered" data-number="" id="decision-tree-algorithm">Decision Tree Algorithm</h4>
            <p>This is the scheme to learn the decision tree:</p>
            <figure>
            <img src="../media/image222.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We do not use any parameters, just like we see in the inputs.</p>
            <figure>
            <img src="../media/image223.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <figure>
            <img src="../media/image224.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>There is also the possibility to apply binary splitting, independently on the values of the attribute we just split two subsets, we may join different attributes.</p>
            <p>We can change the splitting criterio, we may have multiple splits, but this is a generic approach.</p>
            <h5 class="unnumbered" data-number="" id="computing-information-gain-for-continuous-valued-attributes"><strong>Computing Information-Gain for Continuous-Valued Attributes</strong></h5>
            <p>We have to use categorical attributes but in many cases we have numerical attributes.</p>
            <p>The normal approach consists in applying discretization, we saw two supervised approaches, but it was proposed another approach in which we perform discretization during the generation of the tree.</p>
            <p>Let attribute A be a continuous-valued attribute.</p>
            <p>We must determine the best split point for A.</p>
            <ul>
            <li><p>we sort the value A in increasing order, the values in the instance on the training set.</p></li>
            <li><p>Typically, the midpoint between each pair of adjacent values is considered as a possible split point: (ai+ai+1)/2 is the midpoint between the values of ai and ai+1, this is a possible optimal split point for the attributes.</p></li>
            <li><p>The point with the minimum expected information requirement for A is selected as the split-point for A, calculating the info gain.</p></li>
            </ul>
            <p>It works better than discretizing at the beginning but it’s time consuming.</p>
            <blockquote>
            <p>This is why we usually perform the discretization before applying the decision tree approach.</p>
            <p>If we perform the discretization in this way, it’s like we have discretization thinking about tuning the generation of the decision tree.</p>
            </blockquote>
            <p>In this way we have the best performance, but with a lot of data is not pratical.</p>
            <p>Split: D1 is the set of tuples in D satisfying A ≤ split-point, and D2 is the set of tuples in D satisfying A &gt; split-point.</p>
            <p>The info gain we discussed before, suffer of a problem.</p>
            <p>Information gain measure is biased towards attributes with a large number of values</p>
            <p>For instance, attributes that act as unique identifier, we have only one instance in Dj, the partition Dj is characterized by only one instance, so we have one class and Info(Dj) is equal to 0.</p>
            <p>So Infoproduct_ID(D)=0 and we have a maximum value for Gain(A), so we have a pure partition. But it’s not useful for identifying the class, and also when I receive a new object we will have a new value for the id.</p>
            <p>The info gain is not reliable if we have an attribute characterized of an high number of possible values.</p>
            <p>The C4.5 algorithm do not use the info gain but uses a <strong>gain ratio</strong> to overcome the problem.</p>
            <p>It adopts a normalization to information gain using a “split information”:</p>
            <ul>
            <li><img src="../media/image225.png" title="fig:" alt="alt" /></li>
            </ul>
            <p>The value represents the potential information generated by splitting the training data set, D, into v partitions, corresponding to the v outcomes of a test on attribute A.</p>
            <p>The cardinality of Dj will be low in case of few instances, the log2 will be negative and high in module, that’s why we have the minus.</p>
            <p>And at the end we will have:</p>
            <p><span class="math inline">\(GainRatio(A) = \frac{Gain(A)}{SplitInfo(A)}\)</span></p>
            <p>Lower the number of instances, higher is the splitInfo and lower is the GainRatio.</p>
            <p>In our specific case:</p>
            <figure>
            <img src="../media/image226.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>GainRatio(income) = 0.029/1.557 = 0.019</p>
            <ins>
            The attribute with the maximum gain ratio is selected as the splitting attribute.
            </ins>
            <h4 class="unnumbered" data-number="" id="gini-index">Gini Index</h4>
            <p>Another metric we can use is the <strong>gini index</strong>.</p>
            <p>If a data set D contains examples from m classes, the Gini index, gini(D), is defined as:</p>
            <figure>
            <img src="../media/image227.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>where pi is the relative frequency of class i in D</p>
            <p>The Gini index considers a binary split for each attribute, and not a multi-way split.</p>
            <p>From one decision node we have only two branches.</p>
            <p>If A is a discrete variable withv distinct values {a1,...,av}, all possible subsets that can be formed from A are examined to determine the optimal binary split.</p>
            <p>For example, considering three possible values {low, medium, high}, the possible subsets are: {low, medium}, {low, high}, {medium, high}, {low}, {medium} {high}.</p>
            <p>We test all possible combinations and we test the value of the gini index.</p>
            <p>If we have just one class in a dataset D, one probability is 1 and others are 0, and the gini value is 0.</p>
            <p>For a 2-class problem:</p>
            <figure>
            <img src="../media/image228.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>When classes are equally probable, we have the situation with the biggest confusion.</p>
            <p>The trend is similar to the entropy, but the value is different.</p>
            <p>We can also use a misclassification error as index.</p>
            <p>If a dataset D is split on A into two subsets D1 and D2, the gini index giniA(D) is defined as:</p>
            <figure>
            <img src="../media/image229.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>For a discrete attribute, each of the possible binary splits is considered.</p>
            <p>For a continuous attribute, each possible split-point must be considered.</p>
            <p>The Reduction of Impurity is computed similarly:</p>
            <figure>
            <img src="../media/image230.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>The attribute that maximizes the reduction of impurity (or equivalently has the minimum Gini index) is selected as the splitting attribute.</p>
            <p>For example, D has 9 tuples in buys_computer = “yes” and 5 in “no”</p>
            <figure>
            <img src="../media/image231.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Let’s start with the attribute income and consider each of the possible splitting subsets. Consider the subset {low, medium}. This would result in 10 tuples in D1 and 4 in D2</p>
            <figure>
            <img src="../media/image232.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>The Gini values for splits on the remaining subsets are:</p>
            <p>Gini{low, high} = Gini{medium} 0.458; Gini{medium, high} = Gini{low} = 0.450. Thus, split on the {low, medium} (and {high}) since it has the lowest Gini index.</p>
            <p>We select the combination with the lowest gini index.</p>
            <p>The Gini index may need other tools, e.g., clustering, to get the possible split values.</p>
            <h3 data-number="5.1.2" id="comparing-attribute-selection-measures"><span class="header-section-number">5.1.2</span> Comparing Attribute Selection Measures</h3>
            <p>The three measures, in general, return good results but</p>
            <ul>
            <li><p><strong>Information gain</strong>: biased towards multivalued attributes</p></li>
            <li><p><strong>Gain ratio</strong>: tends to prefer unbalanced splits in which one partition is much smaller than the others</p></li>
            <li><p><strong>Gini index</strong>: biased to multivalued attributes, has difficulty when the number of classes is large and tends to favor tests that result in equal-sized partitions and purity in both partitions.</p></li>
            </ul>
            <p>They all suffer from at least one problem, there’s no optimal measure.</p>
            <p>Typically, we exploit the gain ratio.</p>
            <p>There are other attribute selection measures: CHAID, C-SEP, G-statistic, MDL, and CART.</p>
            <p>Which attribute selection measure is the best?</p>
            <p>Most give good results; none is significantly superior to others. All measures have some bias. However, time complexity of decision tree induction increases exponentially with the tree height, we must test a lot of possible splits. Thus, measures which tend to produce shallower trees may be preferred.On the other hand, shallow trees tend to have many leaves and higher error rates.</p>
            <p>Most of times we generate the decision tree, and we work to reduce the overfitting and reducing the size also for interpretability.</p>
            <h2 data-number="5.2" id="overfitting-and-tree-pruning"><span class="header-section-number">5.2</span> Overfitting and Tree Pruning</h2>
            <p>We have the problem of overfitting/overtraining that appears when we have too many branches, some may reflect anomalies due to noise or outliers.</p>
            <p>In some specific branches we just classify noise or outliers, we overfit.</p>
            <p>We achieve high accuracy for the training set, but we get poor accuracy for unseen samples.</p>
            <p>The reason is that the decision tree is strongly specialized in the training set and not able to generalize.</p>
            <figure>
            <img src="../media/image233.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <figure>
            <img src="../media/image234.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Instead of identifying area characterized by specific classes, we try to isolate single or few points, and this is particularly true when we just go towards the deepness of our tree.</p>
            <p>The last branches are very specialized on the specific instances.</p>
            <p>If we have some point in a red zone but we specialize too much, the noise just determines a decision area in which we associate a wrong class for the area containing the instance.</p>
            <p>How can we realize that the tree is affected by overfitting?</p>
            <ol type="1">
            <li><p>Learn the decision model by using the training set</p></li>
            <li><p>Compute the accuracy (ATraining) of the model by classifying the training set, re-using it.</p></li>
            <li><p>Compute the accuracy (ATest) of the model by classifying the test set</p></li>
            </ol>
            <p>We expect that the decision tree will perform better in the training set, but if ATest &lt;&lt; ATraining, then we have overtraining.</p>
            <figure>
            <img src="../media/image235.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Increasing the number of nodes our decision tree get more specialized in the training set and the difference in performance starts to be relevant.</p>
            <p>Our aim at the end is to use the decision tree to generalize.</p>
            <p>We generate a high number of nodes, so we have that the decision area is very small. We isolate the decision area in few instances.</p>
            <p>The intuition to solve the problem is to prune the tree. The problem is close to the leaves, not at the root, it’s there that we have a specialization of the tree.</p>
            <p>The idea is to prune but at the bottom and not at the top.</p>
            <p>We define:</p>
            <ul>
            <li><p>Re-substitution errors: error on training set (eTR), when we pick up it again to verify the performance.</p></li>
            <li><p>Generalization errors: error on test set (eTS), we use an unused unlabeled dataset.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="estimating-the-error">Estimating the Error</h4>
            <p>We can also have some methods for <strong>estimating generalization errors</strong>.</p>
            <blockquote>
            <p>With an optimistic approach we consider eTS = eTR</p>
            </blockquote>
            <p>With a pessimistic approach:</p>
            <ul>
            <li><p>For each leaf node eTS = eTR + 0.5, we add a penalization</p></li>
            <li><p>Total errors: eTS = eTR + 0.5 x N (N: Number of leaf nodes)</p></li>
            </ul>
            <p>For a tree with 30 leaf nodes and 10 errors on training</p>
            <blockquote>
            <p>eTR = 10/1000 = 1%</p>
            <p>eTS = (10 + 30x0.5)/1000 = 2.5%</p>
            </blockquote>
            <p>To consider that often the performance are worse than the training set.</p>
            <p>Another approach is using a reduced error pruning (REP) for estimating it.</p>
            <p>We use a pruning data set to estimate generalization error.</p>
            <p>We get a part of the training set as a test set. We train by using this part of the training set and we test using the pruning set, we test to understand if we can prune the tree.</p>
            <p>Once we pruned the tree, we assess the performance of the tree on the test set.</p>
            <figure>
            <img src="../media/image236.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <ins>
            We use that part to test if we can reduce the size of the tree.
            </ins>
            <p>Given two models of similar generalization errors, one should prefer the simpler model over the more complex.</p>
            <p>For complex models, there is a greater chance that it was fitted accidentally by errors in data.</p>
            <p>Therefore, one should include model complexity when evaluating a model, taking in count the generalization.</p>
            <p>We select the smaller model with accuracy comparable to the bigger models.</p>
            <p><strong>This rule is called Occam’s Razor.</strong></p>
            <h3 data-number="5.2.1" id="overfitting-prevention"><span class="header-section-number">5.2.1</span> Overfitting Prevention</h3>
            <p>We have two approaches to reduce overfitting, both about pruning the decision tree.</p>
            <ul>
            <li><strong>Prepruning</strong>: During the learning phase we stop without specializing too much.</li>
            </ul>
            <p>Halt tree construction early.</p>
            <p>Typical stopping conditions for a node:</p>
            <ul>
            <li><p>Stop if all the instances belong to the same class</p></li>
            <li><p>Stop if all the attribute values are the same</p></li>
            </ul>
            <p>More restrictive conditions:</p>
            <ul>
            <li><p>Stop if the number of instances is less than some user-specified threshold. We do not split one node if it contains less than several instances.</p></li>
            <li><p>‎Stop if class distribution of instances is independent of the available features (e.g., using Chi-square test).</p></li>
            <li><p>Stop if expanding the current node does not improve impurity measures (e.g., information gain or Gini index).</p></li>
            </ul>
            <blockquote>
            <p>The problem is that we must choose appropriate thresholds, for example on the number of instances.</p>
            <p>The risk is that the threshold is very low we have the problem of outliers and noise, if it’s too high we do not split some nodes that we should split.</p>
            </blockquote>
            <p>It is sometimes used when we have to deal with big data but we must fix the threshold.</p>
            <ul>
            <li><strong>Postpruning</strong>: In the postpruning we generate the decision tree fully grown.</li>
            </ul>
            <blockquote>
            <p>We then remove branches from a “fully grown” tree — get a sequence of progressively pruned trees.</p>
            <p>So we remove subtrees from a “fully grown” tree. A subtree at a given node is pruned by removing its branches and replacing it with a leaf. The leaf is labeled with the most frequent class among the subtree being replaced.</p>
            <p>We can use a set of data different from the training data to decide which is the “best pruned tree”. This is why we can use the pruning dataset discussed before.</p>
            </blockquote>
            <p>For example:</p>
            <figure>
            <img src="../media/image237.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If we prune in A5 we replace it with a leaf with the majority class.</p>
            <p>We must understand when we can stop in the postpruning phase. We must balance the classification error and the complexity. Increasing the complexity, we typically increase the classification error.</p>
            <h4 class="unnumbered" data-number="" id="reduced-error-pruning">Reduced Error Pruning</h4>
            <p>We can use <strong>Reduced Error Pruning (REP)</strong> where we use pruning set to estimate accuracy of sub-trees and accuracy of individual nodes.</p>
            <figure>
            <img src="../media/image238.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Let T be a sub-tree rooted at node v. The gain from pruning at v can be defined as:</p>
            <p><span class="math inline">\(Gain from pruning = misclassification_T - misclassification_v\)</span></p>
            <p><strong>Repeat:</strong> prune at node with the largest gain until only negative gain nodes remain</p>
            <p>We have a “Bottom/up restriction”: T can only be pruned if it does not contain a sub-tree with lower error than T. To perform this approach, we must have a pruning set.</p>
            <h5 class="unnumbered" data-number="" id="example-2"><strong>Example:</strong></h5>
            <figure>
            <img src="../media/image239.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We must decide if I maintain the subtree rooted by v2, or prune and replace v2 with a leaf. To decide it we use a pruning set, set not used for learning but part of the training set.</p>
            <p>In V4 we have 2 instances belonging to c1 and 2 in c2 in our pruning set.</p>
            <p>We associate with a leaf the majority class, considering most of the instances we have in the leaves we will prune.</p>
            <p>We have that v4 is labeled as c1 because it is most instances, we have 4 instances belonging to c1 and 2 instances belonging to C2.</p>
            <p>When we must classify the pruning set two instances are correctly classified but one doesn’t, I have one misclassification.</p>
            <p>We then compute the gain from pruning.</p>
            <p>We want to understand the error we have when we consider the overall subtree and the error when we consider the pruning.</p>
            <p>In case we maintain the subtree, in v4 2 instances are correctly classified while we have 1 misclassification.</p>
            <p>In v5 I have 2 misclassifications because I have two instances belonging to c1 but the majority class is c2.</p>
            <p>With the tree in this way we have 3 misclassifications.</p>
            <p>When we use the pruning, we use a leaf with label c1 because the number of instances is greater than c2 (7 &gt; 4).</p>
            <p>With the postpruning we have 2 misclassifications, while without pruning we have 3 misclassifications.</p>
            <p>We accept the prune because we have a gain in misclassification.</p>
            <p>We use the same for the right node.</p>
            <p>We have 1 misclassification in v7, and 0 in v6.</p>
            <p>If we prune the majority class is c2 but it corresponds to have 3 misclassifications which is higher than 1 and the solution in this case is to not prune, we don’t have advantages.</p>
            <p>This approach works because we use a pruning set, if we use the training set it doesn’t work.</p>
            <h4 class="unnumbered" data-number="" id="cost-complexity-method-and-postpruning">Cost Complexity Method and Postpruning</h4>
            <p>Another approach, used in CART, without the pruning set is to <strong>balance the resubstitution error and the complexity</strong> of the tree.</p>
            <p>It exploits the cost complexity function, that compares the number of leaves in the tree and the resubstitution error of the tree. The aim is to find a trade-off between them.</p>
            <p><span class="math inline">\(Cost complexity = Resubstitution Error + ß · Number of leaf nodes\)</span></p>
            <p>where the Resubstitution error is the misclassification rate computed on the training set and ß is a penalty per additional terminal nodes</p>
            <p>Search for the right-sized tree:</p>
            <ul>
            <li><p>prune or collapse some of the branches of the largest tree from the bottom up, using the cost complexity parameter, and cross-validation or an independent test sample to measure the predictive accuracy of the pruned tree.</p></li>
            <li><p>use the resubstitution cost for ranking the subtrees and generating a tree sequence table ordered from the most complex tree at the top to a less complex tree at the bottom</p></li>
            <li><p>Identify the minimum-cost tree and pick an optimal tree as the tree within one standard error of the minimum cost tree. An optimal tree should be the one with the smallest terminal nodes among those that lie within one standard error of the minimum-cost tree.</p></li>
            </ul>
            <p>The idea is to use a decision tree with the minimum number of nodes that allow us to achieve reasonable performance in resubstitution error or cross validation error.</p>
            <p>We just generate several decision trees by pruning the tree and we compute the resubstitution relative cost.</p>
            <p>We order the tree by increasing resubstitution relative cost. Also, in term of number of nodes we have a decreasing order because when we increase the resubstitution cost we increase the number of nodes.</p>
            <figure>
            <img src="../media/image240.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We just pick up a tree that guarantee us the lowest cross-validation and pickup a tree with a cost very close to the lowest cost we have with cross-validation, that guarantee us to have the minimum number of nodes.</p>
            <p>This is an approach that tries to select a tree with the lowest number of nodes considering the resubstitution error and the cross-validation error.</p>
            <p>Pessimistic pruning is similar to the cost complexity pruning but does not use a pruning set.</p>
            <p>It adjusts the error rates obtained by the training set by adding a penalty, computed by adopting a heuristic approach based on statistical theory. If the error rate in the node is lower than the error rate in the subtree originated from the node, then the subtree is pruned.</p>
            <p>In postpruning, pruned trees tend to be more compact than their unpruned counterparts, but can still suffer from two problems.</p>
            <ul>
            <li><p><strong>Repetition</strong> - where an attribute is repeatedly tested along a given branch; this can happen when we have a continuous variable and we split it in two parts. We test the attribute with a threshold lower than 60 and we have two branches because we have binary splitting. We can also have another part of the tree with the same attribute with another split. We can have the same attribute repeated in another part of the tree.</p></li>
            <li><p><strong>Replication</strong> - duplicate subtrees exist within the tree. We could use the attributes in different part of the tree and this can produce the same subtree.</p></li>
            </ul>
            <figure>
            <img src="../media/image241.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We can have them because we prune part of the tree.</p>
            <p>Repetition and Replication can impede the accuracy and comprehensibility of a decision tree.</p>
            <p>The use of multivariate splits (splits based on a combination of attributes) can prevent these problems. Another approach is to use a different form of knowledge representation, such as rules, instead of decision trees.</p>
            <p>Indeed rule-based classifier can be constructed by extracting IF-THEN rules from a decision tree.</p>
            <h2 data-number="5.3" id="enhancements-to-decision-tree-and-challanges"><span class="header-section-number">5.3</span> Enhancements to Decision Tree and Challanges</h2>
            <p>We have a few problems:</p>
            <ul>
            <li><p><strong>The allow for continuous-valued attributes</strong> - Dynamically define new discrete-valued attributes that partition the continuous attribute value into a discrete set of intervals, but this is very computational expensive so we discretize before.</p></li>
            <li><p><strong>Handle missing attribute values</strong> - Typical solutions are assign the most common value of the attribute or assign probability to each of the possible values</p></li>
            <li><p><strong>Attribute construction</strong> - Where we create new attributes based on existing ones that are sparsely represented. This reduces fragmentation, repetition, and replication.</p></li>
            </ul>
            <p>Classification in large databases has a problem:</p>
            <ul>
            <li>Scalability: Classifying data sets with millions of examples and hundreds of attributes with reasonable speed</li>
            </ul>
            <ins>
            Decision Tree can be executed distributedly.
            </ins>
            <p>Why is decision tree induction popular?</p>
            <ul>
            <li><p>relatively faster learning speed (than other classification methods)</p></li>
            <li><p>convertible to simple and easy to understand classification rules</p></li>
            <li><p>can use SQL queries for accessing databases</p></li>
            <li><p>comparable classification accuracy with other methods</p></li>
            </ul>
            <p>We may have advantages in interpretability then other classifiers.</p>
            <p>It is also one of the first classifiers.</p>
            <p>Note: if the training set does not fit in memory, decision tree construction becomes inefficient due to swapping of the training tuples in and out</p>
            <h3 data-number="5.3.1" id="rainforest"><span class="header-section-number">5.3.1</span> RainForest</h3>
            <p>To make scalable the learning process of the decision trees we can use <strong>RainForest</strong>.</p>
            <p>It adapts to the amount of main memory available and applies to any decision tree induction algorithm.</p>
            <p>The idea is to use an AVC-set (of an attribute X ), where AVC stands for (Attribute, Value, Class_label.</p>
            <p>We have the projection of training dataset onto the attribute X and class label where counts of individual class label are aggregated.</p>
            <p>We have for each value of attributes the distribution on classes.</p>
            <p>The method maintains an AVC-set for each attribute at each tree node, describing the training tuples at node.</p>
            <p>Dimension of an AVC-set depends on number of distinct values of A and the number of classes.</p>
            <p>We will have all information to compute the info gain, we summarize the information to do that.</p>
            <figure>
            <img src="../media/image242.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We scan the DB but after that we can directly work on it.</p>
            <h3 data-number="5.3.2" id="boat-bootstrapped-optimistic-algorithm-for-tree-construction"><span class="header-section-number">5.3.2</span> BOAT (Bootstrapped Optimistic Algorithm for Tree Construction)</h3>
            <p>Another approach is to use statistical technique called bootstrapping to create several smaller samples (subsets), each fits in memory.</p>
            <p>Each subset is used to create a tree, resulting in several trees. These trees are examined and used to construct a new tree T’. It turns out that T’ is very close to the tree that would be generated using the whole data set together</p>
            <p>Advantages:</p>
            <ul>
            <li><p>Requires only two scans of DB. Makes one scan over the training database while collecting a small subset of the training database in-memory.</p></li>
            <li><p>Compute the final splitting criteria in only one scan over the training database. An incremental algorithm: can be used for incremental updates.</p></li>
            </ul>
            <h2 data-number="5.4" id="bayes-classification"><span class="header-section-number">5.4</span> Bayes Classification</h2>
            <p>The Bayes classifier is a statistical classifier: performs probabilistic prediction, i.e., predicts class membership probabilities. The foundation is based on Bayes’ Theorem.</p>
            <p>It predict class and also give us the probability of the prediction. We will have a list with classes and also probabilities to have that class,</p>
            <p>Performance: A simple Bayesian classifier, naïve Bayesian classifier, has comparable performance with decision tree and selected neural network classifiers</p>
            <p>We have incremental work, each training example can incrementally increase/decrease the probability that a hypothesis is correct — prior knowledge can be combined with observed data</p>
            <p>Standard Naive Classifier, even when Bayesian methods are computationally intractable, they can provide a standard of optimal decision making against which other methods can be measured</p>
            <p>Let X be a data tuple (“evidence”): class label is unknown. Let H be a hypothesis that X belongs to class C.</p>
            <p>The classification consists of determining P(H|<strong>X</strong>) (posteriori probability): the probability that the hypothesis holds given the observed data sample X.</p>
            <p>We can compute all probabilities for the different classes and use the class with the highest probability.</p>
            <p>X is in bold because it represents an instance characterized by a number of attributes. X is a combination of possible values for the attributes. We have to compute an high number of probabilities. We compute for all possible combination of inputs.</p>
            <p>Let us suppose that H is the hypothesis that a customer will buy a computer and a customer is characterized by age and income. Then, P(H|X) is the probability that X will buy a computer given his age and income.</p>
            <p>We have to compute probabilities for all possible combination of these three values.</p>
            <p>For instance, X is a 35-year-old customer with an income of $40,000.P(H) is the prior probability of H, and it is the the initial probability. It is the probability that a generic customer will buy a computer, regardless of age, income, ... It is the probability of having a specific class independently of the values.</p>
            <p>P(X) is the prior probability of X), it is the probability that sample data is observed</p>
            <p>e.g. Probability that a customer is 35 years old and earns $40,000.</p>
            <p>P(X|H) is the posteriori probability of X conditioned on H, likelyhood. It is the probability of observing the sample X, given that the hypothesis holds.</p>
            <p>Given that X will buy computer, the prob. that X is 31..40, and medium income.</p>
            <p>Note: P(H), P(X) and P(X|H) can be estimated from the given data.</p>
            <p>P(H|X) can be computed by the <strong>Bayes’ theorem</strong>. Given a training data X:</p>
            <figure>
            <img src="../media/image243.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Informally, this can be written asposteriori = likelihood x prior/evidence</p>
            <p>It predicts X belongs to Ci if and only if the probability P(Ci|X) is the highest among all the P(Ck|X) for all the k classes. If we have P(H|X) we compute it for each class and in output we give the class corresponding to the highest probability.</p>
            <p>Practical difficulty: requires initial knowledge of many probabilities, significant computational cost</p>
            <p>We go towards Naive Bayesian Classifier to solve this problem.</p>
            <p>Let D be a training set of tuples and their associated class labels, and each tuple is represented by an n- dimensional attribute vector X = (x1, x2, ..., xn). Suppose there are m classes C1, C2, ..., Cm.Classification objective: to determine the class having the highest posterior probability, i.e., the maximal P(Ci|X). This can be derived from Bayes’ theorem:</p>
            <figure>
            <img src="../media/image244.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Since P(X) is constant for all classes, only:</p>
            <figure>
            <img src="../media/image245.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>needs to be maximized.</p>
            <p>P(Ci) is easy to be estimated by |Ci,D| / |D|.</p>
            <p>The real problem is to compute P(X|Ci), we have to compute it for each possible combination of each value of all attributes.</p>
            <p>We can do it with this assumption: attributes are conditionally independent (i.e., no dependence relation between attributes). In that case:</p>
            <figure>
            <img src="../media/image246.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>This greatly reduces the computation cost. Only counts the class distribution.</p>
            <p>We just compute the probability of each value of each attribute given the class.</p>
            <p>We don’t have any more combination of values.</p>
            <p>Ex. P(age = old | C1) and so on.</p>
            <p>This reduces the computational time but we need to have that assumption.</p>
            <p>If Ak is categorical, P(xk|Ci) is the number of tuples in Ci having value xk for Ak divided by |Ci, D| (number of tuples of Ci in D).</p>
            <p>If Ak is continous-valued, P(xk|Ci) is usually computed based on Gaussian distribution with a mean μ and standard deviation σ:</p>
            <figure>
            <img src="../media/image247.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>and:</p>
            <figure>
            <img src="../media/image248.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>where μCi and σCi are the mean value and the standard deviation, respectively, of the values of attribute Ak for training tuples of class Ci.</p>
            <p>This is true if we can assume that the distribution is gaussian.</p>
            <h5 class="unnumbered" data-number="" id="example-3"><strong>Example:</strong></h5>
            <p>Let’s use the Naive Bayesian Classifier.</p>
            <figure>
            <img src="../media/image249.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Classes:</p>
            <p>C1:buys_computer = ‘yes’</p>
            <p>C2:buys_computer = ‘no’</p>
            <p>Data sample:</p>
            <p>X = (age =youth, Income = medium, Student = yes Credit_rating = Fair)</p>
            <p>First of all we have to compute the probability of classes, we just need to count the number of instances in which we have a class and divide by the number of instances.</p>
            <p><span class="math inline">\(P(Ci): P(buys\_computer = &quot;yes&quot;) = 9/14 = 0.643\)</span></p>
            <p><span class="math inline">\(P(buys\_computer = &quot;no&quot;) = 5/14= 0.357\)</span></p>
            <p>Then we compute <span class="math inline">\(P(X|Ci)\)</span> for each class, this corresponds to compute it for each possible value for each attribute and each class. <span class="math inline">\(P(age = &quot;youth&quot; \| buys\_computer = &quot;yes&quot;) = 2/9 = 0.222\)</span></p>
            <p><span class="math inline">\(P(age = &quot;youth&quot; \| buys\_computer = &quot;no&quot;) = 3/5 = 0.6P(income = &quot;medium&quot; \| buys\_computer = &quot;yes&quot;) = 4/9 = 0.444\)</span></p>
            <p><span class="math inline">\(P(income = &quot;medium&quot; \| buys\_computer = &quot;no&quot;) = 2/5 = 0.4\)</span></p>
            <p><span class="math inline">\(P(student = &quot;yes&quot; \| buys\_computer = &quot;yes) = 6/9 = 0.667\)</span></p>
            <p><span class="math inline">\(P(student = &quot;yes&quot; \| buys\_computer = &quot;no&quot;) = 1/5 = 0.2\)</span></p>
            <p><span class="math inline">\(P(credit\_rating = &quot;fair&quot; \| buys\_computer = &quot;yes&quot;) = 6/9 = 0.667\)</span></p>
            <p><span class="math inline">\(P(credit\_rating = &quot;fair&quot; \| buys\_computer = &quot;no&quot;) = 2/5 = 0.4\)</span></p>
            <p>We limit the computations for simplyfing the computation. We focus on the instance in which we have that class and we count the number of instance in which we have that value for that attribute.</p>
            <p>We obtain all probabilities we need to compute the probability of our customer given a specific class.</p>
            <p>X = (age = youth, income = medium, student = yes, credit_rating = fair)</p>
            <p>P(X|Ci) :</p>
            <p>P(X|buys_computer = “yes”) = 0.222 x 0.444 x 0.667 x 0.667 = 0.044</p>
            <p>P(X|buys_computer = “no”) = 0.6 x 0.4 x 0.2 x 0.4 = 0.019</p>
            <p>P(X|Ci)*P(Ci) :</p>
            <p>P(X|buys_computer = “yes”) * P(buys_computer = “yes”) = 0.028</p>
            <p>P(X|buys_computer = “no”) * P(buys_computer = “no”) = 0.007</p>
            <p>Therefore, X belongs to class (“buys_computer = yes”) because this class is characterized by the highest probability.</p>
            <p>It’s normal to have the low magnitude because we use different products between number less than 1.</p>
            <h3 data-number="5.4.1" id="avoiding-the-zero-probability-problem"><span class="header-section-number">5.4.1</span> Avoiding the Zero-Probability Problem</h3>
            <p>Naïve Bayesian prediction requires each conditional probability be non-zero. Otherwise, the predicted probability will be zero:</p>
            <figure>
            <img src="../media/image250.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Ex. Suppose a dataset with 1000 tuples, income=low (0), income= medium (990), and income = high (10)</p>
            <p>If we just apply this probability, for low we have 0.</p>
            <p>TO avoid it we can use <strong>Laplacian correction</strong> (or Laplacian estimator) that consists in adding 1 to each case.</p>
            <p>Prob(income = low) = 1/1003=0.001</p>
            <p>Prob(income = medium) = 991/1003=0.988</p>
            <p>Prob(income = high) = 11/1003=0.011</p>
            <p>It doesn’t affect a lot the second two.</p>
            <p>The “corrected” probability estimates are close to their “uncorrected” counterparts.</p>
            <p>Advantages:</p>
            <ul>
            <li><p>Easy to implement</p></li>
            <li><p>Good results obtained in most of the cases</p></li>
            </ul>
            <p>Disadvantages</p>
            <ul>
            <li><p>Assumption: class conditional independence, therefore loss of accuracy because of this assumption</p></li>
            <li><p>Practically, dependencies exist among variables E.g., hospitals: patients: Profile: age, family history, etc. Symptoms: fever, cough etc., Disease: lung cancer, diabetes, etc. We have a relation between attributes in this case.</p></li>
            <li><p>Dependencies among these cannot be modeled by Naïve Bayesian Classifier If we have dependency in attributes we can’t expect to have high performance.</p></li>
            </ul>
            <p>We can deal with these dependencies using <strong>Bayesian Belief Networks</strong>. They are also known as Bayesian networks, probabilistic networks and they allow the representation of dependencies among subsets of attributes (both discrete- and continuous-valued). They are defined by two components: a directed acyclic graph and a set of conditional probability tables. A directed acyclic graphical model of casual relationship represents dependencies among variables, gives a specification of joint probability distribution.</p>
            <figure>
            <img src="../media/image251.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Nodes: random variables</p>
            <p>Links: dependency</p>
            <p>X and Y are the parents of Z, and Y is the parent of P</p>
            <p>No dependency between Z and P</p>
            <p>Has no loops/cycles</p>
            <p>Nodes may correspond to actual attributes given in the data or to “hidden conditional probability tables variables” believed to form a relationship (e.g., in the case of medical data, a hidden variable may indicate a syndrome, representing a number of symptoms that, together, characterize a specific disease).</p>
            <p><strong>CPT</strong>: Conditional Probability Table for variable LungCancer:</p>
            <figure>
            <img src="../media/image252.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>It shows the conditional probability for each possible combination of the values of its parents</p>
            <figure>
            <img src="../media/image253.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If we have LC, we know that it depends on S and fH and the CPT gives us the probability given possible values for the parents, the probability to have LC or not LC</p>
            <p>If we have cancer in family history and the patient is smoker we have a probability of 0.8, and so on.</p>
            <p>We can use these networks in several scenarios.</p>
            <h3 data-number="5.4.2" id="training-a-bayesian-network"><span class="header-section-number">5.4.2</span> Training a Bayesian Network</h3>
            <p>Let X=(x1,..,xn) be a data tuple described by the variables or attributes Y1, ..., Yn, respectively.</p>
            <p>Note that each variable is conditionally independent of its nondescendants in the network graph, given its parents.</p>
            <p>A complete representation of the existing joint probability distribution can be obtained by the following equation:</p>
            <figure>
            <img src="../media/image254.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Just an example. Suppose that we have five variables A, B, C, D and E.</p>
            <figure>
            <img src="../media/image255.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If we do not specify the dependencies explicitly, then all the variables are assumed to be dependent on each other.</p>
            <p>It means that:</p>
            <p>p(A,B,C,D,E) = p(A|B,C,D,E)*p(B|C,D,E)*p(C|D,E)*p(D|E)*P(E)</p>
            <p>If the dependencies are explicitly modeled, we can reduce the computation:</p>
            <p>p(A,B,C,D,E) = p(A|B)*p(B|C,E)*p(C|D)*p(D)*p(E)</p>
            <p>This is possible because we modeled explicitly the dependencies.</p>
            <p>A node within the network can be selected as an “output” node, representing a class label attribute.</p>
            <p>There may be more than one output node.</p>
            <p>Rather than returning a single class label, the classification process can return a probability distribution that gives the probability of each class.</p>
            <p>Belief networks can be used to answer probability of evidence queries (e.g., what is the probability that an individual will have LungCancer, given that they have both PositiveXRay and Dyspnea) and most probable explanation queries (e.g., which group of the population is most likely to have both PositiveXRay and Dyspnea).</p>
            <p><strong>We have 4 possible scenarios for Bayesian Networks:</strong></p>
            <p><strong>Scenario 1</strong>: Given both the network structure and all variables observable: compute only the CPT entries.</p>
            <p><strong>Scenario 2</strong>: Network structure known, some variables are hidden; gradient descent (greedy hill-climbing) method, i.e., search for a solution along the steepest descent of a criterion function. Some nodes correspond to actual variables, others do not. We must compute the CPT for all nodes and to perform this we don’t have evidence for all variables. We just need to exploit a method.</p>
            <ul>
            <li><p>Weights (CPT entries) are initialized to random probability values</p></li>
            <li><p>At each iteration, it moves towards what appears to be the best solution now, without backtracking</p></li>
            <li><p>Weights are updated at each iteration and converge to local optimum</p></li>
            </ul>
            <p><strong>Scenario 3</strong>: Network structure unknown, all variables observable: search through the model space to reconstruct network topology, because we have all information.</p>
            <p><strong>Scenario 4</strong>: Unknown structure, all hidden variables: No good algorithms known for this purpose.</p>
            <p>We focus on the 2nd scenario.</p>
            <p>Let wijk be a CPT entry for the variable Yi=yij having the parents: Ui=uik, where wijk=P(Yi=yij|Ui=uik).</p>
            <p>If wijk is the upper leftmost CPT entry in the previous example, then Yi is LungCancer, yij is its value “yes”, Ui lists the parent nodes of Yi, namely {FamilyHistory, Smoker} and uik list the values of the parent nodes, namely {“yes”, “yes”}.</p>
            <p>A gradient descent strategy is used to search for the wijk values that best model the data, based on the assumption that each possible setting of wijk is equally likely.</p>
            <p>It searches for a solution along the negative of the gradient of a criterion function.</p>
            <p>This function is that we maximize the probability assigned by the network to the observed data when the CPT parameters are set to w. We try to maximize this probability:</p>
            <figure>
            <img src="../media/image256.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>This can be done by following the gradient of Pw(D).</p>
            <p>The algorithm proceeds as follows:</p>
            <ul>
            <li>Compute the gradients (for each training tuple Xd)</li>
            </ul>
            <figure>
            <img src="../media/image257.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <ul>
            <li>Take a small step in the direction of the gradient (where l is the learning rate). If l is very low the climb is slow, if large we climb the ill very fast but we may pass-over the peak so we should rule this learning rate so that we approach the peak fast but without passing the peak itself.</li>
            </ul>
            <figure>
            <img src="../media/image258.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <ul>
            <li>Renormalize the weights (all the weights have to be between 0 and 1 being probabilities and <img src="../media/image259.png" alt="alt" /> must equal 1 for all i,k.).</li>
            </ul>
            <h2 data-number="5.5" id="rule-based-classification"><span class="header-section-number">5.5</span> Rule-based Classification</h2>
            <p>We have classifiers based on rules.</p>
            <p>The knowledge is represented in the form of IF-THEN rules</p>
            <p>e.g. IF age = youth AND student = yes THEN buys_computer = yes</p>
            <p>When we receive an unlabeled object we just try if the rule can be fired, we see if the antecedent condition is satisfied and in case the consequent is triggered.</p>
            <p>Assessment of a rule: coverage and accuracy</p>
            <p>The coverage represents how many instances the rule cover.</p>
            <p>ncovers = number of instances covered by rule R</p>
            <p>ncorrect = number of instances correctly classified by rule R</p>
            <p>coverage(R) = ncovers /|D|, where D is the training data set</p>
            <p>accuracy(R) = ncorrect / ncovers</p>
            <p>We want to achieve ncorrect = ncovers.</p>
            <p>If more than one rule is triggered, we need conflict resolution.</p>
            <p>It happens when one instance fire more than ore rule and the consequent is different.</p>
            <p>Possible conflict resolution strategies can be achieved fixing orders.</p>
            <ul>
            <li><p>Size ordering: we assign the highest priority to the triggering rules that has the “toughest” requirement (i.e., with the most attribute tests (rule antecedent size)). We see this rule as very specialized and so trustable.</p></li>
            <li><p>Class-based ordering: decreasing order of prevalence (rules for the most frequent class come first) or misclassification cost per class (rules for the class with the highest cost come first)</p></li>
            <li><p>Rule-based ordering (decision list): rules are organized into one long priority list, according to some measure of rule quality (accuracy, coverage or size) or based on advice from domain experts.</p></li>
            </ul>
            <blockquote>
            <p>Of course, each rule in a decision list implies the negation of the rules that come before it in the list (difficult to interpret). If I’m at the 3rd, the two before weren’t fired.</p>
            </blockquote>
            <p>How can we mine rules?</p>
            <p>1. Rule extraction from a decision tree</p>
            <figure>
            <img src="../media/image260.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We can extract rules from a decision tree. We travel each path from the root to the leaves and for each path we generate a rule.</p>
            <ul>
            <li><p>Rules are easier to understand than large trees</p></li>
            <li><p>One rule is created for each path from the root to a leaf</p></li>
            <li><p>Each attribute-value pair along a path forms a conjunction: the leaf holds the class prediction</p></li>
            <li><p>Rules are mutually exclusive (no rule conflict) and exhaustive (one rule for each possible attribute-value)</p></li>
            </ul>
            <p>Note: one rule per leaf! With decision trees which suffer from repetition and replication, the extracted rule base can be difficult to interpret</p>
            <p>For example:</p>
            <figure>
            <img src="../media/image261.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We have 5 leaves so we will have 5 rules. If we have only one branch, we will have only one condition in the antecedent.</p>
            <h3 data-number="5.5.1" id="rule-extraction-from-a-decision-tree-pruning-the-rules"><span class="header-section-number">5.5.1</span> Rule Extraction from a Decision Tree: Pruning the Rules</h3>
            <p>Let’s see how we can prune the rule set.</p>
            <p>Each condition which does not improve the estimated accuracy of the rule can be pruned.</p>
            <p>C4.5 uses a pessimistic approach for counteracting the bias generated by using the training set.</p>
            <p>We try to generalize the rules. Further, any rule which does not contribute to the overall accuracy is pruned.</p>
            <p>After pruning, the rules will no longer be mutually exclusive and exhaustive.</p>
            <p>C4.5 adopts a class-based ordering scheme: groups the rules per class and then determines a ranking of these class rule sets to minimize the number of false-positive errors (the rule predicts a class C, but the actual class is not C). The class rule set with the least number of false positives is examined first.</p>
            <p>We also have a default class, the class which contains the highest number of tuples not covered by any rule (the majority class will likely have many rules for its tuples).</p>
            <p>If we do not prune, we have mutually exclusiveness and exhaustiveness, if we apply pruning, we lose these characteristic.</p>
            <p>Rule Induction: Sequential Covering Method</p>
            <p>2. <strong>Heuristic approaches</strong></p>
            <p>They work in a sequential way.</p>
            <p>They extract rules directly from training data, using some heuristics.</p>
            <h4 class="unnumbered" data-number="" id="sequential-covering-algorithm">Sequential Covering Algorithm</h4>
            <p>A typical sequential covering algorithm is <strong>FOIL</strong>.</p>
            <p>Rules are learned sequentially, each for a given class Ci will cover many tuples of Ci but, ideally, none (or few) of the tuples of other classes</p>
            <p>Steps:</p>
            <ul>
            <li><p>Rules are learned one at a time</p></li>
            <li><p>Each time a rule is learned, the tuples covered by the rules are removed, because we want to focus on the generation of the next rule on other instances.</p></li>
            <li><p>The process repeats on the remaining tuples unless termination condition, e.g., when no more training examples or when the quality of a rule returned is below a user-specified threshold. This is a sequential approach, so we need to fix a stopping condition.</p></li>
            </ul>
            <p>In terms of performance, it is similar to decision tree induction but in this case we learn rule by rule.</p>
            <p>When we use a decision-tree we learn a set of rules simultaneously.</p>
            <p>This is the difference, but in terms of performance we have approximately the same performance.</p>
            <p>It is important to think in terms of interpretability.</p>
            <p>The sequential covering algorithm works this way:</p>
            <p><em>while (enough target tuples left)</em></p>
            <p><em>generate a rule</em></p>
            <p><em>remove positive target tuples satisfying this rule</em></p>
            <p>positive means covered by the rule in the same class.</p>
            <figure>
            <img src="../media/image262.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Formally we have this algorithm:</p>
            <figure>
            <img src="../media/image263.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We start with an empty rule set and for each class and we repeat the three statements until terminating condition.</p>
            <p>How is possible to learn one rule?</p>
            <p>To generate a rule:</p>
            <p><em>while(true)</em></p>
            <p><em>find the best predicate p</em></p>
            <p><em>if foil-gain(p) &gt; threshold then</em></p>
            <p><em>add p to current rule</em></p>
            <p><em>else break</em></p>
            <p>If this metric is higher than a threashold we add b to the current rule.</p>
            <figure>
            <img src="../media/image264.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We select one attribute with a specific value, this is our condition. If we use just one condition, we cover a number of instances.</p>
            <p>Positive examples are example covered by the rule and belong to the same class we used in the consequent.</p>
            <p>All other examples belonging to other classes are negative examples.</p>
            <p>Using one condition we expect a cover of a lot of instances but there is a higher confusion.</p>
            <p>We can then add another condition in the antecedent part, it’s like we start from a large subspace, and we reduce it.</p>
            <p>Each condition introduces other constraints, we add conditions with the aim of reducing the number of negative examples.</p>
            <p>But we reduce also positive examples, because we reduce the instances in which rule is fired.</p>
            <p>This is a greedy depth-first strategy, we try to decide the optimal condition step by step. We cannot guarantee the global optimum but adding a condition in the antecedent we are performing an optimization.</p>
            <ul>
            <li>Start with the most general rule possible:</li>
            </ul>
            <blockquote>
            <p>condition (antecedent part) = empty</p>
            <p>This rule is always fired and cover all instances.</p>
            <p>Suppose our training set, D, consists of loan application data. Attributes regarding each applicant include their age, income, education level, residence, credit rating, and the term of the loan.</p>
            <p>We start with the rule:</p>
            <figure>
            <img src="../media/image265.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Then, we consider each possible attribute that may be added to the rule.</p>
            <p>We must use some metric to decide which is the best attribute to add in the rule.</p>
            </blockquote>
            <figure>
            <img src="../media/image266.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We must choose the metric. The best one should take in consideration accuracy.</p>
            <p>We must select the condition that allow us to achieve the highest accuracy, which is the number of positive examples over all examples covered by the rule.</p>
            <p>But we could have this situation:</p>
            <figure>
            <img src="../media/image267.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>R1 = 95% accuracy</p>
            <p>R2 = 100% accuracy</p>
            <p>R1 in terms of accuracy is lower but R2 has a very small coverage.</p>
            <p>We would like to use accuracy but also, we have to take in consideration the number of instances.</p>
            <p>If we have few instances, we also can reach overtraining. If we select rules that cover a limited number of instances, we have the risk to produce a classifier with overtraining.</p>
            <p>We must take in consideration accuracy and coverage.</p>
            <p>We use a rule-quality measure that is <strong>Foil-gain</strong>, that assesses info_gain by extending condition (FOIL = First Order Inductive Learner).</p>
            <figure>
            <img src="../media/image268.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>where <span class="math inline">\(pos\)</span> <span class="math inline">\((pos&#39;)\)</span> and <span class="math inline">\(neg\)</span> <span class="math inline">\((neg&#39;)\)</span> are the numbers of positive and negative tuples covered by current rule R (new rule R’, rule obtained adding one condition).</p>
            <p>We compare the rule we are producing adding one condition at the rule we have before the step.</p>
            <p>We decide if adding or not using this gain.</p>
            <p>Foil-gain favors rules that have high accuracy and cover many positive tuples.</p>
            <p>In fact, pos’ express the coverage of positive tuples, and in the parenthesis we take in consideration the accuracy we have,</p>
            <p>In the algorithm we use it and we stop when we can’t add other conditions.</p>
            <p>We also have <strong>rule pruning</strong>.</p>
            <p>We expect that when we remove instances from training set we expect to create rules very specialized.</p>
            <p>This is the reason why we typically have rule pruning.</p>
            <p>Learn_One_Rule does not employ a test set when evaluating rules. Thus, the evaluation is optimistic. Rule pruning based on an independent set of test tuples.</p>
            <p>Pruning is carried out by removing a conjunct, we remove a condition at each step, and we test if we had some gain.</p>
            <p>Removing a rule, we enlarge the subspace covered by the rule. By convention, this approach, RIPPER, starts with the most recently added conjunct when considering pruning.</p>
            <p>FOIL uses a simple yet effective method:</p>
            <ul>
            <li><img src="../media/image269.png" title="fig:" alt="alt" /></li>
            </ul>
            <p>If FOIL_Prune is higher for the pruned version of R, prune R.</p>
            <p>Conjuncts are pruned one at a time if this results in an improvement.</p>
            <p>We are working with a pruning set, not using for the training.</p>
            <p>If we would work with the training set, we would not have any advantage.</p>
            <h2 data-number="5.6" id="classification-by-using-frequent-patterns"><span class="header-section-number">5.6</span> Classification by Using Frequent Patterns</h2>
            <p>At the end of the execution of the algorithms we saw we had frequent patterns, from which we could extract association rules.</p>
            <p>We can use frequent patterns for extracting association rules.</p>
            <p>In the classification problem, association rules have a specific format, in the consequent part we have the class.</p>
            <p>We mine data to find strong associations between frequent patterns (conjunctions of attribute-value pairs) and class labels:</p>
            <p>Association rules are generated in the form of:</p>
            <p>p1 ^ p2 ... ^ pl =&gt; “A class = C” (conf, sup)</p>
            <p>In this analysis we exploit transactions.</p>
            <p>The consequent part is one of the possible classes.</p>
            <p>In frequent pattern analysis we exploit transactions. When we talk about classification, we don’t have them, but we have instances.</p>
            <p>Each item corresponds to a possible value of our attribute.</p>
            <p>From this perception we can apply the same algorithms we have</p>
            <p>The objects we have in the training set are our transactions, a set of items and the last item is the class.</p>
            <p>When we apply this approach, we can generate a high number of rules.</p>
            <p>This type of classifiers is very effective because:</p>
            <ul>
            <li><p>They explore highly confident associations among multiple attributes and may overcome some constraints introduced by decision-tree induction, which considers only one attribute at a time. In this way we explore all conjunctions and we expect higher performances.</p></li>
            <li><p>Associative classification has been found to be often more accurate than some traditional classification methods, such as C4.5 (used to learn decision trees).</p></li>
            </ul>
            <p><strong>CBA (Classification Based on Associations)</strong> is an approach of classification with association rules.</p>
            <p>This approach mine possible association rules in the form of: <span class="math inline">\(cond-set\)</span> (a set of attribute-value pairs) =&gt; class label</p>
            <p>Build the classifier with an heuristic method, where the rules are organized according to decreasing precedence based on confidence and then support</p>
            <p>If a set of rules has the same antecedent, then the rule with the highest confidence is selected to represent the set.</p>
            <p>Classify a new tuple:</p>
            <ul>
            <li><p>the first rule satisfying the tuple is used to classify it</p></li>
            <li><p>The classifier also contains a default rule, having the lowest precedence</p></li>
            </ul>
            <p><strong>CMAR (Classification based on Multiple Association)</strong> is another approach.</p>
            <p>Build the classifier: adopts a variant of the FP-growth algorithm to find the complete set of rules satisfying the minimum confidence and minimum support thresholds.</p>
            <p>Rule pruning whenever a rule is inserted into the rule base:</p>
            <ul>
            <li><p>Given two rules, R1 and R2, if the antecedent of R1 is more general than that of R2 and conf(R1) &gt;= conf(R2), then R2 is pruned</p></li>
            <li><p>Prunes rules for which the rule antecedent and class are not positively correlated, based on a χ2 test of statistical significance</p></li>
            </ul>
            <p>When we have to classify a new tuple:</p>
            <ul>
            <li><p>If the rules matching the new object are not consistent in class label, the tuple we are considering fire rules with different consequent, then statistical analysis on multiple rules are performed.</p>
            <ul>
            <li><p>Rules are divided into groups according to class labels</p></li>
            <li><p>Uses a weighted χ2 measure to find the strongest group of rules, based on statistical correlation of rules.</p></li>
            <li><p>The new tuple is assigned to the class label of the strongest group.</p></li>
            </ul></li>
            </ul>
            <p>CMAR has slightly higher average accuracy and more efficient use of memory than CBA.</p>
            <p><strong>CPAR (Classification based on Predictive Association Rules)</strong></p>
            <ul>
            <li>Build the classifier: Generation of predictive rules based on a rule generation algorithm (FOIL-like analysis) rather than frequent itemset mining. In FOIL, each time a rule is generated, the positive samples it satisfies are removed. In CPAR, the covered tuples remain under consideration, but reducing their weight.</li>
            </ul>
            <p>When we classify a new tuple:</p>
            <ul>
            <li><p>Rules are divided into groups according to class labels</p></li>
            <li><p>The best k rules, based on expected accuracy, of each group are used to predict the class label</p></li>
            </ul>
            <p>High efficiency, accuracy like CMAR.</p>
            <p>We can generate standard metrics for generating association rules, like FP_Growth.</p>
            <p>But we have to solve the conflict problem, we have high probability that an unlabeled tuple can fire rules with different consequent.</p>
            <p>The strategy we use is to determine how much compact are the group of rules related to the same class activated by the instance.</p>
            <p>They tried to use different heuristic to select rules.</p>
            <p>They all exploit techniques from rule-based classifiers (FOIL) or frequent pattern analysis but have a different strategy to perform conflict resolution.</p>
            <p>There is no best strategy, we have to choose the best for our application.</p>
            <h2 data-number="5.7" id="lazy-learners"><span class="header-section-number">5.7</span> Lazy Learners</h2>
            <p>Lazy vs. eager learning:</p>
            <ul>
            <li><p><strong>Lazy learning</strong> (e.g., instance-based learning): Simply stores training data (or only minor processing), they don’t learn a model. They just wait until it is given a test tuple.</p></li>
            <li><p><strong>Eager learning</strong> (the so far discussed methods): Given a set of training tuples, constructs a classification model before receiving new (e.g., test) data to classify</p></li>
            </ul>
            <p>Lazy learners spend less time in training but more time in predicting. For some sets we spend a lot of time in classification.</p>
            <p>We can save learning time, but we spend more in the classification phase.</p>
            <p>In terms of accuracy, lazy methods effectively use a richer hypothesis space since it uses many local linear functions to form an implicit global approximation to the target function.</p>
            <p>We may have a risk of overtraining.</p>
            <p>Eager: must commit to a single hypothesis that covers the entire instance space</p>
            <p>They are Instance-based learning: Store training examples and delay the processing (“lazy evaluation”) until a new instance must be classified.</p>
            <p>Typical approaches are:</p>
            <ul>
            <li><p><strong>k-nearest neighbor</strong> approach, where instances represented as points in a Euclidean space. We assume that our instances are represented by numerical attributes.</p></li>
            <li><p><strong>Locally weighted regression</strong>: constructs local approximation. We exploit locally the instances in the training set.</p></li>
            <li><p><strong>Case-based reasoning</strong>: uses symbolic representations and knowledge-based inference. We have to understand one case similar to our case.</p></li>
            </ul>
            <h3 data-number="5.7.1" id="k-nearest-neighbor-algorithm"><span class="header-section-number">5.7.1</span> K-Nearest Neighbor Algorithm</h3>
            <p>All instances correspond to points in the n-D space.</p>
            <p>The nearest neighbor are defined in terms of Euclidean distance, <span class="math inline">\(dist(X_1, X_2)\)</span>.</p>
            <p>Target function could be discrete- or real-valued, we can use them for classification or prediction.</p>
            <p>For discrete-valued, k-NN returns the most common value among the k training examples nearest to <span class="math inline">\(x_q\)</span>, the unlabeled target.</p>
            <p>We consider the majority class for these neighbors.</p>
            <p>We receive the unlabeled data, we compute the distance between the point and all instances in the training set to determine the K-Nearest Neighbors.</p>
            <p>Then, we sort the distances in an increasing order and pickup the first k points.</p>
            <p>The complexity is linear with the number of instances in the data set, we have O(n*f), where f is the number of attributes.</p>
            <p>The complexity for the sorting algorithm is O(n*logn) for some algorithms.</p>
            <p>If we fix k at the beginning we can speed-up the computation actually.</p>
            <figure>
            <img src="../media/image270.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>The optimal value of k is hard to choose.</p>
            <p>How is possible to determine a good value for k?</p>
            <p>If we select k = 1, we consider the closest point to the unlabeled point.</p>
            <p>But we may have as nearest point one point of another class and classify the point to a wrong class.</p>
            <p>We can mitigate the problem using a higher value for k.</p>
            <p>Experimentally, we could use increasing values of k and chooses k with the maximum accuracy, using a validation set.</p>
            <p>Generally, the larger the number training instances, the larger the value of k. It depends on the distribution of classes; we have to text experimentally.</p>
            <p>How can the distance be computed for non-numeric attributes (categorical attributes)?Compare the corresponding values of the attributes: if they are identical, the difference is 0; otherwise, is 1.</p>
            <p>What about missing values?</p>
            <p>If a value of a given attribute is missing in one of the two tuples, then assume the maximum possible difference.</p>
            <p>This difference is 1 for nominal attributes and for numeric attributes when the value is missing in both the tuples.</p>
            <p>Choice of the distance: different distances can be used for incorporating attribute weighting and the pruning of noisy data tuples.</p>
            <p>Computational efficiency: 1-NN requires O(|D|) comparisons.</p>
            <ul>
            <li><p>By sorting and arranging the tuples into search trees, the number of comparisons can be reduced to O(log(|D|))</p></li>
            <li><p>Parallel implementations, maybe splitting dataset into subsets, compute in each subset and merge in some way</p></li>
            <li><p>Partial distance: use only a subset of the n attributes; if the distance is higher than a threshold, then computation of distance is stopped</p></li>
            <li><p>Editing methods: remove training tuples that prove to be useless.</p></li>
            </ul>
            <p>For reducing complexity, we have to reduce the number of instances in the training set and we have to select them in a correct way.</p>
            <h4 class="unnumbered" data-number="" id="editing-methods">Editing Methods</h4>
            <p><strong>Wilson editing</strong></p>
            <p>Wilson editing cleans interclass overlap regions, thereby leading to smoother boundaries between classes.</p>
            <figure>
            <img src="../media/image271.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>breaking ties randomly if we have the same number of neighbors for each class.</p>
            <p>This corresponds on eliminating an object that it is in a zone in which most neighbors belong to other classes. We must perform this reduction one time.</p>
            <p>We remove objects in zones in where we have objects of other classes and consider them as noise.</p>
            <figure>
            <img src="../media/image272.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>It depends on the value of k, in the first case we use k = 1 so we eliminate also the black one.</p>
            <p>With this approach we remove instances that may cause misclassifications.</p>
            <p><strong>Multi-Edit</strong></p>
            <p>We have to edit in such a way to maintain the performance of our classifier.</p>
            <p>Multi-edit repeatedly applies Wilson editing to N random subsets of the original dataset until no more examples are removed.</p>
            <figure>
            <img src="../media/image273.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We use one of the subsets for training and others for test and we apply all these combinations.</p>
            <p><strong>Citation Editing</strong></p>
            <p>We exploit an analogy here, which is that if a paper cites another article, the paper is related to that article. Similarly, if a paper is cited by an article, the paper is also related to that article. Thus both the citers and references are considered to be related to a given paper.</p>
            <figure>
            <img src="../media/image274.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We use KNN for oi but also the citers, that are objects in o which count oi among its KNN. We use Wilson editing exploiting the views of citers and cited.</p>
            <p><strong>Supervised Clustering</strong></p>
            <p>In this approach we exploit clustering, grouping objects related to each other.</p>
            <p>The idea is to use one representative for the cluster, which has to be label, and use it instead of elements in the cluster.</p>
            <p>O is replaced by subset Or which consists of cluster representatives that have been selected by the supervised clustering algorithm.</p>
            <figure>
            <img src="../media/image275.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We have the objective to preserve accuracy but reducing the number of instances.</p>
            <p>We have to see if these methods reach our aim.</p>
            <p>Let’s compare the different approaches by using a dataset.</p>
            <p>We use two types of datasets.</p>
            <figure>
            <img src="../media/image276.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>In the first column we use the overall training set, in the second Wilson editing, and so on.</p>
            <p>The Wilson, Citation and SC Editing allow us to improve performances.</p>
            <p>The second row give us:</p>
            <figure>
            <img src="../media/image277.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>which is the percentage of the dataset we can reduce.</p>
            <p>SC Editing guarantee us the highest compression rate, in fact we use representatives.</p>
            <p>In the other dataset SC Editing do not guarantee a similar performance but the compression rate is huge. During the classification phase this guarantee us to be very fast.</p>
            <p>We can conclude that the first three can typically increase the accuracy but are not able to reduce a lot the instances, while SC editing summarize a lot the instances but we may have loss in terms of accuracy.</p>
            <p>Visualization of the application of editing methods on Complex9_RN323 dataset:</p>
            <figure>
            <img src="../media/image278.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>With Citation and Wilson editing we delete points but not so many. With SC Editing we have just few selected points.</p>
            <h4 class="unnumbered" data-number="" id="k-nearest-neighbours-for-prediction">K-Nearest Neighbours for Prediction</h4>
            <p>k-NN for <em>real-valued prediction</em> for a given unknown tuple returns the mean values of the k nearest neighbors.</p>
            <p>We can also use distance-weighted nearest neighbor algorithm.In this case it weights the contribution of each of the k neighbors according to their distance to the query xq. We can give greater weight to closer neighbors:</p>
            <figure>
            <img src="../media/image279.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We weight more the points close to Xq.</p>
            <p>KNN is quite robust to noisy data by averaging k-nearest neighbors, especially when we increment k.We have a curse of dimensionality because we use distance between neighbors, and it could be dominated by irrelevant attributes.</p>
            <p>To overcome it, we can use axes stretch or elimination of the least relevant attributes.</p>
            <h3 data-number="5.7.2" id="case-based-reasoning-cbr"><span class="header-section-number">5.7.2</span> Case-Based Reasoning (CBR)</h3>
            <p>Case-based reasoning (CBR) is the process of solving new problems based on the solutions of equal or similar past problems.</p>
            <p>The core assumption is that similar problems have similar solutions.</p>
            <p>If I found a good solution to that case, I can apply this solution here too.</p>
            <p>This is hard to implement because we have to compare cases for their similarity.</p>
            <p>We use a database of problem solutions to solve new problems</p>
            <p>We store symbolic description (tuples or cases) and not points in a Euclidean space.</p>
            <p>Everyday examples of CBR:</p>
            <ul>
            <li><p>A lawyer who advocates a particular outcome in a trial based on legal precedents or a judge who creates case law.</p></li>
            <li><p>An engineer copying working elements of nature (practicing biomimicry) is treating nature as a database of solutions to problems.</p></li>
            </ul>
            <p>Methodology:</p>
            <p>Cases are represented by rich symbolic descriptions (e.g., function graphs).</p>
            <p>We search for similar cases, multiple retrieved cases may be combined.</p>
            <p>When a new (successful) solution to the new problem is found, a new experience is made, which can be stored in the case-base to increase its competence, thus implementing a learning behavior.</p>
            <p>I retrieve old cases, adopt my solution and store my solution with the case.</p>
            <p>I increase the possibility to use this solution for other cases.</p>
            <p>Tight coupling between case retrieval, knowledge-based reasoning, and problem solving.</p>
            <p>The CBR cycle consists of 4 sequential steps:</p>
            <figure>
            <img src="../media/image280.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We receive a new case, we retrieve similar cases, we analyze them, and we reuse the solutions we adopted in those cases, we revise this solution and we test this solution.</p>
            <p>Then we save it for the future.</p>
            <ul>
            <li>Retrieve</li>
            </ul>
            <p>One or several cases from the case base are selected, based on the modeled similarity. The retrieval task is defined as finding a small number of cases from the case-base with the highest similarity to the query, that can be described in a complex way. We must compare them considering this complexity. This is a k-nearest-neighbor retrieval task considering a specific similarity function. When the case base grows, the efficiency of retrieval decreases. We need to apply methods that improve retrieval efficiency, e.g. specific index structures such as kd-trees, case-retrieval nets, or discrimination networks.</p>
            <ul>
            <li>Reuse</li>
            </ul>
            <p>Reusing a retrieved solution can be quite simple if the solution is returned unchanged as the proposed solution for the new problem.</p>
            <p>Otherwise, we have to adapt it to a specific case (if required, e.g. for synthetic tasks).</p>
            <p>We have Several techniques for adaptation in CBR: Transformational adaptation, we transform the solution; Generative adaptation, we generate a specific solution</p>
            <p>Most practical CBR applications today try to avoid extensive adaptation for pragmatic reasons.</p>
            <ul>
            <li>Revise</li>
            </ul>
            <p>In this phase, feedback related to the solution constructed so far is obtained.</p>
            <p>This feedback can be given in the form of a correctness rating of the result or in the form of a manually corrected revised case.</p>
            <p>The revised case or any other form of feedback enters the CBR system for its use in the subsequent retain phase.</p>
            <ul>
            <li>Retain</li>
            </ul>
            <p>The retain phase is the learning phase of a CBR system (adding a revised case to the case base).</p>
            <p>We add the case with the solution for new specific cases. Explicit competence models have been developed that enable the selective retention of cases (because of the continuous increase of the case-base).</p>
            <p>This is a lazy learner because we do not generate a model, but we exploit what we have in the learning set.</p>
            <p>Let’s take this example:</p>
            <figure>
            <img src="../media/image281.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>The first thing to do is to understand if we have similar cases to adopt the solution inside those cases. We search similar cases, and we discover that something is present in our case base but it’s not the same problem.</p>
            <p>When two cases are similar? We must establish it.</p>
            <p>The solution may change depending on the model of the car, we must take care of the complexity of this situation.</p>
            <p>If we find a solution, we can adapt it with the symptoms we have in the query.</p>
            <p>We have the front light and not the break light, but we can exploit similar solutions.</p>
            <p>If we apply the change and it works this is the solution we wanted.</p>
            <p>Then we can store the solution in our use-case.</p>
            <figure>
            <img src="../media/image282.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>It’s difficult to compute similarity.</p>
            <p>Challenges with this application are:</p>
            <p>Find a good similarity metric</p>
            <p>Indexing based on syntactic similarity measure, and when failure, backtracking, and adapting to additional cases</p>
            <p>It is a lazy learner because we use the training set without creating a model.</p>
            <h2 data-number="5.8" id="model-evaluation-and-selection"><span class="header-section-number">5.8</span> Model Evaluation and Selection</h2>
            <p>How can we measure accuracy? Are there other metrics to consider?</p>
            <p>We use a test set of class-labeled tuples instead of training set when assessing accuracy.</p>
            <p><strong>Methods for estimating a classifier’s accuracy:</strong></p>
            <ul>
            <li><p>Holdout method, random subsampling</p></li>
            <li><p>Cross-validation</p></li>
            <li><p>Bootstrap</p></li>
            </ul>
            <p><strong>Comparing classifiers:</strong></p>
            <ul>
            <li><p>Confidence intervals</p></li>
            <li><p>Cost-benefit analysis and ROC Curves</p></li>
            </ul>
            <p>A complete information is given by the <strong>confusion matrix</strong>.</p>
            <h3 data-number="5.8.1" id="confusion-matrix"><span class="header-section-number">5.8.1</span> Confusion Matrix</h3>
            <p>We consider a two-class problem but it can be generalized with more classes.</p>
            <figure>
            <img src="../media/image283.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Given m classes, an entry, <span class="math inline">\(CM_{i,j}\)</span> in a confusion matrix indicates the number of tuples in class i that were labeled by the classifier as class j.</p>
            <p>It may have extra rows/columns to provide totals.</p>
            <p>True positives are instances belonging to class C1 and classified as class C1, and so on.</p>
            <p>The confusion matrix gives us all information to decide if a classifier is working well and give us information about confusion between classes. The best situation is when it is diagonal.</p>
            <p>Example of Confusion Matrix:</p>
            <figure>
            <img src="../media/image284.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <h4 class="unnumbered" data-number="" id="classifier-evaluation-metrics-accuracy-error-rate-sensitivity-and-specificity">Classifier Evaluation Metrics: Accuracy, Error Rate, Sensitivity and Specificity</h4>
            <p>We can compare classifier in terms of TN or TP, taking in difference the two cases.</p>
            <p>By exploiting it we can define some metrics that summarize the confusion matrix:</p>
            <ul>
            <li><strong>Classifier Accuracy</strong>, or recognition rate: percentage of test set tuples that are correctly classified:</li>
            </ul>
            <p>Accuracy = (TP + TN)/All</p>
            <ul>
            <li><strong>Error rate</strong>: 1 - accuracy, or</li>
            </ul>
            <p>Error rate = (FP + FN)/All</p>
            <p>Accuracy is a good metric but if a value is good, depends on the distribution of the data in classes.</p>
            <p>If we have a class with 95% of data and the other class with only 5% we can have a stupid classifier that always choose the first class and reach a 95% of accuracy.</p>
            <p>This is called the <strong>Class Imbalance Problem</strong>.</p>
            <p>If I have a high value of TP but low in TN, the classifier is not that useful.</p>
            <p>One class may be rare, e.g. fraud, or HIV-positive.</p>
            <p>In case of imbalanced problem, we cannot use only the accuracy, we use pair of values.</p>
            <ul>
            <li><strong>Sensitivity</strong>: True Positive recognition rate</li>
            </ul>
            <p>Sensitivity = TP/P</p>
            <ul>
            <li><strong>Specificity</strong>: True Negative recognition rate</li>
            </ul>
            <p>Specificity = TN/N</p>
            <p>I should have for 1 in the best case.</p>
            <p>Higher their value and better is my classifier.</p>
            <h4 class="unnumbered" data-number="" id="classifier-evaluation-metrics-precision-and-recall-and-f-measures">Classifier Evaluation Metrics: Precision and Recall, and F-measures</h4>
            <p>We also have other metrics:</p>
            <ul>
            <li><strong>Precision</strong>: exactness - what % of tuples the classifier labeled as positive are actually positive.</li>
            </ul>
            <figure>
            <img src="../media/image285.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>It expresses the precision of the classifier.</p>
            <ul>
            <li><strong>Recall</strong>: completeness - what % of positive tuples the classifier labeled as positive</li>
            </ul>
            <figure>
            <img src="../media/image286.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Considering all real positive tuples how many are classifies positive.</p>
            <p>We have a perfect precision score of 1.0 when every tuple that the classifier labeled as belonging to class C does indeed belong to class C. However, it does not tell us anything about the number of class C tuples that the classifier mislabeled.</p>
            <p>The precision only is not enough to give us all the information we need.</p>
            <p>We have a perfect recall score of 1.0 when every item from class C was labeled as such, but it does not tell us how many other tuples were incorrectly labeled as belonging to class C.</p>
            <p>Again, it gives us a partial view of the result.</p>
            <p>We must use both, especially for imbalanced dataset.</p>
            <p>There is an inverse relationship between precision and recall.</p>
            <p>A medical classifier may achieve high precision by labeling all cancer tuples that present a certain way as cancer but may have low recall if it mislabels many other instances of cancer tuples.</p>
            <p>This means that if we analyze some positive with cancer, they are not classified as positive. If we have a low precision, instead, we should have a high value of FP, which means that we have healthy patients classified as positives.</p>
            <p>Which value has to be high depends on the application, if they’re not both high.</p>
            <p>If we must select the best between different classifier, if the dataset is not imbalanced, we have to use these two values.</p>
            <p>We can have a classifier that perform better in recall and the other that perform better in precision.</p>
            <p>Precision and recall are combined into a single measure.</p>
            <ul>
            <li><strong>F measure</strong> (F1 or F-score): harmonic mean of precision and recall</li>
            </ul>
            <figure>
            <img src="../media/image287.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <ul>
            <li><strong>Fß</strong>: weighted measure of precision and recall assigns ß times as much weight to recall as to precision:</li>
            </ul>
            <figure>
            <img src="../media/image288.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Commonly used Fß measures are F2 (weights recall more than precision) and F0.5 (weights precision more than recall).</p>
            <p>Let’s take this example.</p>
            <p>This confusion matrix is given:</p>
            <figure>
            <img src="../media/image289.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Precision = 90/230 = 39.13%</p>
            <p>Recall = 90/300 = 30.00%</p>
            <p>Let’s suppose we don’t consider that this is an unbalanced dataset, and we just use accuracy, which is (90+9560)/10000 = 96.4%, which gives us a high value.</p>
            <p>But we misclassified a lot of ill patients.</p>
            <p>This is an imbalanced dataset in fact, accuracy is not a good metric.</p>
            <p>Sensitivity is 30% and specificity is 98.56%. We find out that sensitivity is very low.</p>
            <p>For this application this is not good.</p>
            <p>Both precision and recall are low, and for unbalanced dataset these are very important values.</p>
            <p>When we evaluate the classifier, we can use accuracy if the dataset is balanced, we have approximately the same number of instances for all classes.</p>
            <p>We must work with the pairs we saw. If we have different classifiers we use the f-measure.</p>
            <h3 data-number="5.8.2" id="holdout-cross-validation-methods"><span class="header-section-number">5.8.2</span> Holdout &amp; Cross-Validation Methods</h3>
            <p>A unique execution of different classifiers cannot allow concluding that one classifier is better than another.</p>
            <p>In one trial, by chance one classifier could have an accuracy higher than the other We need to perform different trials. But how? We have only a labelled dataset.</p>
            <p><strong>Holdout method</strong></p>
            <p>Given data is randomly partitioned into two independent sets:</p>
            <ul>
            <li><p>Training set (e.g., 2/3) for model construction</p></li>
            <li><p>Test set (e.g., 1/3) for accuracy estimation</p></li>
            </ul>
            <p>We perform random sampling: a variation of holdout.</p>
            <p>We repeat holdout k times, accuracy = avg. of the accuracies obtained.</p>
            <figure>
            <img src="../media/image290.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>For each time we use the left part as training set and the right part as test set.</p>
            <p>Only if distribution are independent I can conclude that one classifier is better than another. If two distributions are equal, classifiers are equal in terms of performance.</p>
            <p><strong>Cross-validation</strong></p>
            <p>Is it also called k-fold, where k = 10 is the most popular choice.</p>
            <p>We randomly partition the data into k mutually exclusive subsets, folds, each approximately equal size. At i-th iteration, use Di as test set and others as training set.</p>
            <p>I repeat until all folds are used as test sets.</p>
            <p>With only one dataset we created different trials.</p>
            <p>Some variations:</p>
            <ul>
            <li><p>Leave-one-out: k folds where k = number of tuples, that is, only a sample is left out at a time for the test set. Suitable for small sized data.</p></li>
            <li><p>Stratified cross-validation: folds are stratified so that class dist. in each fold is approx. the same as that in the initial data</p></li>
            </ul>
            <figure>
            <img src="../media/image291.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <h3 data-number="5.8.3" id="bootstrap"><span class="header-section-number">5.8.3</span> Bootstrap</h3>
            <p>Bootstrap samples the given training tuples uniformly with replacement.</p>
            <p>i.e., each time a tuple is selected, it is equally likely to be selected again and re-added to the training set. It works well with small data sets.</p>
            <p>We have several bootstrap methods, and a commonly used is .<em>632 bootstrap</em>.</p>
            <p>A data set with d tuples is sampled d times, with replacement, resulting in a training set of d samples. The data tuples that did not make it into the training set end up forming the test set. About 63.2% of the original data end up in the bootstrap, and the remaining 36.8% form the test set (since the probability of not being chosen is <span class="math inline">\((1 - 1/d)^d\)</span> , when d is large results to be ≈ e-1 = 0.368).</p>
            <p>Therefore the method has this name.</p>
            <p>We repeat the sampling procedure k times. The overall accuracy of the model is obtained as:</p>
            <figure>
            <img src="../media/image292.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <h3 data-number="5.8.4" id="estimating-confidence-intervals"><span class="header-section-number">5.8.4</span> Estimating Confidence Intervals</h3>
            <p>Suppose we have 2 classifiers, M1 and M2, which one is better than the other?</p>
            <p>We use the same dataset and 10-fold cross-validation and use the 10 accuracies to obtain err(M1) and err(M2). These mean error rates are just estimates of error on the true population of future data cases.</p>
            <p>What if the difference between the 2 error rates is just attributed to chance?</p>
            <p>We also must analyze how accuracies values are distributed.</p>
            <p>I could have that the distribution of accuracy are completely distinct.</p>
            <p>If we compare the average accuracy, we cannot conclude if one classifier is better than another.</p>
            <p>The average value is different, but distributions are almost overlapping.</p>
            <figure>
            <img src="../media/image293.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Considering the average values of accuracy is not sufficient.</p>
            <p>We must use a test of statistical significance that allow me to understand if the two distributions are statistically different, if it occurs, we can conclude that the distribution with the higher value of accuracy is better than the other.</p>
            <p>We must be sure that the distribution of accuracy is different.</p>
            <p>We use 10 because it is a good number to make a reliable statistic.</p>
            <p>We fix some confidence that we want to have in the statistical test and we apply it to verify if the hypothesis we are testing is true or not.</p>
            <p>Obtain confidence limits for our error estimates. For instance “One model is better than the other by a margin of error of 4%.”</p>
            <h4 class="unnumbered" data-number="" id="parametric-statistical-test-t-test">Parametric Statistical Test: t-test</h4>
            <p>Assumptions: the samples (for instance, classification accuracy) are normally distributed within each group (output of each classifier) and the variances of the two populations are not reliably different.</p>
            <p>If we have this assumptions we can apply a popular statistical test, the t-test (or Student’s t-test).</p>
            <p>We talk about distributions of values of accuracy (or f-measures values) and not of samples anymore when we talk about statistical test.We want to evaluate the <strong>null hypothesis</strong>. Null Hypothesis: the two distributions of accuracy for M1 and M2 are the same If we want to apply this test we need to be sure that the distribution is normal and the variances are not reliably different.</p>
            <p>If we can reject null hypothesis, then we conclude that the difference between M1 and M2 is statistically significant.</p>
            <p>We will choose the model with lowest error rate. If we used accuracy, the highest value of accuracy.</p>
            <p>Let’s see how apply the t-test to decide if distributions of accuracy/errors are different.</p>
            <ul>
            <li><p>We use the same test set used for M1 and M2: pairwise comparison.</p></li>
            <li><p>For the ith round of 10-fold cross-validation, we use the same cross partitioning is used to obtain err(M1)i and err(M2)i. We use the same training and test set at each iteration for both classifiers.</p></li>
            <li><p>We calculate the average over 10 rounds to get err(M1) and err(M2) and then, t-test computes t-statistic with k-1 degrees of freedom:</p></li>
            </ul>
            <figure>
            <img src="../media/image294.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If two test sets are available we can use non-paired t-test:</p>
            <figure>
            <img src="../media/image295.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>where k1 &amp; k2 are the number of cross-validation samples used for M1 &amp; M2, resp.</p>
            <p>We have to decide if M1 and M2 are significantly different.</p>
            <ul>
            <li><p>Compute t with the before formula and we select a significance level (typically sig = 5%)</p></li>
            <li><p>Consult table for t-distribution: Find t value corresponding to k-1 degrees of freedom (in our case, the degree of freedom is 9). Since we have 10 possible values and we know average values we have just 9 degree of freedom because if we fix 9 values the last value can be computed because we have the average. If we don’t have two we can’t.</p></li>
            <li><p>t-distribution is symmetric: typically upper % points of the distribution is shown → look up value for confidence limit z=sig/2 (here, 0.025), ust because we consider half of the t distribution.</p></li>
            <li><p>We interpret what we find in the table like follow.</p></li>
            <li><p>If t &gt; z or t &lt; -z, then t value lies in rejection region, this means that we can reject the null hypothesis that means error rates of M1 and M2 are same. We can conclude that there’s statistically significant difference between M1 and M2, and one classifier is better than another.</p></li>
            </ul>
            <p>Otherwise, we conclude that any difference is chance.</p>
            <p>In non-paired version, the number of degrees of freedom used is taken as the minimum number of degrees of the two models.</p>
            <p>In this example we have different rows depending on the degree of freedom. IN our case we have to focus on df = 9 and we look for the confidence value proposed in the first row and corresponding on the confidence value we have we search for the t-value, the threshold.</p>
            <p>If we compute t, and it is t &gt; threshold, we can reject the null hypothesis.</p>
            <figure>
            <img src="../media/image296.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>With the decrease of alpha, the value of t increases, but the decreasing of alpha corresponds to have the higher confidence in the rejection of the hypothesis.</p>
            <p>If we have a low value of alpha we have a high probability of differences for classifiers, I’m decreasing the confidence in rejecting the null hypothesis. If we want to be sure we have to choose a high value of alpha. alpha corresponds to the probability of the null hypothesis.</p>
            <p>If we search for a high value of confidence, the probability of the null hypothesis is very low.</p>
            <p>We compute t, the df = k-1, we arrive at the table and find the value of t to reject the null hypothesis corresponding to the df and the confidence value we want.</p>
            <p>To apply the t-test we have to be sure that the distribution is normal and there isn’t difference reliable of variances of the two populations.</p>
            <p>Typically, errors have a normal distribution.</p>
            <p>There are some tests to verify if the distribution is normal.</p>
            <h4 class="unnumbered" data-number="" id="non-parametric-statistical-tests-wilcoxon-signed-rank-sum-test">Non Parametric Statistical Tests: Wilcoxon Signed Rank Sum Test</h4>
            <p>We have another statistical approach without having these assumptions.</p>
            <p>The Wilcoxon signed rank sum test is an example of non-parametric (we don’t assume the distribution is normal) or distribution free test.</p>
            <p>The null hypothesis for this test is that the medians of two samples are equal (in other words, we test whether two populations have the same distribution with the same median).</p>
            <p>It is generally used:</p>
            <ul>
            <li><p>As a non-parametric alternative to the one-sample t test or paired t test.</p></li>
            <li><p>For ordered (ranked) categorical variables without a numerical scale.</p></li>
            </ul>
            <p>Assumptions we must do are (as in t-test, except for normal distribution):</p>
            <ol type="1">
            <li><p>The two samples are independent of one another</p></li>
            <li><p>The two populations have equal variance or spread</p></li>
            </ol>
            <p><strong>Wilcoxon test for Paired data</strong></p>
            <ol type="1">
            <li><p>Calculate each paired difference, di = xi - yi, where xi, yi are the pairs of observations. We compute the accuracy for each fold in the first model and we substract the accuracy achieved in the second model with the same test.</p></li>
            <li><p>We rank the difference di ignoring the signs (i.e. assign rank 1 to the smallest |di|, rank 2 to the next one, etc.</p></li>
            <li><p>We label each rank with its sign, according to the sign of di</p></li>
            <li><p>We calculate W+, the sum of ranks of the positive differences di and W- the sum of the ranks of the negative differences di. (As a check if they’re computed correctly, the total W+ + W- should be equal to n(n+1)/ 2, where n is the number of pairs of observations in the sample).</p></li>
            </ol>
            <p>Under the null hypothesis, we would expect the distribution of the differences to be approximately symmetric around zero and the distribution of positives and negatives to be distributed at random among the ranks. Under this assumption, it is possible to work out the exact probability of every possible outcome for W as follows:</p>
            <ol start="5" type="1">
            <li><p>Choose W=min(W+, W-)</p></li>
            <li><p>Use tables of critical values for the Wilcoxon signed rank sum test to find the probability of observing a value of W or more extreme. Most tables give both one-sided and two-sided p-values. If not, double the one-sided p-value to obtain the two-sided p-value.</p></li>
            </ol>
            <p>If we assume that the number of observations/pairs is such that n(n+1)/2 is large enough (&gt;20), a normal approximation can be used where the mean value of W and the standard deviation of W:</p>
            <figure>
            <img src="../media/image297.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Dealing with ties:</p>
            <p>Observations in the sample may be exactly equal to the median value M (i.e. 0 in the case of paired differences). Ignore such observations and adjust n accordingly. Two or more observations/differences may be equal. If so, average the ranks across the tied observations and reduce the variance by <img src="../media/image298.png" alt="alt" /> for each group of t tied ranks.</p>
            <p>Let’s see an example where we use a 12-fold cross-validation.</p>
            <figure>
            <img src="../media/image299.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>For the two classifiers we have these errors. We compute the differences that can be positive or negative. Then we rank differences without taking in consideration signs, we just store them:</p>
            <figure>
            <img src="../media/image300.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Calculating W+ and W- gives:</p>
            <p><span class="math inline">\(W^-=1+2+4=7\)</span>, summing ranks of negative differences <span class="math inline">\(W^+=3+5.5+5.5+7+8+9+10+11+12=71\)</span>, summing ranks of positive differences.</p>
            <p>Therefore:</p>
            <figure>
            <img src="../media/image301.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>It is greater than 20, so we can adopt the normal approximation and we must consider:</p>
            <figure>
            <img src="../media/image302.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Since we can use a normal approximation. We have one group of 2 tied ranks, so we must reduce the variance by (8-2)/48 = 0.125</p>
            <p>We can compute the z-score defined as:</p>
            <figure>
            <img src="../media/image303.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>A z-score is a measure of how many standard deviations below or above the population mean a raw score is.</p>
            <blockquote>
            <figure>
            <img src="../media/image304.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            </blockquote>
            <p>Looking up this score in the z-table*, we get an area of 0.9880, equal to a two-tailed p-value of 0.012. This is a tiny p-value, a strong indication that the medians are significantly different so we can reject the null hypothesis.</p>
            <p>The z-table here is to the right of the mean, so I had to double the actual result I found (.4940).</p>
            <p>We can also work in a different way, fixing apriori a value of p, compute z and see if we have a value of probability higher/lower of this way.</p>
            <p>We must refer to the table to decide if this value allow us to reject the null hypotesis with the confidence we fixed.</p>
            <p>In this way we can assume statistical independence and assume that one classifier is better than another.</p>
            <p>If we have more than two classifiers the solution to choose the best classifier we can compute them in pairs, but it would be better to use some specific statistical test for this aim.</p>
            <p>We must use same training and test sets for both classifiers in k-fold cross-validation and store values, then we apply one of the tests we analyzed.</p>
            <p>Only if we can reject the null hypothesis, we conclude that a classifier is better or not.</p>
            <p>Typically, a confidence of 0.05 is used, we have to take care of this value.</p>
            <h4 class="unnumbered" data-number="" id="statistical-significance">Statistical Significance</h4>
            <p>We applied the statistical tests to the accuracy (just one value).</p>
            <p>How can we compare two classifiers whether the dataset is imbalanced using statistical tests?</p>
            <p>In these cases, we cannot use accuracy but two metrics.</p>
            <p>You recall in this case we have the pairs (Recall-Precision) or (sensitivity-specificity).</p>
            <p>But the test we analyzed before are test used with accuracy.</p>
            <p>Possible solutions are:</p>
            <ul>
            <li><p>F-measure (combination between recall and precision)</p></li>
            <li><p>AUC - Area Under the ROC curve, we can use also this area.</p></li>
            </ul>
            <p>Using this area is another way when we work with imbalanced datasets.</p>
            <h3 data-number="5.8.5" id="roc-curve"><span class="header-section-number">5.8.5</span> ROC Curve</h3>
            <p>The <strong>ROC</strong> (Receiver Operating Characteristics) curves method is used for visual comparison of classification models.It is originated from signal detection theory.</p>
            <p>It shows the trade-off between the true positive rate and the false positive rate.</p>
            <p>Vertical axis represents the true positive rate.</p>
            <p>Horizontal axis rep. the false positive rate.</p>
            <p>The plot also shows a diagonal line.</p>
            <figure>
            <img src="../media/image305.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We can change parameters of a classifier to have different TPR and FPR:</p>
            <ul>
            <li><p>TPR= TP/P</p></li>
            <li><p>FPR= FP/N</p></li>
            </ul>
            <p>TPR and FPR are computed considering the classifier we have trained in the test set.</p>
            <p>The optimal classifier in the ROC curve should be located on the (0,1) point.</p>
            <p>A model with perfect accuracy will have an area of 1.0.</p>
            <p>Each classifier close to this point is a good classifier.</p>
            <p>Let’s assume that we can vary the parameters of classifiers and obtain the M2 curve and M1 curve.</p>
            <p>The area under the ROC curve (AUC) is a measure of the accuracy of the model.</p>
            <p>A larger area means the classifier is closer to the optimal classifier, the (0,1) point.</p>
            <figure>
            <img src="../media/image306.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If we have only one possible combination of parameters, we can connect the point (FPR,TPR) corresponding to the classifier to point (0,1) and to point (1,1) and compute the area under the curve.</p>
            <p>We pass from having two values for measuring the performance of the two classifiers to have one value, the area under the curve that takes in consideration TPR and FPR.</p>
            <p>Instead of f-measure we can use the area under the curve computed in this way.</p>
            <p>For each fold we can compute TPR, FPR, the area under the curve and we will have a distribution of 10 values and apply statistical tests.</p>
            <p>This is a way to use one value instead of a pair.</p>
            <p>The closer to the diagonal line (i.e., the closer the area is to 0.5), the less accurate is the model.</p>
            <p><img src="../media/image307.png" alt="alt" /><img src="../media/image308.png" alt="alt" /></p>
            <p>In this eample we use a classifier based on the probability.</p>
            <p>We classify an instance as a positive instance if the probability is higher than the threshold we have in the column probability, all others are negative.</p>
            <p>If we consider the distribution of the first instances, one is considered positive because probability is higher than 0.9 and all others as negative.</p>
            <p>Decreasing the probability threshold, we increase the probability to classify tuples as positive,</p>
            <p>Changing the parameters, we can have the curve we discussed, that is represented by the dashed line, obtained decreasing the probability threshold.</p>
            <p>Once did this, we can obtain the convex hull, which is the minimal convex set containing the points of the ROC curve. This hull allows us to make the following consideration.</p>
            <h4 class="unnumbered" data-number="" id="model-selection-cost-of-a-classifier">Model Selection: Cost of a Classifier</h4>
            <p>We can evaluate the cost of a classifier represented by the point (FPR, TPR).</p>
            <p>A positive misclassified for a negative typically has a higher cost tan a negative misclassified for a positive.</p>
            <p>Let’s assume that we can establish a cost for misclassification.</p>
            <p>We can express the cost of the classifier as following.</p>
            <figure>
            <img src="../media/image309.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>P(n) and P(p) are the a-priori probabilities of a negative example and a positive example.</p>
            <p>C(Y,n) (cost to misclassify a negative for a positive) and C(N,p) (cost to misclassify a positive for a negative) are the false positive cost and false negative cost.</p>
            <p>Once fixed the values of P(n), c(Y,n), P(p) and C(N,p), we can obtain a family of parallel lines (called iso-cost lines) with slope:</p>
            <figure>
            <img src="../media/image310.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We fixed the slope and obtained these lines that allow us to minimize the classification cost, which is the tangent point between the ROC curve and the family of parallel lines.</p>
            <p>Because this classifier is the one closer to the optimal, we have the lowest value of FPR and FNR.</p>
            <p>The points belonging to the same line have the same cost, and the cost decreases as we move to parallel lines closer to the point (0,1), i.e., more north-west.</p>
            <figure>
            <img src="../media/image311.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Once fixed the cost and the probability this allow us to find the best classifier for the specific application.</p>
            <p>We can change the parameters to obtain different TPR and FPR, in this way we can compute the slope for iso-cost lines, find the tangent and determine the optimal classifier for this specific application.</p>
            <p>We are choosing here:</p>
            <figure>
            <img src="../media/image312.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>because the cost of misclassification a positive for a negative is typically higher.</p>
            <h4 class="unnumbered" data-number="" id="comparing-different-classifiers-with-auc-case-imbalanced-datasets">Comparing Different Classifiers with AUC (case imbalanced datasets)</h4>
            <p>If we can draw the ROC curve, we can compute the convex hull and determine the best classifier.</p>
            <p>The tangent point is in the convex hull; therefore we compute the convex hull.</p>
            <p>The ROC curve is the method we use when we deal with imbalanced datasets.</p>
            <p>When we have the cost, we can choose if we prefer FP or FN.</p>
            <p>If we are able to determine the cost, we can find the correct balance between FP and FN.</p>
            <h2 data-number="5.9" id="techniques-to-improve-classification-accuracy-ensemble-methods"><span class="header-section-number">5.9</span> Techniques to Improve Classification Accuracy: Ensemble Methods</h2>
            <p>One idea is to use a combination of models to increase accuracy.</p>
            <p>We combine a series of k learned models <span class="math inline">\(M_1, M_2, ..., M_k\)</span> with the aim of creating an improved model <span class="math inline">\(M^*\)</span>.</p>
            <figure>
            <img src="../media/image313.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We aggregate their results to improve accuracy.</p>
            <p>Popular ensemble methods are:</p>
            <ul>
            <li><p><strong>Bagging</strong>: averaging the prediction over a collection of classifiers</p></li>
            <li><p><strong>Boosting</strong>: we give a weighted vote with a collection of classifiers</p></li>
            <li><p><strong>Ensemble</strong>: combining a set of heterogeneous classifiers</p></li>
            </ul>
            <h3 data-number="5.9.1" id="bagging-bootstrap-aggregation"><span class="header-section-number">5.9.1</span> Bagging (Bootstrap Aggregation)</h3>
            <p>Analogy: Diagnosis are based on multiple doctors’ majority vote</p>
            <p>Training:</p>
            <ul>
            <li><p>Given a set <span class="math inline">\(D\)</span> of <span class="math inline">\(d\)</span> tuples, at each iteration <span class="math inline">\(i\)</span>, a training set <span class="math inline">\(D_i\)</span> of <span class="math inline">\(d\)</span> tuples is sampled with replacement from <span class="math inline">\(D\)</span> (i.e., bootstrap).</p></li>
            <li><p>A classifier model <span class="math inline">\(M_i\)</span> is learned for each training set <span class="math inline">\(D_i\)</span></p></li>
            </ul>
            <p>We generated a different dataset with <span class="math inline">\(d\)</span> tuples for each model we generated.</p>
            <p>Classification: classify an unknown sample <span class="math inline">\(X\)</span></p>
            <ul>
            <li><p>Each classifier <span class="math inline">\(M_i\)</span> returns its class prediction</p></li>
            <li><p>The bagged classifier <span class="math inline">\(M^*\)</span> counts the votes and assigns the class with the most votes to <span class="math inline">\(X\)</span></p></li>
            </ul>
            <p>In this way we generate different models, and we change the training set, this is why we expect to improve accuracy.</p>
            <figure>
            <img src="../media/image314.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>It’s possible to use it for the prediction of continuous values by taking the average value of each prediction for a given test tuple.</p>
            <p>Accuracy of bagging is often significantly better than a single classifier derived from <span class="math inline">\(D\)</span>. For noise data: not considerably worse, and typically more robust.</p>
            <p>We typically have a proved improved accuracy in prediction.</p>
            <p>To help illustrate the power of an ensemble, consider a simple two-class problem described by two attributes, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. The problem has a linear decision boundary.</p>
            <figure>
            <img src="../media/image315.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Figure (a) shows the decision boundary of a decision tree classifier on the problem.</p>
            <p>Figure (b) shows the decision boundary of an ensemble of decision tree classifiers on the same problem. We combine different boundaries and we obtain a different form, more adapted.</p>
            <h3 data-number="5.9.2" id="boosting"><span class="header-section-number">5.9.2</span> Boosting</h3>
            <p>Analogy: Consult several doctors, based on a combination of weighted diagnoses; the weight is assigned based on the previous diagnosis accuracy.</p>
            <p>How boosting works?</p>
            <ul>
            <li><p>Weights are assigned to each training tuple</p></li>
            <li><p>A series of <span class="math inline">\(k\)</span> classifiers is iteratively learned</p></li>
            <li><p>After a classifier <span class="math inline">\(M_i\)</span> is learned, the weights associated with each tuple are updated to allow the subsequent classifier, <span class="math inline">\(M_{i+1}\)</span>, to pay more attention to the training tuples that were misclassified by <span class="math inline">\(M_i\)</span>.</p></li>
            <li><p>The final <span class="math inline">\(M^*\)</span> combines the votes of each individual classifier, where the weight of each classifier's vote is a function of its accuracy.</p></li>
            </ul>
            <p>A classifier with a higher accuracy is considered more reliable.</p>
            <p>Each classifier has its own weight too.</p>
            <p>Boosting algorithm can be extended for numeric prediction. Comparing with bagging, Boosting tends to have greater accuracy, but it also risks overfitting the model to misclassified data.</p>
            <p>We start to assign the same weights to all instances.</p>
            <figure>
            <img src="../media/image316.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>The approach just generates <span class="math inline">\(k\)</span> classifiers but at each round tuples from <span class="math inline">\(D\)</span> are sampled with replacements to form a training set, but the chance to select a tuple is based on its weight.</p>
            <p>Higher the weight and higher is the probability of selection.</p>
            <p>At the beginning all instances have the same probability, we generate the training set for the first classifier and let’s assume it classifies correctly the instances not circled. We increase the weight for he circled instances because the next classifier must focus on them.</p>
            <p>For the subsequent classifier we will have other instances misclassified and so on.</p>
            <figure>
            <img src="../media/image317.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>At the end we have a set of classifiers, each able to focus on specific instances and specific zones of the space.</p>
            <figure>
            <img src="../media/image318.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>When we put them together we expect higher performances.</p>
            <h4 class="unnumbered" data-number="" id="adaboost-algorithm">Adaboost Algorithm</h4>
            <p>Given a set of <span class="math inline">\(d\)</span> class-labeled tuples, <span class="math inline">\((X_1, y_1), ..., (X_d, y_d)\)</span>.</p>
            <p>Initially, all the weights of tuples are set to the same <span class="math inline">\(1/d\)</span>.</p>
            <p>Generate <span class="math inline">\(k\)</span> classifiers in <span class="math inline">\(k\)</span> rounds.</p>
            <p>At round <span class="math inline">\(i\)</span>,</p>
            <ul>
            <li><p>Tuples from <span class="math inline">\(D\)</span> are sampled (with replacement) to form a training set <span class="math inline">\(D_i\)</span> of the same size</p></li>
            <li><p>Each tuple’s chance of being selected is based on its weight</p></li>
            <li><p>A classification model <span class="math inline">\(M_i\)</span> is derived from <span class="math inline">\(D_i\)</span></p></li>
            <li><p>Its error rate is calculated using <span class="math inline">\(D_i\)</span> as a test set</p></li>
            <li><p>Classifier <span class="math inline">\(M_i\)</span> error rate is the sum of the weights of the misclassified tuples: <img src="../media/image319.png" alt="alt" /></p></li>
            </ul>
            <p>where <span class="math inline">\(err(X_j)\)</span> is the misclassification error of tuple <span class="math inline">\(X_j\)</span>.</p>
            <ul>
            <li>If a tuple <span class="math inline">\((X_j, y_j)\)</span>, is correctly classified, then the weight is updated as follows:</li>
            </ul>
            <figure>
            <img src="../media/image320.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>We decrease the weight of correct classifier’s instances.</p>
            <ul>
            <li><p>Once the weights are updated, the weights for all the tuples (both correctly and incorrectly classified) are normalised so that their sum remains the same it was before. If we normalize and decrease the weight for correctly classified instances it’s like we increase it for misclassified instances.</p></li>
            <li><p>‎The normalization is performed by multiplying the weight by the sum of the old weights and dividing it by the sum of the new weights (the weights of misclassified tuples are increased!)</p></li>
            </ul>
            <p>Once the boosting is complete, how is the ensemble of classifiers used to predict the class label of a tuple?</p>
            <p>Boosting assigns a weight to each classifier’s vote, based on how well the classifier performed.</p>
            <p>The lower a classifier’s error rate, the more accurate it is and therefore the higher its weight for voting should be</p>
            <p>The weight of classifier <span class="math inline">\(M_i\)</span>’s vote is:</p>
            <figure>
            <img src="../media/image321.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>For each class <span class="math inline">\(c\)</span> we sum the weights of each classifier that assigned class <span class="math inline">\(c\)</span> to <span class="math inline">\(X\)</span>. The class with the highest sum is the “winner” and is returned as the class prediction for tuple <span class="math inline">\(X\)</span>.</p>
            <p>This is the Adaboost algorithm:</p>
            <figure>
            <img src="../media/image322.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <figure>
            <img src="../media/image323.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <figure>
            <img src="../media/image324.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>Boosting vs Bagging:</p>
            <figure>
            <img src="../media/image325.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <h3 data-number="5.9.3" id="random-forest"><span class="header-section-number">5.9.3</span> Random Forest</h3>
            <p>The idea is to have a large ensemble of decision trees. Each classifier is a decision tree classifier and is generated using a random selection of attributes at each node to determine the split.</p>
            <p>Each tree votes and the most popular class is returned.</p>
            <figure>
            <img src="../media/image326.jpeg" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>If I have a training set and I learn a lot of trees we obtain always the same decision tree. We have to consider another approach to learn decision trees.</p>
            <p><strong>Classical Algorithm:</strong></p>
            <p>Let <span class="math inline">\(N\)</span> and <span class="math inline">\(M\)</span> be the number of training instances and attributes, respectively. Let <span class="math inline">\(m\)</span> be the number of attributes to be used for choosing the decision attribute at any node.</p>
            <ol type="1">
            <li><p>Choose a training set by randomly extracting <span class="math inline">\(N\)</span> samples with replacement from all available training instances (i.e. take a bootstrap sample), for each decision tree (the number of samples will be still the same). Use the rest of the instances to estimate the error of the tree.</p></li>
            <li><p>For each node of the tree, randomly choose <span class="math inline">\(m\)</span> attributes on which to base the decision at that node, then we determine the optimal attribute between the set of selected attributes. Calculate the best split based on these <span class="math inline">\(m\)</span> variables in the training set.</p></li>
            <li><p>Each tree is fully grown and not pruned (as may be done in constructing a normal tree classifier). We don’t need a pruning phase because with the second step when we use randomly some attributes, we just create a variation to the classical approach and when we combine different trees we take the peculiarity of each tree and this avoid overtraining.</p></li>
            </ol>
            <p>Classification: Each tree assigns a class to the unlabeled instance. The most popular class is returned.</p>
            <p><strong>Advantages:</strong></p>
            <ul>
            <li><p>RF is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier.</p></li>
            <li><p>RF runs efficiently on large databases.</p></li>
            <li><p>RF can handle thousands of input variables without variable deletion.</p></li>
            <li><p>RF gives estimates of what variables are important in the classification.</p></li>
            <li><p>RF generates an internal unbiased estimate of the generalization error as the forest building progresses.</p></li>
            <li><p>RF has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing.</p></li>
            </ul>
            <p><strong>Disadvantages:</strong></p>
            <ul>
            <li><p>RFs have been observed to overfit for some datasets with noisy classification/regression tasks. We still have problems of overtraining.</p></li>
            <li><p>For data including categorical variables with different number of levels, RFs are biased in favor of those attributes with more levels. Therefore, the variable importance scores from random forest are not reliable for this type of data.</p></li>
            <li><p>Another disadvantage is that the interpretability of random forest is lower than decision trees. Each decision tree has a rule activated and at the end we just exploit the majority, combining information from different trees.</p></li>
            </ul>
            <h2 data-number="5.10" id="classification-of-class-imbalanced-data-sets"><span class="header-section-number">5.10</span> Classification of Class-Imbalanced Data Sets</h2>
            <p>When we work with imbalanced datasets we can’t work with accuracy.</p>
            <p>When we work with imbalanced datasets, we have problems in terms of learning. In classifiers where we reduce the error between the prediction and the ground-truth, misclassification of majority classes weight more and that’s why they tend to learn more about majority classes, they tend to reduce the misclassifications of those classes.</p>
            <p>In decision tree learning we don’t minimize the error but increasing the info_gain, that’s why it works quite good with imbalanced datasets.</p>
            <p>But in learning classifiers which minimize misclassification errors, we have to try to rebalance the training set.</p>
            <p>Class-imbalance problem: Rare positive example but numerous negative ones, e.g., medical diagnosis, fraud, oil-spill, fault, etc.</p>
            <p>Traditional methods assume a balanced distribution of classes and equal error costs: not suitable for class-imbalanced data</p>
            <p>Typical methods for imbalance data in 2-class classification:</p>
            <ul>
            <li><p>Oversampling: re-sampling of data from positive class, we assume the positive class is the minority class</p></li>
            <li><p>Under-sampling: randomly eliminate tuples from negative class (majority class)</p></li>
            <li><p>Threshold-moving: moves the decision threshold, t, so that the rare class tuples are easier to classify, and hence, less chance of costly false negative errors</p></li>
            <li><p>Ensemble techniques: We use ensemble multiple classifiers introduced above and in this way we reduce this problem.</p></li>
            </ul>
            <p>Still difficult for class imbalance problem on multiclass tasks, in that case this is more complex.</p>
            <p>When we want to rebalance a dataset, we apply it only for the training set. In reality, in fact, we don’t have the test set is available.</p>
            <p><strong>SMOTE: Synthetic Minority Over-sampling Technique</strong></p>
            <p>It is an example of oversampling in which the minority class is over-sampled by creating “synthetic” examples rather than by over-sampling with replacement.</p>
            <figure>
            <img src="../media/image327.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>In the ROC curve we want to have TPR close to 1 and FRP close to 0, with oversampling or under sampling our aim is learn a classifier quite close to the ideal point.</p>
            <p>The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors.</p>
            <p>We randomly exploit a point along the line.</p>
            <p>Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen.</p>
            <p>For instance, if the amount of over-sampling needed is 200%, only two neighbors from the five nearest neighbors are chosen and one sample is generated in the direction of each.</p>
            <p>Synthetic samples are generated in the following way:</p>
            <ul>
            <li><p>Take the difference between the feature vector (sample) under consideration and its nearest neighbor.</p></li>
            <li><p>Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration.</p></li>
            <li><p>This causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the minority class to become more general.</p></li>
            </ul>
            <figure>
            <img src="../media/image328.png" alt="" /><figcaption>alt</figcaption>
            </figure>
            <p>In case of imbalanced dataset, we must rebalance the training set.</p>
            <p>When we apply 10-cross validation, first we select 9 folds and then rebalance this training set, learn the classifier and measure the performance on the remaining.</p>
            <h1 data-number="6" id="cluster-analysis"><span class="header-section-number">6</span> Cluster Analysis</h1>
            <p>A cluster is a collection of data objects characterized by similarity.</p>
            <p>We have to achieve similar (or related) objects belonging to the same group and dissimilar (or unrelated) to the objects in other groups.</p>
            <p>Cluster analysis consists on finding similarities between data according to the characteristics found in the data and grouping similar data objects into clusters.</p>
            <p>This is an <strong>unsupervised learning approach</strong>: no predefined classes</p>
            <h2 data-number="6.1" id="typical-applications"><span class="header-section-number">6.1</span> Typical applications:</h2>
            <ul>
            <li><p>As a stand-alone tool to get insight into data distribution</p></li>
            <li><p>As a preprocessing step for other algorithms, like k-nearest neighbors</p></li>
            </ul>
            <h3 data-number="6.1.1" id="possible-applications-are"><span class="header-section-number">6.1.1</span> Possible applications are:</h3>
            <ul>
            <li><p>Biology: taxonomy of living things: kingdom, phylum, class, order, family, genus and species</p></li>
            <li><p>Information retrieval: document clustering</p></li>
            <li><p>Land use: Identification of areas of similar land use in an earth observation database</p></li>
            <li><p>Marketing: Help marketers discover distinct groups in their customer bases, and then use this knowledge to develop targeted marketing programs</p></li>
            <li><p>City-planning: Identifying groups of houses according to their house type, value, and geographical location  </p></li>
            <li><p>Earth-quake studies: Observed earth quake epicenters should be clustered along continent faults</p></li>
            <li><p>Climate: understanding earth climate, find patterns of atmospheric and ocean</p></li>
            <li><p>Economic Science: market research</p></li>
            </ul>
            <h3 data-number="6.1.2" id="we-can-use-clustering-as-a-preprocessing-tool"><span class="header-section-number">6.1.2</span> We can use clustering as a preprocessing tool:</h3>
            <ul>
            <li><p>Summarization: Preprocessing for regression, PCA, classification, and association analysis</p></li>
            <li><p>Compression: Image processing: vector quantization</p></li>
            <li><p>Finding K-nearest Neighbor: Localizing search to one or a small number of clusters</p></li>
            <li><p>Outlier detection: Outliers are often viewed as those “far away” from any cluster</p></li>
            </ul>
            <h3 data-number="6.1.3" id="clustering-evalutation"><span class="header-section-number">6.1.3</span> Clustering evalutation</h3>
            <p>A good clustering method will produce high quality clusters, which means:</p>
            <ul>
            <li><p>high intra-class similarity: cohesive within clusters</p></li>
            <li><p>low inter-class similarity: distinctive between clusters</p></li>
            </ul>
            <p>The quality of a clustering method depends on</p>
            <ul>
            <li><p>the similarity measure used by the method</p></li>
            <li><p>its implementation, and</p></li>
            <li><p>its ability to discover some or all of the hidden patterns</p></li>
            </ul>
            <p><strong>Similarity</strong> is expressed in terms of a distance function, typically metric: d(i, j).</p>
            <p>The definitions of <strong>distance</strong> functions are usually rather different for interval-scaled, boolean, categorical, ordinal ratio, and vector variables.</p>
            <h3 data-number="6.1.4" id="considerations-for-cluster-analysis"><span class="header-section-number">6.1.4</span> Considerations for Cluster Analysis</h3>
            <ul>
            <li><p>Partitioning criteria</p>
            <ul>
            <li><p>It can be <strong>single level</strong>, we just group objects in cluster</p></li>
            <li><p><strong>hierarchical partitioning</strong>, where we create an hierarchy of clusters</p></li>
            </ul></li>
            <li><p>Separation of clusters</p>
            <ul>
            <li>It can be <strong>exclusive</strong> (e.g. one customer belongs to only one</li>
            <li><strong>non-exclusive</strong> (e.g., one document may belong to more than one class with different membership degree)</li>
            </ul></li>
            <li><p>Similarity measure.</p>
            <p>Most of them are distance-based (e.g., Euclidian, road network, vector), while others are connectivity-based (e.g., density or contiguity).</p></li>
            <li><p>Clustering space.</p>
            <p>We can cluster considering: Full space (often when low dimensional) vs. subspaces (often in high-dimensional clustering)</p></li>
            <li><p>Scalability</p>
            <p>Clustering all the data instead of only on samples. When we have a high number of samples we can reduce the number of data in which we apply clustering.</p></li>
            <li><p>Ability to deal with different types of attributes, which can be numerical, binary, categorical, ordinal, linked, and mixture of these.</p></li>
            <li><p>Constraint-based clusteringUser may give inputs on constraintsUse domain knowledge to determine input parameters</p></li>
            <li><p>Interpretability and usability</p></li>
            <li><p>Others</p>
            <ul>
            <li><p>Discovery of clusters with arbitrary shape</p></li>
            <li><p>Ability to deal with noisy data</p></li>
            <li><p>Incremental clustering and insensitivity to input order. Most of algorithms are not updatable if we receive other instances, while in incremental clustering approaches it is possible.</p></li>
            <li><p>When we have high dimensionality we have the curse of dimensionality, instances appear far from each other. We have to use suitable distances, for example the cosine similarity.</p></li>
            </ul></li>
            </ul>
            <h2 data-number="6.2" id="major-clustering-approaches-are"><span class="header-section-number">6.2</span> Major clustering approaches are:</h2>
            <ul>
            <li><p><strong>Partitioning approach</strong></p>
            <p>Construct various partitions and then evaluate them by some criterion, ex:</p>
            <ul>
            <li>minimizing the sum of square errors</li>
            <li>optimizing a cost function.</li>
            </ul>
            <p><br></p>
            <p>A typical approach given the number <span class="math inline">\(k\)</span> of partitions to construct, the partitioning method uses an iterative relocation technique that attempts to improve the partitioning by moving objects from one group to another (<code>k-means</code>, <code>k-medoids</code>, <code>CLARANS</code>)</p></li>
            <li><p><strong>Hierarchical approach</strong></p>
            <p>Create a hierarchical decomposition of the set of data (or objects) using some criterion. We have two approaches: Agglomerative and Divisive approaches.</p>
            <p>They suffer from the fact that once a step (merge or split) is done, it can never be undone.</p>
            <p>Typical methods: <code>Diana</code>, <code>Agnes</code>, <code>BIRCH</code>, <code>CHAMELEON</code></p></li>
            <li><p><strong>Density-based approach</strong></p>
            <p>Based on connectivity and density functions. The idea is to create clusters following dense regions. Typical methods: DBSCAN, OPTICS, DenClue</p></li>
            <li><p><strong>Grid-based approach</strong></p>
            <p>based on a multiple-level granularity structure.</p>
            <p>All the clustering operations are performed on the grid structure. It has fast processing time.</p>
            <p>Typical methods: <code>STING</code>, <code>WaveCluster</code>, <code>CLIQUE</code></p></li>
            </ul>
            <h3 data-number="6.2.1" id="recap"><span class="header-section-number">6.2.1</span> Recap</h3>
            <p><img src="../media/image329.png" /></p>
            <ul>
            <li><p>Partitioning methods have prefix shape of clusters beacuse they have to fix a distance to use. In k-means it is spherical because we use euclidian distance.</p></li>
            <li><p>Hierarchical methods are a sort of pre-clustering.</p></li>
            <li><p>Density methods follow dense regions, that gives them the ability to use an arbitrary shape. The problem is to define a concept of dense, that consists on a number of points for considering it dense.</p>
            <p>Outliers depend on how we define dense.</p></li>
            </ul>
            <h3 data-number="6.2.2" id="we-have-other-approaches"><span class="header-section-number">6.2.2</span> We have other approaches:</h3>
            <ul>
            <li><p><strong>Model-based</strong>: A model is hypothesized for each of the clusters and tries to find the best fit of that model to each other.</p></li>
            <li><p><strong>Frequent pattern-based</strong>: Based on the analysis of frequent patterns.</p></li>
            <li><p><strong>User-guided or constraint-based</strong>: Clustering by considering user-specified or application-specific constraints.</p></li>
            <li><p><strong>Link-based clustering</strong>: Objects are often linked together in various ways. Massive links can be used to cluster objects: SimRank, LinkClus</p></li>
            </ul>
            <h2 data-number="6.3" id="assessing-clustering-tendency"><span class="header-section-number">6.3</span> Assessing Clustering Tendency</h2>
            <p>Given a dataset, we must consider if it has some clustering tendency.</p>
            <p>Applying a clustering algorithm, we obtain cluster, but we can’t be sure if they’re natural clusters.</p>
            <p>Before applying any clustering method on your data, it’s important to evaluate whether the data sets contain meaningful clusters or not.</p>
            <p>If we apply k-means to the left example, we produce two clusters also if natural clusters are not present.</p>
            <p><img src="../media/image330.png" /></p>
            <p>Clustering tendency assessment determines whether a given data set has a non-random structure, which may lead to meaningful clusters.</p>
            <h2 data-number="6.4" id="hopkins-statistic"><span class="header-section-number">6.4</span> Hopkins statistic</h2>
            <p>The intuition is to understand if our dataset is uniformly distributed in a data space. In this case clusters we discover are clusters without any meaning.</p>
            <p><strong>Hopkins statistic</strong> tests the spatial randomness of a variable as distributed in a space. It is used to assess the clustering tendency of a data set by measuring the probability that a given data set is generated by a uniform data distribution. In the example seen before we don’t have tendency in clusters.</p>
            <blockquote>
            <p>In other words, it tests the spatial randomness of the data.</p>
            </blockquote>
            <h3 data-number="6.4.1" id="algorithm-2"><span class="header-section-number">6.4.1</span> Algorithm</h3>
            <p>Given a dataset <span class="math inline">\(D\)</span> which is regarded as a sample of a random variable <span class="math inline">\(X\)</span>, we want to determine how far away <span class="math inline">\(X\)</span> is from being uniformly distributed in the data space.</p>
            <p>It works in this way:</p>
            <ol type="1">
            <li><p>Sample <span class="math inline">\(n&lt;&lt;N\)</span> points <span class="math inline">\(&lt;p_1, \dots, p_n&gt;\)</span>, uniformly from <span class="math inline">\(D\)</span>.</p>
            <p>For each point <span class="math inline">\(p_i\)</span>, we find its nearest neighbor in <span class="math inline">\(D\)</span>.</p>
            <p>Let <span class="math inline">\(x_i\)</span> the distance between <span class="math inline">\(p_i\)</span> and its nearest neighbor in <span class="math inline">\(D\)</span>.</p>
            <p>We find the minimum of the distance of <span class="math inline">\(p_i\)</span> and the nearest neighbor:</p>
            <p><span class="math display">\[
                 x_i = \min_{V\in D}{\{dist(p_i, V)\}}
             \]</span></p>
            <blockquote>
            <p>the minimum distance, between <span class="math inline">\(n\)</span> random points and their nearest neighbor</p>
            </blockquote></li>
            </ol>
            <p><br></p>
            <ol start="2" type="1">
            <li><p>Generate a simulated data set (<span class="math inline">\(random_D\)</span>) drawn from a random uniform distribution with <span class="math inline">\(n\)</span> points <span class="math inline">\(&lt;q1, \dots, qn&gt;\)</span> and the same variation as the original dataset <span class="math inline">\(D\)</span>.</p>
            <p>For each point <span class="math inline">\(q_i\)</span>, find the nearest neighbor of <span class="math inline">\(q_i\)</span> in <span class="math inline">\(D\)</span>.</p>
            <p>Let <span class="math inline">\(y_i\)</span> the distance between <span class="math inline">\(q_i\)</span> and its nearest neighbor in <span class="math inline">\(D\)</span></p>
            <p><span class="math display">\[
                 y_i = \min_{V\in D, V \neq Q}{\{dist(q_i, V)\}}
             \]</span></p>
            <p>ex:</p>
            <p><img src="../media/image333.png" /></p>
            <p>In the first case we compute the distance between the points we selected and the points in <span class="math inline">\(D\)</span>, in the second case the fictitious point and the closest point in <span class="math inline">\(D\)</span>.</p></li>
            <li><p>Calculate the Hopkins Statistic</p>
            <p><span class="math display">\[
                 H = \frac{\sum_{i=1}^{n}{y_i}}{\sum_{i=1}^{n}{x_i} + \sum_{i=1}^{n}{y_i}}
             \]</span></p>
            <p>If <span class="math inline">\(D\)</span> were uniformly distributed, then the two terms at the denominator would be close to each other and therefore H would be about <span class="math inline">\(0.5\)</span>.</p>
            <p>Picking up samples from the distribution of data and the space approximately gives similar points.</p></li>
            </ol>
            <h3 data-number="6.4.2" id="considerations-3"><span class="header-section-number">6.4.2</span> Considerations</h3>
            <p>In presence of clustering tendency, then the first term at the denominator would be smaller than the second and then <span class="math inline">\(H\)</span> will increase.</p>
            <p>The sum of <span class="math inline">\(x_i\)</span> is lower than the sum of <span class="math inline">\(y_i\)</span>.</p>
            <p>The minimum value of <span class="math inline">\(0.5\)</span> in case of uniform distributed point, while <span class="math inline">\(H\)</span> tends to be equal to <span class="math inline">\(1\)</span> when we have a tendency to have a cluster.</p>
            <p>The null and alternative hypotheses are defined as follows:</p>
            <ul>
            <li><p><strong>Null hypothesis</strong>: the data set D is uniformly distributed (i.e., no meaningful clusters)</p></li>
            <li><p><strong>Alternative hypothesis</strong>: the data set D is not uniformly distributed (i.e., contains meaningful clusters)</p></li>
            </ul>
            <p>The Hopkins statistic is computed for several random selection of points and the average of all results for <span class="math inline">\(H\)</span> is used for a decision:</p>
            <ul>
            <li>we can heuristically conclude that if <span class="math inline">\(H\)</span> is larger than <span class="math inline">\(0.75\)</span>, it indicates a clustering tendency at the <span class="math inline">\(90%\)</span> confidence level.</li>
            </ul>
            <blockquote>
            <p>Before applying clustering algorithms, we should understand if exists clustering tendency or we could find non-natural clusters.</p>
            </blockquote>
            <h2 data-number="6.5" id="partitioning-methods"><span class="header-section-number">6.5</span> Partitioning Methods</h2>
            <p>First group of clustering algorithms are partitioning algorithms.</p>
            <p>A partitioning method consists in partitioning a database <span class="math inline">\(D\)</span> of <span class="math inline">\(n\)</span> objects into a set of <span class="math inline">\(k\)</span> clusters, such that the sum of squared distances is minimized (where <span class="math inline">\(c_i\)</span> is the centroid or medoid of cluster <span class="math inline">\(C_i\)</span>).</p>
            <p>The cost function we minimize if the following:</p>
            <p><span class="math display">\[
                E = \sum_{i=1}^{k}{\sum_{p\in C_i}{(p-C_i)^2}}
            \]</span></p>
            <p>We have the difference between each point belonging to a cluster and the representative of the cluster.</p>
            <p>We tend to minimize this difference for all point belonging to the different clusters.</p>
            <h3 data-number="6.5.1" id="fixed-k"><span class="header-section-number">6.5.1</span> Fixed <span class="math inline">\(k\)</span></h3>
            <p>Given <span class="math inline">\(k\)</span> find a partition of <span class="math inline">\(k\)</span> clusters that optimizes the chosen partitioning criterion.</p>
            <p>This method forces us to fix the number of clusters we want to obtain before the execution of the algorithm. This constraint is strong because we don’t know very much of our dataset and how many clusters we can find there. We have to fix <span class="math inline">\(k\)</span> arbitrarly.</p>
            <p>This approach try to optimize clusters minimizing the cost function.</p>
            <p>One solution to do that is to try all possible partitions but this solution is not possible, with an high number of objects we have an high number of combinations.</p>
            <p>Global optimal: exhaustively enumerate all partitions!</p>
            <p>We have to find heuristic methods to find an optimum of this cost function.</p>
            <p>Heuristic methods are <code>k-means</code> and <code>k-medoids</code> algorithms.</p>
            <ul>
            <li><p><strong>k-means</strong>: Each cluster is represented by the center of the cluster, <span class="math inline">\(c_i\)</span> is the center of the cluster (we average feature be feature all points belonging to the cluster). This means that k-means can be applied if we have all objects described by numerical features.</p></li>
            <li><p><strong>k-medoids or PAM</strong> (Partition around medoids) : Each cluster is represented by one of the objects in the cluster. In this approach we can have different types of features.</p></li>
            </ul>
            <h3 data-number="6.5.2" id="k-means-clustering-method"><span class="header-section-number">6.5.2</span> K-means clustering method</h3>
            <p>The centroid is defined as the mean value of the points within the cluster.</p>
            <h4 class="unnumbered" data-number="" id="pseudocode">pseudocode</h4>
            <div class="sourceCode" id="cb14"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a><span class="kw">def</span> kmeans(data, k):</span>
            <span id="cb14-2"><a href="#cb14-2" aria-hidden="true"></a>    </span>
            <span id="cb14-3"><a href="#cb14-3" aria-hidden="true"></a>    <span class="co"># initialize k centroids randomly from data points</span></span>
            <span id="cb14-4"><a href="#cb14-4" aria-hidden="true"></a>    centroids <span class="op">=</span> randomly_initialize_centroids(data, k)</span>
            <span id="cb14-5"><a href="#cb14-5" aria-hidden="true"></a></span>
            <span id="cb14-6"><a href="#cb14-6" aria-hidden="true"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
            <span id="cb14-7"><a href="#cb14-7" aria-hidden="true"></a>        </span>
            <span id="cb14-8"><a href="#cb14-8" aria-hidden="true"></a>        <span class="co"># assign each data point to the most similar to the centroid</span></span>
            <span id="cb14-9"><a href="#cb14-9" aria-hidden="true"></a>        clusters <span class="op">=</span> assign_data_to_clusters(data, centroids)</span>
            <span id="cb14-10"><a href="#cb14-10" aria-hidden="true"></a></span>
            <span id="cb14-11"><a href="#cb14-11" aria-hidden="true"></a>        <span class="co"># calculate new centroids</span></span>
            <span id="cb14-12"><a href="#cb14-12" aria-hidden="true"></a>        new_centroids <span class="op">=</span> mean(clusters)</span>
            <span id="cb14-13"><a href="#cb14-13" aria-hidden="true"></a></span>
            <span id="cb14-14"><a href="#cb14-14" aria-hidden="true"></a>        <span class="co"># check for convergence</span></span>
            <span id="cb14-15"><a href="#cb14-15" aria-hidden="true"></a>        <span class="cf">if</span> centroids_have_converged(new_centroids, centroids):</span>
            <span id="cb14-16"><a href="#cb14-16" aria-hidden="true"></a>            <span class="cf">return</span> clusters</span>
            <span id="cb14-17"><a href="#cb14-17" aria-hidden="true"></a>        </span>
            <span id="cb14-18"><a href="#cb14-18" aria-hidden="true"></a>        centroids <span class="op">=</span> new_centroids</span></code></pre></div>
            <blockquote>
            <p>We will converge to have optimal clusters because we move objects in clusters and recompute means.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="example-4">Example</h4>
            <p><img src="../media/image337.png" /></p>
            <ul>
            <li><p>Let’s suppose K = 2, we re-assing each object to clusters by computing distance between each object and the center of the cluster and re-assing each object and then update cluster centroids.</p></li>
            <li><p>We can also have other termination conditions, for example if the center moves from one iteration to another and so on.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="consideations">Consideations</h4>
            <ul>
            <li><p>This approach, as we can see, is really sensitive to the initialization phase. We have the risk to fall in a local minimum instead of a maximum.</p></li>
            <li><p>These clusters are clusters with spherical shape. The reason is that in k-means we assess the similarity between each point and the center of the cluster using the euclidian distance.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="strengths">Strengths</h4>
            <ul>
            <li><p><strong>Efficient</strong>: <span class="math inline">\(O(tkn)\)</span> where</p>
            <ul>
            <li><span class="math inline">\(n\)</span> is the number of objects</li>
            <li><span class="math inline">\(k\)</span> is the number of clusters</li>
            <li><span class="math inline">\(t\)</span> is the number of iterations.</li>
            </ul>
            <p>Normally, <span class="math inline">\(k, t &lt;&lt; n\)</span>. We don’t need a lot of iterations typically and this is why we could fall into a local minimum.</p>
            <p>Comparing: - PAM: <span class="math inline">\(O(k(n-k)2)\)</span> - CLARA: <span class="math inline">\(O(ks2 + k(n-k))\)</span> (<span class="math inline">\(s\)</span> is the size of the data sample)</p></li>
            </ul>
            <p>Comment: Often terminates at a local optimal.</p>
            <p>In fact, we can use k-means several times using different initializations and use the result of the execution that achieve the minimum value of the cost function.</p>
            <h4 class="unnumbered" data-number="" id="weakness">Weakness</h4>
            <ul>
            <li><p>Applicable only to objects in a <strong>continuous n-dimensional space</strong>.</p>
            <p>This is not because we determine the distance, but because we have to <strong>compute the center of the clusters</strong>, obtained averaging features’ values.</p>
            <p>We can also use the k-modes method for categorical data.</p>
            <p>The modes are the most probable values, and we don’t need to compute the average. In comparison, <code>k-modes</code> in k-medoids can be applied to a wide range of data, but we have problems of complexity.</p></li>
            <li><p><strong>Need to specify <span class="math inline">\(k\)</span></strong>, the number of clusters, in advance.</p>
            <p>But there are ways to automatically determine the best <span class="math inline">\(k\)</span>.</p>
            <p>We could think to execute k-means a number of times by increasing the value of k and use the value of k that guarantees us to have the minimum value of the cost function.</p>
            <p>The minimum value is achieved when <span class="math inline">\(k = n\)</span>, we only have one point in the cluster and <span class="math inline">\(p - c_i = 0\)</span>, with the increasing of the number of clusters we have the decreasing of <span class="math inline">\(E\)</span>.</p>
            <p>This is why this approach is not correct. I cannot use the minimum value of the cost function to determine the value of <span class="math inline">\(k\)</span>. But we will se an approach to do that.</p></li>
            <li><p><strong>Sensitive to noisy data and outliers</strong>.</p>
            <p>If we have an outlier, for how the algorithm works, the outlier has to be located in one cluster.</p>
            <p>The effect that we have is attracting the center of the cluster, moving it form the natural position.</p>
            <p><code>k-means</code> is sensitive to outliers because each object has to be place in one cluster.</p>
            <blockquote>
            <p>The sensitivity to noise can be defeated with <code>k-memoids</code>, in which we use as representative directly one point on the cluster. Instead of computing the mean, we choose some point in the cluster, limiting the effect.</p>
            </blockquote></li>
            <li><p><strong>Not suitable for discovering clusters with non-convex shapes</strong></p>
            <p>it derives from the use of euclidian distance but also with other types of distances. We can determine only convex shapes, in particular with the euclidian we have a spheric shape.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="k-means-variants">k-means variants</h4>
            <p>Most of the variants of the k-means differ in:</p>
            <ul>
            <li><p>Selection of the initial k means</p></li>
            <li><p>Dissimilarity calculations</p></li>
            <li><p>Strategies to calculate cluster means</p></li>
            </ul>
            <p>If we want to handle categorical data, we can use <strong>k-medoids</strong>, replacing means of clusters with modes.</p>
            <p>We will use new dissimilarity measures to deal with categorical objects and a frequency-based method to update modes of clusters.</p>
            <p>We can also use a mixture of categorical and numerical data, the <code>k-prototype method</code>.</p>
            <h4 class="unnumbered" data-number="" id="what-is-the-problem-of-the-k-means-method">What Is the Problem of the K-Means method ?</h4>
            <blockquote>
            <p>The k-means method is sensitive to outliers!</p>
            </blockquote>
            <p>the k-medoids solve this problem by choosing the most centrally located object in a cluster. Ex:</p>
            <p><img src="../media/image338.png" /></p>
            <h3 data-number="6.5.3" id="k-medoids"><span class="header-section-number">6.5.3</span> K-medoids</h3>
            <h4 class="unnumbered" data-number="" id="the-idea">The idea</h4>
            <ul>
            <li><p>Initial representatives are chosen randomly</p></li>
            <li><p>The iterative process of replacing representative objects by no representative objects continues as long as the quality of the clustering is improved</p>
            <ul>
            <li><p>For each representative <code>Object O</code> (medoid) and <code>object R</code> non-representative swap <code>O</code> and <code>R</code></p></li>
            <li><p>Choose the configuration with the lowest cost</p></li>
            <li><p>Cost function is the difference in absolute error-value if a current representative object is replaced by a non-representative object</p></li>
            </ul></li>
            </ul>
            <blockquote>
            <p>The cost function is the difference in absolute error-value if a current representative object is replaced by a non- representative object</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="pam-typical-k-medoids-algorithm">PAM (typical K-Medoids Algorithm)</h4>
            <p><img src="../media/image339.png" /></p>
            <p>We assume we want to determine <span class="math inline">\(k = 2\)</span> clusters, choose arbitrarily k-object as medoids, and assign each remaining object to the nearest medoid.</p>
            <p>We select a non-medoid object and compute the total cost of swapping, if it improves the quality of the cluster (reduce the cost function) we will accept swapping, otherwise we will select another non-medoid object.</p>
            <p><img src="../media/image340.png" /></p>
            <p>We ask if we can replace the medoid <span class="math inline">\(8\)</span> with another medoid to decrease the cost function.</p>
            <p>First, we compute the cost function for the current situation we have, for medoids <span class="math inline">\(2\)</span> and <span class="math inline">\(8\)</span>. We find out that the cost is <span class="math inline">\(20\)</span>.</p>
            <p><span class="math display">\[
                E = \sum_{i=1}^{k}{\sum_{p\in C_i}{|p-o_i|}} =  |o_1-o_2| + |o_3-o_2| + |o_4-o_2|
            \]</span> <span class="math display">\[
                + \ |o_5-o_8| + |o_6-o_8| + |o_7-o_8| + |o_9-o_8| + |o_{10}-o_8| =
            \]</span> <span class="math display">\[
                = (3+4+4) + (3+1+1+2+2) = 20
            \]</span></p>
            <p>We extract randomly the object 7 to replace object 8.</p>
            <p>We must be sure that this gives benefits.</p>
            <p><span class="math display">\[
                S = newE - E = 22 - 20 = 2
            \]</span></p>
            <p><span class="math display">\[
                S &gt; 0 \implies \text{no replacement} 
            \]</span></p>
            <h3 data-number="6.5.4" id="different-cases"><span class="header-section-number">6.5.4</span> 4 different cases</h3>
            <p><img src="../media/image344.png" /></p>
            <p>Let’s assume we have the two classes at the beginning.</p>
            <p>If we change the representative for the cluster 2, from that B to the other, the situation do not change.</p>
            <p>In the second case, B is replaced from another B and the cluster changes, and P that initially was in cluster 2, will belong to cluster 1, just because is closer to medoid A than medoid 2.</p>
            <p><img src="../media/image345.png" /></p>
            <p>In this other case we have just a change in B but P remains.</p>
            <p>In the fourth, P is reassigned to B instead of A.</p>
            <p>The cluster can change because the point can be attracted by another cluster.</p>
            <p>Partitioning Around Medoids (PAM) approach has this pseudo-code:</p>
            <h4 class="unnumbered" data-number="" id="current-pam-pseudocde">Current PAM pseudocde</h4>
            <div class="sourceCode" id="cb15"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="kw">def</span> PAM(data, k):</span>
            <span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a></span>
            <span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a>    medoids <span class="op">=</span> randomly_initialize_medoids(data, k)</span>
            <span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a></span>
            <span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
            <span id="cb15-6"><a href="#cb15-6" aria-hidden="true"></a></span>
            <span id="cb15-7"><a href="#cb15-7" aria-hidden="true"></a>        <span class="co"># assign each data point to the closest medoid</span></span>
            <span id="cb15-8"><a href="#cb15-8" aria-hidden="true"></a>        clusters <span class="op">=</span> assign_data_to_clusters(data, medoids)</span>
            <span id="cb15-9"><a href="#cb15-9" aria-hidden="true"></a></span>
            <span id="cb15-10"><a href="#cb15-10" aria-hidden="true"></a>        <span class="co"># calculate the total cost of the current medoids</span></span>
            <span id="cb15-11"><a href="#cb15-11" aria-hidden="true"></a>        current_cost <span class="op">=</span> calculate_total_cost(clusters, medoids)</span>
            <span id="cb15-12"><a href="#cb15-12" aria-hidden="true"></a></span>
            <span id="cb15-13"><a href="#cb15-13" aria-hidden="true"></a>        <span class="co"># initialize variables to track new medoids and lowest cost</span></span>
            <span id="cb15-14"><a href="#cb15-14" aria-hidden="true"></a>        new_medoids <span class="op">=</span> []</span>
            <span id="cb15-15"><a href="#cb15-15" aria-hidden="true"></a>        lowest_cost <span class="op">=</span> current_cost</span>
            <span id="cb15-16"><a href="#cb15-16" aria-hidden="true"></a></span>
            <span id="cb15-17"><a href="#cb15-17" aria-hidden="true"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
            <span id="cb15-18"><a href="#cb15-18" aria-hidden="true"></a></span>
            <span id="cb15-19"><a href="#cb15-19" aria-hidden="true"></a>            <span class="co"># select a random medoid for each current medoid </span></span>
            <span id="cb15-20"><a href="#cb15-20" aria-hidden="true"></a>            temp_medoids[i] <span class="op">=</span> random.sample(data, k)</span>
            <span id="cb15-21"><a href="#cb15-21" aria-hidden="true"></a></span>
            <span id="cb15-22"><a href="#cb15-22" aria-hidden="true"></a>            <span class="co"># assign data points to clusters with new medoids</span></span>
            <span id="cb15-23"><a href="#cb15-23" aria-hidden="true"></a>            temp_clusters <span class="op">=</span> assign_data_to_clusters(data, temp_medoids)</span>
            <span id="cb15-24"><a href="#cb15-24" aria-hidden="true"></a></span>
            <span id="cb15-25"><a href="#cb15-25" aria-hidden="true"></a>            <span class="co"># calculate the total cost of the new medoids</span></span>
            <span id="cb15-26"><a href="#cb15-26" aria-hidden="true"></a>            temp_cost <span class="op">=</span> calculate_total_cost(temp_clusters, temp_medoids)</span>
            <span id="cb15-27"><a href="#cb15-27" aria-hidden="true"></a></span>
            <span id="cb15-28"><a href="#cb15-28" aria-hidden="true"></a>            <span class="co"># if new medoids result in lower cost, update new medoids and lowest cost</span></span>
            <span id="cb15-29"><a href="#cb15-29" aria-hidden="true"></a>            <span class="cf">if</span> temp_cost <span class="op">&lt;</span> lowest_cost:</span>
            <span id="cb15-30"><a href="#cb15-30" aria-hidden="true"></a>                new_medoids <span class="op">=</span> temp_medoids</span>
            <span id="cb15-31"><a href="#cb15-31" aria-hidden="true"></a>                lowest_cost <span class="op">=</span> temp_cost</span>
            <span id="cb15-32"><a href="#cb15-32" aria-hidden="true"></a></span>
            <span id="cb15-33"><a href="#cb15-33" aria-hidden="true"></a>        <span class="co"># check for convergence by comparing new and old medoids</span></span>
            <span id="cb15-34"><a href="#cb15-34" aria-hidden="true"></a>        <span class="cf">if</span> medoids_have_converged(medoids, new_medoids):</span>
            <span id="cb15-35"><a href="#cb15-35" aria-hidden="true"></a>            <span class="cf">return</span> clusters</span>
            <span id="cb15-36"><a href="#cb15-36" aria-hidden="true"></a></span>
            <span id="cb15-37"><a href="#cb15-37" aria-hidden="true"></a>        medoids <span class="op">=</span> new_medoidss</span></code></pre></div>
            <h4 class="unnumbered" data-number="" id="original-pam-pseudocde">Original PAM pseudocde</h4>
            <p><img src="../media/image347.png" /> <img src="../media/image348.png" /></p>
            <p>This version constructs the initial medoid <span class="math inline">\(m_1\)</span> as the minimum of the distance of each object on the dataset and the medoid we are considering.</p>
            <p>The complexity is high just for the initial construction.</p>
            <p>In second step we use pairs where <span class="math inline">\(i\)</span> are the k-medoids and <span class="math inline">\(j\)</span> are the other objects, We have to consider all other objects to decrease the objective cost.</p>
            <p>When the number of objects is high, this algorithm is not applicable.</p>
            <p>The original PAM is computationally very heavy.</p>
            <h4 class="unnumbered" data-number="" id="pam-vs-k-means">PAM vs K-means</h4>
            <ul>
            <li><p>Pam is more robust than k-means in the presence of noise and outliers because a medoid is less influenced by outliers or other extreme values than a mean.</p></li>
            <li><p>Pam works efficiently for small data sets but does not scale well for large data sets. The complexity is <span class="math inline">\(O(k(n-k)^2)\)</span> for each iterationwhere n is the number of data and k is the number of clusters.</p></li>
            </ul>
            <p>In the literature were proposed approaches to have efficiency improvement on PAM.</p>
            <h3 data-number="6.5.5" id="clara"><span class="header-section-number">6.5.5</span> CLARA</h3>
            <p><strong>CLARA</strong> draw a sample of the dataset and applies PAM on the sample to find the medoids.</p>
            <p>We reduce computational effort reducing number of objects, using sampling.</p>
            <p>If the sample is representative the medoids of the sample should approximate the medoids of the entire dataset.</p>
            <p>Medoids are chosen from the sample.</p>
            <p>Note that the algorithm cannot find the best solution if one of the best k‐medoids is not among the selected sample.</p>
            <p>We can have a good approximation anyway.</p>
            <p>To improve the approximation, multiple samples are drawn and the best clustering is returned as the output.The clustering accuracy is measured by the average dissimilarity of all objects in the entire dataset.</p>
            <p>We start from the overall dataset, generate samples and apply PAM on each sample and then we just the best clustering we have from the different outputs</p>
            <p><img src="../media/image349.png" /></p>
            <h4 class="unnumbered" data-number="" id="idea">Idea</h4>
            <p>For <span class="math inline">\(i\)</span> from 1 to <span class="math inline">\(R\)</span>, repeat the following steps</p>
            <ol type="1">
            <li><p>Draw a sample of objects randomly from the entire data set and call the algorithm PAM to find k medoids of the sample.</p></li>
            <li><p>For each object in the entire data set, determine which of the k medoids is the most like it.</p></li>
            <li><p>Calculate the average dissimilarity ON THE ENTIRE DATASET of the clustering obtained in the previous step. If this value is less than the current minimum, use this value as the current minimum, and retain the k medoids found in Step (b) as the best set of medoids obtained so far.</p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="strength">Strength:</h4>
            <ul>
            <li>deals with larger data sets than PAM, the complexity is squared but with the size of the sample and linear with the total number of objects. It is <span class="math inline">\(O(ks^2 + k(n-k))\)</span>, where s is the size of the sample, k the number of clusters and n the number of objects.</li>
            </ul>
            <h4 class="unnumbered" data-number="" id="weakness-1">Weakness:</h4>
            <ul>
            <li><p>Efficiency depends on the sample size</p></li>
            <li><p>A good clustering based on samples will not necessarily represent a good clustering of the whole data set if the sample is biased</p></li>
            </ul>
            <h3 data-number="6.5.6" id="clarans"><span class="header-section-number">6.5.6</span> CLARANS</h3>
            <p>To improve its quality was proposed <code>CLARANS</code> (A Clustering Algorithm based on Randomized Search).</p>
            <p>The clustering process can be presented as searching a graph where every node is a potential solution, that is, a set of k medoids.</p>
            <p>Two nodes are neighbors if their sets differ by only one medoid.</p>
            <p><img src="../media/image350.png" /></p>
            <p>Each node is associated with a cost that is defined to be the total dissimilarity between every object and the medoid of its cluster.</p>
            <p>The problem corresponds to search for a minimum on the graph.</p>
            <p>At each step, all neighbors of <code>current_node</code> are searched; the neighbor who corresponds to the deepest descent in cost is chosen as the next solution.</p>
            <p>We start from a node that consists of a set of medoid and we investigate changing of one medoid if we can improve the cost function. In that case we move to the node that improves the cost function and we investigate all directly connected nodes with one medoid different.</p>
            <p>For large values of <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>, examining <span class="math inline">\(k(n‐k)\)</span> neighbors is time consuming.</p>
            <p>At each step, <code>CLARANS</code> draws sample of neighbors to examine.</p>
            <p>We initialize a sample of neighbors instead of all possible.</p>
            <p>Note that <code>CLARA</code> draws a sample of nodes at the beginning of search; therefore, <code>CLARANS</code> has the benefit of not confining the search to a restricted area.</p>
            <p>If the local optimum is found, <code>CLARANS</code> starts with a new randomly selected node in search for a new local optimum. The number of local optimums to search for is a parameter.</p>
            <p>It is more efficient and scalable than both PAM and CLARA; returns higher quality clusters.</p>
            <p>Complexity is <span class="math inline">\(O(n)\)</span></p>
            <p><img src="../media/image351.png" /></p>
            <p>We must fix numlocal and maxneighbor.</p>
            <p>Each node in <span class="math inline">\(G_{n,k}\)</span> is represented by a set of k objects</p>
            <p>Two nodes are neighbors if their sets differ by only one object. More formally, two nodes <span class="math inline">\(S_1=\{O_{m1}, \dots, O_{mk}\}\)</span> and <span class="math inline">\(S_2 = \{O_{w1},\dots, O_{wk}\}\)</span> are neighbors if and only if the cardinality of the intersection of <span class="math inline">\(S_1, S_2\)</span> is <span class="math inline">\(k-1\)</span>.</p>
            <h3 data-number="6.5.7" id="differences-between-clara-and-clarans"><span class="header-section-number">6.5.7</span> Differences between <code>CLARA</code> and <code>CLARANS</code></h3>
            <p><img src="../media/image352.png" /></p>
            <p>In CLARA we draw a sample of nodes at the beginning of the source, neighbors are from chosen sample and we are confining this way the area.</p>
            <p><img src="../media/image353.png" /></p>
            <p>Here we work with all original dataset.</p>
            <p>In conclusion, the clustering process can be presented as searching a graph where every node is a potential solution, that is, a set of k medoids. Two nodes are neighbors in the graph if their sets differ by only one object.</p>
            <h3 data-number="6.5.8" id="conclusions-on-k-medois-methods"><span class="header-section-number">6.5.8</span> Conclusions on K-medois methods</h3>
            <ul>
            <li><p><code>PAM</code> examines all the neighbors of the current node in its search for a minimum cost</p></li>
            <li><p><code>CLARA</code> draws a sample of nodes at the beginning of a search and apply PAM to the subsets</p></li>
            <li><p><code>CLARANS</code> dynamically draws a random sample of neighbors in each step of a search. We explore neighbors of nodes to investigate if we can reduce the cost function.</p></li>
            </ul>
            <blockquote>
            <p>At the end we have the partition with <code>k-medoids</code>.</p>
            </blockquote>
            <blockquote>
            <p><code>k</code> must be fixed and only convex shapes can be found.</p>
            </blockquote>
            <h2 data-number="6.6" id="hierarchical-methods"><span class="header-section-number">6.6</span> Hierarchical methods</h2>
            <p>In these approaches we determine a hierarchy of clusters. Some clusters are included in clusters at higher level.</p>
            <p>They use distance matrix as clustering criteria. These methods do not require the number of clusters <span class="math inline">\(k\)</span> as an input, but needs a termination condition.</p>
            <p>We have two approaches to generate hierarchies:</p>
            <ul>
            <li><p><strong>agglomerative approach</strong> that starts from one cluster for each object, and creates the hierarchy joining clusters.</p></li>
            <li><p><strong>divisive approach</strong> that starts with all objects in one cluster and tries to split clusters in sub-clusters. At each level it splits clusters creating this hierarchy.</p></li>
            </ul>
            <p><img src="../media/image354.png" /></p>
            <p>We don’t have the view we have before, we have a tridimensional view, at each level we have different partitions.</p>
            <p>We will cut the hierarchy at some level and exploit the partition I have at some specific level.</p>
            <p>In some applications we can exploit the relation between a cluster and its descendent. We use the hierarchy because the cluster is a generalization of what I can have at the lower levels.</p>
            <h4 class="unnumbered" data-number="" id="strategies-for-joiningdividing-clusters">Strategies for joining/dividing clusters</h4>
            <p><img src="../media/image355.png" /></p>
            <p>In <code>Diana</code> if we realize that we realize that some objects are far away from others, we probably must split.</p>
            <h4 class="unnumbered" data-number="" id="connectivity-matrix">Connectivity matrix</h4>
            <p>Hierarchical clustering frequently deals with the matrix of distances (dissimilarities) or similarities between training samples, and not with the matrix of objects.</p>
            <p>It can speed-up the computation. We can compute it from the matrix of objects.</p>
            <p>It is sometimes called connectivity matrix.</p>
            <p>To merge or split subsets of points rather than individual points, the distance between individual points must be generalized to the distance between subsets.</p>
            <h4 class="unnumbered" data-number="" id="linkage-metrics">Linkage metrics</h4>
            <p>I cannot reason on distance between objects, but we have to generalize the concept of distance defining distance between clusters.</p>
            <p>Such derived proximity measure is called a <strong>linkage metric</strong>.</p>
            <p>Linkage metrics are constructed from elements of the connectivity matrix. The type of the linkage metric used significantly affects hierarchical algorithms, since it reflects the concept of closeness and connectivity.</p>
            <p>If we change the linkage metric, we use the results change.</p>
            <p>These are metrics to measure distances between sets, major inter-cluster linkage metrics include <strong>single link</strong>, <strong>average link</strong>, and <strong>complete link</strong>.</p>
            <p>We exploit the dissimilarity matrix to compute them.</p>
            <blockquote>
            <p>The underlying dissimilarity measure (usually, distance) is computed for every pair of points with one point in the first set and another point in the second set. A specific operation such as minimum (single link), average (average link), or maximum (complete link) is applied to pair-wise dissimilarity measures:</p>
            </blockquote>
            <p><span class="math display">\[  
                d = \underset{x \in C_i, y \in C_2}{operation}(d(x,y))
            \]</span></p>
            <ul>
            <li><p><strong>Single</strong> link (nearest neighbor). The distance between two clusters is determined by the distance of the two closest objects (nearest neighbors) in the different clusters.</p>
            <p>This rule will, in a sense, string objects together to form clusters, and the resulting clusters tend to represent long chains.</p>
            <p><img src="../media/image357.png" /></p>
            <p>I must compute the distance between each object of the first and second cluster and select the minimum distance. It corresponds to the distance of nearest neighbors.</p></li>
            <li><p><strong>Complete link</strong> (furthest neighbor). The distance between two clusters is determined by the greatest distance between any two objects (furthest neighbors) in the different clusters.</p>
            <p>This method usually performs quite well in cases when the objects form naturally distinct "clumps." If the clusters tend to be somehow elongated or of a "chain" type nature, then this method is inappropriate.</p>
            <p><img src="../media/image358.png" /></p>
            <p>I must compute the distance between each object of the first and second cluster and select the largest distance.</p></li>
            <li><p><strong>Pair-group average</strong>. The distance between two clusters is calculated as the average distance between all pairs of objects in the two different clusters. This method is also very efficient when the objects form natural distinct "clumps," however, it performs equally well with elongated, "chain" type.</p>
            <p><img src="../media/image359.png" /></p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="considerations-4">Considerations</h4>
            <p>These metrics correspond to this type of scenario.</p>
            <p><img src="../media/image360.png" /></p>
            <ul>
            <li><p>In case of single link, we just put these two clusters together.</p></li>
            <li><p>In case of complete link, we connect the two clusters in the picture, the distance between the two horizontal clusters is higher than verticals.</p></li>
            </ul>
            <p>If we change linkage metrics, we change results we have in case of hierarchies.</p>
            <h4 class="unnumbered" data-number="" id="represent-the-output">Represent the output</h4>
            <p>To represent the output, we use a tree structure.</p>
            <p>A tree structure called a dendogram is commonly used to represent the process of hierarchical clustering</p>
            <p>It represents at each level the merging of clusters.</p>
            <p>A clustering of the data objects is obtained by cutting the dendogram at the desired level, then each connected component forms a cluster</p>
            <p><img src="../media/image361.png" /></p>
            <p>If we cut the dendogram at level 2 the partition we obtain:</p>
            <div class="sourceCode" id="cb16"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true"></a>   </span>
            <span id="cb16-2"><a href="#cb16-2" aria-hidden="true"></a>clusters <span class="op">=</span> [</span>
            <span id="cb16-3"><a href="#cb16-3" aria-hidden="true"></a>    [a, b],</span>
            <span id="cb16-4"><a href="#cb16-4" aria-hidden="true"></a>    [c],</span>
            <span id="cb16-5"><a href="#cb16-5" aria-hidden="true"></a>    [d,e],</span>
            <span id="cb16-6"><a href="#cb16-6" aria-hidden="true"></a>] </span></code></pre></div>
            <p>It allows us to have this relation from course clusters and fine clusters in the hierarchy.</p>
            <h4 class="unnumbered" data-number="" id="measures-for-distance-between-clusters">Measures for distance between clusters</h4>
            <ul>
            <li><p><strong>minimum distance</strong></p>
            <p><span class="math display">\[
                  dist_{min}(C_i, C_j) = \min_{p \in C, p&#39; \in C_J}{|p-p&#39;|}
              \]</span></p></li>
            <li><p><strong>maximum distance</strong></p>
            <p><span class="math display">\[
                  dist_{min}(C_i, C_j) = \max{p \in C, p&#39; \in C_J}{|p-p&#39;|}
              \]</span></p></li>
            <li><p><strong>mean distance</strong></p>
            <p><span class="math display">\[
                  dist_{min}(C_i, C_j) = |m_i-m_j|
              \]</span></p></li>
            <li><p><strong>average distance</strong></p>
            <p><span class="math display">\[
                  dist_{min}(C_i, C_j) = \frac{1}{n_i n_j} \sum_{p \in C, p&#39; \in C_J}{|p-p&#39;|}
              \]</span></p></li>
            </ul>
            <h3 data-number="6.6.1" id="algorithms"><span class="header-section-number">6.6.1</span> Algorithms</h3>
            <ul>
            <li><p><strong>Nearest-neighbor clustering algorithm</strong></p>
            <p>the minimum distance to measure the distance between clusters.</p></li>
            <li><p><strong>Single-linkage algorithm</strong></p>
            <p>The clustering process is terminated when the minimum distance between the nearest clusters exceeds a user-defined threshold.</p>
            <p>If we view the data points as nodes of a graph, with edges forming a path between the nodes in a cluster, then the merging of two clusters, Ci and Cj, corresponds to adding an edge between the nearest pair of nodes in Ci and Cj. We just connect the closest points.The resulting graph will generate a tree.Agglomerative hierarchical clustering algorithm that uses the minimum distance measure is also called minimal spanning tree algorithm.</p></li>
            <li><p><strong>Farthest-neighbor clustering algorithm</strong></p>
            <p>when an algorithm uses the maximum distance to measure the distance between clusters</p></li>
            <li><p><strong>Complete-linkage algorithm</strong>, the clustering process is terminated when the maximum distance between nearest clusters exceeds a user-defined threshold.</p>
            <p>By viewing data points as nodes of a graph, with edges linking nodes, we can think of each cluster as a complete subgraph, that is, with edges connecting all the nodes in the clusters. It tends to minimize the increase in diameter of the clusters at each iteration.High quality in case of clusters compact and of approximately equal size.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="considearations">Considearations</h4>
            <p>When we use single link we don’t care about the diameter of clusters (maximum distance between objects), we just merge them considering the nearest neighbors.</p>
            <p>In case of complete link we take the diameter in consideration.</p>
            <p>This is the real difference between the two approaches.</p>
            <p>If we use different types of linkage metrics, we will have different dendograms.</p>
            <h4 class="unnumbered" data-number="" id="example-5">Example</h4>
            <p>We want to apply a hierarchical clustering approach using two different linkage metrics: single-linkage (b), complete-linkage(c)</p>
            <p><img src="../media/image363.png" /></p>
            <ol type="1">
            <li><p><strong>Single linkage</strong></p>
            <ul>
            <li><p>At the beginning, we have one cluster for each object and we start to merge them.</p></li>
            <li><p>We have to find the minimum distance between clusters, at the beginning corresponds to compute distance between points, we have a point in each cluster.</p></li>
            <li><p>We merge A,B then C,D then F,G then H,J.</p></li>
            <li><p>At the second level of the dendrogram we have to decide which cluster have to be merged.</p></li>
            <li><p>If we analyze distances between clusters, for example the distance between F,G and H,J is the distance between G and H.</p></li>
            <li><p>We choose to merge E with F,G we assume E is closer to F than D and we create a cluster with three elements.</p></li>
            <li><p>At the next level we merge clusters closest considering the single-linkage and so on. Cluster E,F,G is close to C,D.</p></li>
            <li><p>A,B is closer to C,D,E,F,G rather than H,J.</p></li>
            <li><p>In the last level we just merge the two remaining clusters.</p></li>
            <li><p>I can cut at some level the dendrogram and obtain a flat partition.</p></li>
            </ul></li>
            <li><p><strong>complete-linkage</strong></p>
            <ul>
            <li><p>we have no differences at the first level, because when we consider each point in each clusters we compute the distance between points.</p></li>
            <li><p>From the second level we consider the maximum distance between objects belonging to each class.</p></li>
            <li><p>We are considering the maximum distance between clusters and we select the minimum between these maximum distances,</p></li>
            <li><p>The maximum distance between E and F,G is the distance between E and G.</p></li>
            <li><p>There are differences between the two dendrograms, due to the different kind of linkage metrics we are using. The process is the same.</p></li>
            </ul></li>
            </ol>
            <h3 data-number="6.6.2" id="agnes-agglomerative-nesting"><span class="header-section-number">6.6.2</span> AGNES (Agglomerative Nesting)</h3>
            <p>In this strategy we start from one point for each cluster and we agglomerate.</p>
            <p>We use the single-link method and the dissimilarity matrix, not directly the objects.</p>
            <p>Initially, each object is placed into a cluster. Clusters are merged according to some criteria.</p>
            <p>For instance, if the distance between two objects belonging to two different clusters is the minimum distance between any two objects from different clusters (single-linkage approach).</p>
            <p>The cluster merging process repeats until all of the objects are eventually merged to form one cluster.</p>
            <p>We can enforce a stopping condition, for example the maximum number of objects inside a cluster</p>
            <p><strong>Single-linkage approach</strong>: each cluster is represented by all of the objects in the cluster, and the similarity between two clusters is measured by the similarity of the closest pair of data points belonging to different clusters.</p>
            <p><img src="../media/image364.png" /></p>
            <h4 class="unnumbered" data-number="" id="example-using-single-linkage-approach">Example using single-linkage approach</h4>
            <p>Let’s consider <span class="math inline">\(5\)</span> objects and in particular the dissimilarity metric.</p>
            <h5 class="unnumbered" data-number="" id="steps-2">steps</h5>
            <p>Each value represents the distance between the object in the row and in the column.</p>
            <p>We start from one object for each cluster, so if we have to merge clusters we have to merge objects. We don’t consider the diagonal, of course.</p>
            <ul>
            <li><p><strong>d = 1</strong></p>
            <p>given the distance matrix</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>A</th>
            <th>B</th>
            <th>C</th>
            <th>D</th>
            <th>E</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><strong>A</strong></td>
            <td>0</td>
            <td>1</td>
            <td>2</td>
            <td>2</td>
            <td>3</td>
            </tr>
            <tr class="even">
            <td><strong>B</strong></td>
            <td>1</td>
            <td>0</td>
            <td>2</td>
            <td>4</td>
            <td>3</td>
            </tr>
            <tr class="odd">
            <td><strong>D</strong></td>
            <td>2</td>
            <td>2</td>
            <td>0</td>
            <td>1</td>
            <td>3</td>
            </tr>
            <tr class="even">
            <td><strong>C</strong></td>
            <td>2</td>
            <td>4</td>
            <td>1</td>
            <td>0</td>
            <td>3</td>
            </tr>
            <tr class="odd">
            <td><strong>E</strong></td>
            <td>3</td>
            <td>3</td>
            <td>5</td>
            <td>3</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p>we merge</p>
            <p><span class="math display">\[
                  A \cup B    \\
                  C \cup D    
              \]</span></p></li>
            <li><p><strong>d = 2</strong></p>
            <p>We create two clusters and recompute the distance matrix.</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>AB</th>
            <th>CD</th>
            <th>E</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><strong>AB</strong></td>
            <td>0</td>
            <td>2</td>
            <td>3</td>
            </tr>
            <tr class="even">
            <td><strong>CD</strong></td>
            <td>2</td>
            <td>0</td>
            <td>3</td>
            </tr>
            <tr class="odd">
            <td><strong>E</strong></td>
            <td>3</td>
            <td>3</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p>the minimum is 2 and so we merge</p>
            <p><span class="math display">\[
                  (AB) \cup (CD)    
              \]</span></p></li>
            <li><p><strong>d = 3</strong></p>
            <p>given the distance matrix</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th>ABCD</th>
            <th>E</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><strong>ABCD</strong></td>
            <td>0</td>
            <td>3</td>
            </tr>
            <tr class="even">
            <td><strong>E</strong></td>
            <td>3</td>
            <td>0</td>
            </tr>
            </tbody>
            </table>
            <p>we merge</p>
            <p><span class="math display">\[
                  (ABCD) \cup (E)    
              \]</span></p></li>
            </ul>
            <h3 data-number="6.6.3" id="diana-divisive-analysis"><span class="header-section-number">6.6.3</span> DIANA (Divisive Analysis)</h3>
            <p><code>DIANA</code> uses the inverse order of <code>AGNES</code>. We start from all objects inside the cluster and we terminate when we have one object for each cluster.</p>
            <p><img src="../media/image368.png" /></p>
            <ul>
            <li><p>The algorithm constructs a hierarchy of clusters, starting with one large cluster containing all <span class="math inline">\(n\)</span> samples. Clusters are divided until each cluster contains only a single sample.</p></li>
            <li><p>At each stage, the cluster with the largest dissimilarity between any two of its samples is selected.</p></li>
            <li><p>To divide the selected cluster, the algorithm first looks for its most disparate sample.</p>
            <p>This observation initiates the <code>splinter group</code>.</p>
            <p>In subsequent steps, the algorithm reassigns observations that are closer to the <code>splinter group</code> than to the <code>old party</code>.</p>
            <p>When we split clusters we start from the most different point and verify if the points in the old cluster belongs to the new cluster or have to remain in the old cluster.</p>
            <p>We have to compute the distance between this new cluster and the cluster we had before and we move points to the new cluster if the distance is smaller with it then the old cluster. The result is a division of the selected cluster into two new clusters.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="the-algorithm">The algorithm</h4>
            <ol type="1">
            <li><p>We start from objects in one single cluster. Find the object, which has the highest average dissimilarity to all other objects.</p>
            <p>This object initiates a new cluster a sort of a splinter group.</p></li>
            <li><p>For each object <span class="math inline">\(i\)</span> outside the splinter group compute</p>
            <p><span class="math display">\[
                 D_i = [ average d(i,j),  j \notin R_{splinter group} ]  \\ 
                 - [ average d(i,j), j \in R_{splinter group} ]
             \]</span></p></li>
            <li><p>Find an object <span class="math inline">\(h\)</span> for which the difference <span class="math inline">\(D_h\)</span> is the largest.</p>
            <p>If <span class="math inline">\(D_h\)</span> is positive, then <span class="math inline">\(h\)</span> is, on the average close to the splinter group.</p></li>
            <li><p>Repeat Steps 2 and 3 until all differences <span class="math inline">\(D_h\)</span> are negative. The data set is then split into two clusters.</p></li>
            <li><p>Select the cluster with the largest diameter. The diameter of a cluster is the largest dissimilarity between any two of its objects.</p>
            <p>Then divide this cluster, following steps 1-4.</p></li>
            <li><p>Repeat Step 5 until all clusters contain only a single object.</p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="considerations-5">Considerations</h4>
            <ul>
            <li><p>When we split the initial cluster, we have to decide which is the cluster to which we have to work to generate the splinter group. We select one cluster among all available, and it will be the cluster with the largest diameter.</p></li>
            <li><p>The diameter is the largest dissimilarity between any two objects belonging to the cluster.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="example-6">Example</h4>
            <p>Let’s suppose to have <span class="math inline">\(8\)</span> data objects and that each object is described by only one attribute.</p>
            <p><img src="../media/image369.png" /></p>
            <p>We generate. starting from the dataset, the dissimilarity matrix.</p>
            <p>We compute the distance between pairs of objects.</p>
            <p>Let’s assume all objects are contained in the same cluster.</p>
            <p>We want to decide how to split this cluster.</p>
            <p><img src="../media/image370.png" /></p>
            <p>The mean for each row is the mean of distances between the object in the row and all others.</p>
            <p>If it’s very high, this is quite different from the other objects.</p>
            <p>We compute this mean from all objects and we start the splinter group with objects with the highest mean dissimilarity.</p>
            <p><img src="../media/image371.png" /></p>
            <p>All other points remain in the old cluster.</p>
            <p>Now we have to decide if objects belonging to the old cluster have to remain there or pass to the new cluster, established by the dissimilarity between each point and the other points in the cluster and the points in the new cluster we found.</p>
            <p><img src="../media/image372.png" /></p>
            <p>We obtain that the maximum mean distance for A1 is obtained by X2, so we compute how much is dissimilar from A1 and B1.</p>
            <p>The mean dissimilarity between X2 and B1 is lower, so we can decide that X2 is closer to the new cluster than the old one.</p>
            <p>Now we consider the most dissimilar in A2, X3, and compute the distance between it and the new cluster.</p>
            <p><img src="../media/image373.png" /></p>
            <p>Again, we conclude that X3 has to be moved to the new cluster.</p>
            <p><img src="../media/image374.png" /></p>
            <p>X5 remains in the old cluster.</p>
            <p>We have two clusters now, to choose which cluster to divide:</p>
            <p><img src="../media/image375.png" /></p>
            <p>We have to choose the cluster by considering the diameter, the maximum distance between objects belonging to the same cluster.</p>
            <p>Diam B4&gt;diam A4, thus we are going to divide the cluster B4</p>
            <p>The most dissimilar point is X4 and so we start a new splinter group with it.</p>
            <p><img src="../media/image376.png" /></p>
            <p>We need to consider if other points can belong to the splinter group.</p>
            <p>In B5 the most dissimilar point is X3, we compute the mean dissimilarity.</p>
            <p><img src="../media/image377.png" /></p>
            <p><img src="../media/image378.png" /></p>
            <p>We repeat until we get one cluster for each object.</p>
            <p><img src="../media/image379.png" /></p>
            <p>If we store at each step the partition, we generate we get the dendogram.</p>
            <h4 class="unnumbered" data-number="" id="weaknessesof-hierarchical-clustering">weaknessesof hierarchical clustering</h4>
            <ul>
            <li><p>Can never undo what was done previously</p></li>
            <li><p>Do not scale well: time complexity of at least <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the number of total objects, each time we split we have to reconsider if we have to move objects between clusters.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="major-strengths">Major strengths</h4>
            <ul>
            <li><p>It’s nice that you get a hierarchy instead of an amorphous collection of groups</p></li>
            <li><p>Don’t need to specify k, because at each level we have a different k.If you want k groups, just cut the (k‐1) longest links. We just must decide the cutting condition and implicitly we fix it implicitly. But we do not use parameters.</p></li>
            <li><p>In general, give better quality clusters than k‐means’ like methods. The shape of clusters is convex</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="conslusions">Conslusions</h4>
            <p>To manage the complexity of this algorithm, several approaches to reduce it have been proposed.</p>
            <ul>
            <li><p><code>BIRCH</code> (1996): uses CF-tree and incrementally adjusts the quality of sub-clusters</p></li>
            <li><p><code>CHAMELEON</code> (1999): hierarchical clustering using dynamic modeling</p></li>
            </ul>
            <h3 data-number="6.6.4" id="birch-balanced-iterative-reducing-and-clustering-using-hierarchies"><span class="header-section-number">6.6.4</span> BIRCH: Balanced Iterative Reducing and Clustering Using Hierarchies</h3>
            <p>This is an agglomerative clustering designed for clustering a large amount of numerical data.</p>
            <h4 class="unnumbered" data-number="" id="what-does-birch-algorithm-try-to-solve">What does BIRCH algorithm try to solve?</h4>
            <ul>
            <li><p>Most of the existing algorithms DO NOT consider the case that datasets can be too large to fit in main memory, we try to work with a description of the dataset in the main memory</p></li>
            <li><p>They DO NOT concentrate on minimizing the number of scans of the dataset</p></li>
            <li><p>I/O costs are very high, it tries to reduce it</p></li>
            </ul>
            <p>The complexity of <code>BIRCH</code> is <span class="math inline">\(O(n)\)</span> where <span class="math inline">\(n\)</span> is the number of objects to be clustered.</p>
            <p><strong>BIRCH works with sequence of objects.</strong></p>
            <h4 class="unnumbered" data-number="" id="idea-1">Idea</h4>
            <p>At the beginning we are at the first object; we receive the second and if the cluster of the first is too large adding the object 2 we split the cluster.</p>
            <p>Being too large means fixing the diameter of the cluster, and if it is larger, we split the cluster into two clusters.</p>
            <p>This decision of splitting or not is taken by considering the diameter of the cluster obtained by adding the new object.</p>
            <p><img src="../media/image380.png" /></p>
            <p>In our data structure we have a data structure where an entry identifies the first cluster and another identify the second,</p>
            <p><img src="../media/image381.png" /></p>
            <p>When we analyze the third object, we must decide if it has to be included in the first or second cluster.</p>
            <p>It is closer to object 1 than 2, we try to add it to cluster 1, if it becomes too large by adding it we split the cluster.</p>
            <p><img src="../media/image382.png" /></p>
            <p>We add another entry that identify the new cluster we added.</p>
            <p><img src="../media/image383.png" /></p>
            <p>Analyzing the fourth we discover it is closest to the entry 3.</p>
            <p><img src="../media/image384.png" /></p>
            <p>For entry 3 we have a cluster with two elements, the cluster 2 remains compact when adding it.</p>
            <p><img src="../media/image385.png" /></p>
            <p>Cluster 3 becomes too larger, and we split it.</p>
            <p>We have a limit to the number of entries a node can have; we do not split only the cluster but also the node.</p>
            <p>We create two leaf-node, one for cluster 1 and 3 and the other with cluster 2 and 4. And we create another node, a non-leaf node that contains links to the leaf nodes.</p>
            <p>Entry 1 will contain links to entry 1.1 and entry 1.2 and entry 2 will have links to entry 2.1 and entry 2.2.</p>
            <p><img src="../media/image386.png" /></p>
            <p>When we receive object 6, it is closest to entry 1.2 and we verify that the diameter of cluster 3 can be acceptable.</p>
            <p><img src="../media/image387.png" /></p>
            <p>We generate clusters but also a tree that helps us to store all information we need to manage clusters. Each node contains a description named Clustering Feature that consists of three numbers, (<span class="math inline">\(CF = (N,LS,SS)\)</span>).They store the summary of the statistics for a given cluster: the 0-th, 1st and 2nd moments of the cluster from the statistical point of view.</p>
            <p>They are used to compute centroids of clusters and measure the compactness and distance of clusters.</p>
            <p>The three values we store are:</p>
            <ul>
            <li><p>N: number of data points</p></li>
            <li><p>LS: linear sum of N points</p>
            <p><span class="math display">\[
                  LS = \sum_{i=1}^{n}{x_i}
              \]</span></p></li>
            <li><p>SS: square sum of N points</p>
            <p><span class="math display">\[
                  SS = \sum_{i=1}^{n}{x_i^2}
              \]</span></p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="example-7">Example</h4>
            <p><img src="../media/image390.png" /></p>
            <p>Cluster 1 is characterized by CF1, 3 because 3 are the points contained on each cluster, the linear sum of the first feature of points, and the second features’ sum.</p>
            <p>Then we have for the third value the sum of squared valued for first and second feature.</p>
            <p>The CF of cluster 3 can be obtained by the sum of the clustering feature of the two inner clusters. When we update CF1 and CF2 we also update CFs of levels toward the root.</p>
            <p>CF entry is a summary of statistics of the cluster and has sufficient information to calculate the centroid, radius, diameter and many other measures:</p>
            <ul>
            <li><p><strong>Centroid</strong>: is obtained as:</p>
            <p><span class="math display">\[
                  x_0 = \frac{\sum_{i=1}^{n}{x_i}}{n} = \frac{LS}{n}
              \]</span></p></li>
            <li><p><strong>Radius</strong>: square root of average distance from any point of the cluster to its centroid:</p>
            <p><span class="math display">\[
                  R = \sqrt{\frac{\sum_{i=1}^{n}{(x_i-x_0)^2}}{n}} = \sqrt{\frac{\sum_{i=1}^{n}{nSS -  2LS^2 + nLS^2}}{n^2}}
              \]</span></p></li>
            <li><p><strong>Diameter</strong>: square root of average mean squared distance between all pairs of points in the cluster:</p>
            <p><span class="math display">\[
                  R = \sqrt{\frac{\sum_{i=1}^{n}\sum_{k=1}^{n}{(x_i-x_j)^2}}{n(n-1)}} = \sqrt{\frac{2nSS -2LS^2}{n(n-1)}}
              \]</span></p></li>
            </ul>
            <p>By using three values we can determine parameters to define our cluster.</p>
            <p>In this approach we can produce these metrics receiving data in streaming, we process each object each time and update <span class="math inline">\(CFs\)</span>.</p>
            <blockquote>
            <p>We don’t need to store data but each time we receive an object, we update CFs, and discard objects.</p>
            </blockquote>
            <p>The data structure we use is the CF-Tree, which is an height-balance tree (exploiting parameters we preserve the balance of the tree).</p>
            <p>We use two parameters:</p>
            <ul>
            <li><p><strong>Number of entries in each node</strong>, we fix this number of entries and when the number of entries is higher of the one we fix we have to split the node, and this is why we create a balanced tree.</p></li>
            <li><p>The <strong>diameter of all entries in a leaf node</strong>, we put in the same cluster objects which the length of the diameter is lower than a prefixed threshold.</p></li>
            </ul>
            <p>We need to fix parameters.</p>
            <p>Leaf nodes are connected via prev and next pointers just to create this data structure.</p>
            <p>A CF-tree is a height-balanced tree that stores the clustering features CFs for a hierarchical clustering,</p>
            <p><img src="../media/image394.png" /></p>
            <p>At each level we have the description of cluster including the clusters at the subsequent level, and that’s why it’s hierarchical. <span class="math inline">\(Cf1\)</span> is a description of a cluster that contains all clusters identified by the CF you have in the descendants.</p>
            <p>Parameters:</p>
            <ul>
            <li><p><code>B</code> = Branching factor specifies the maximum number of children per nonleaf node.</p></li>
            <li><p><code>T</code> = Threshold parameter specifies the maximum diameter of subclusters stored at the leaf nodes of the tree.</p></li>
            </ul>
            <p>Sometimes we can also use:</p>
            <ul>
            <li><code>L</code> = Max. number of entries in a leaf</li>
            </ul>
            <p>We deal with non-leaf nodes and leaf nodes in different ways.</p>
            <p>CF entry in parent corresponds to the sum of CF entries of a child of that entry. This is useful because we have all the information that we need to define this super cluster exploiting the CF without accessing to the raw data in the disc.</p>
            <p>Splitting and splitting we have that the clusters in the same leaf-node are close to each other.</p>
            <p><img src="../media/image395.png" /></p>
            <p>A Leaf node represents a cluster.</p>
            <p>A sub‐cluster in a leaf node must have a diameter no greater than a given threshold <code>T</code>.</p>
            <p>A point is inserted into the leaf node (cluster) to which is closer.</p>
            <p>When one item is inserted into a cluster at the leaf node, the restriction <code>T</code> must be satisfied. The corresponding <code>CF</code> must be updated and we have to update <code>CFs</code> in the chain from the leaf to the root. If there is no space on the node the node is split.</p>
            <p>The <code>BIRCH</code> algorithm incrementally construct a <code>CF tree</code>, a hierarchical data structure for multiphase clustering.</p>
            <h4 class="unnumbered" data-number="" id="steps-3">Steps</h4>
            <ul>
            <li><p><strong>Phase 1</strong>: scan DB to build an initial in‐memory CF tree</p>
            <p>We pickup an object each time and travel the tree to identify if we can add this point to a cluster or we have to split the cluster and recompute CFs from the leaves to the root.</p>
            <p>Check if threshold condition is violated:</p>
            <ul>
            <li><p>If there is room to insert <span class="math inline">\(\to\)</span> Insert point as a single cluster</p></li>
            <li><p>If not</p>
            <ul>
            <li><p>Leaf node split: take two farthest CFs and create two leaf nodes, put the remaining CFs (including the new one) into the closest node</p></li>
            <li><p>Update CF for non‐leafs. Insert new non‐leaf entry into parent node.</p></li>
            <li><p>We may have to split the parent as well. Spilt the root increases tree height by one. If not we insert point into the closest cluster</p></li>
            </ul></li>
            </ul></li>
            <li><p><strong>Phase 2</strong>: use an arbitrary clustering algorithm to cluster the leaf nodes of the CF‐tree,</p>
            <p>The cluster we have in the leaf-node can be used as cluster or we can use another clustering algorithm, exploiting the clusters we have in the leaf node just to create some cluster with a number of clusters in the leaf node.</p>
            <p>We repeat the partition starting form clusters in the leaf-node.</p></li>
            </ul>
            <p>These 2 phases are the main one, but if we analyze more the <code>BIRCH</code> algorithm we have phases to manage trees not included in the main memory.</p>
            <p><img src="../media/image396.png" /></p>
            <h4 class="unnumbered" data-number="" id="summary-1">Summary</h4>
            <ul>
            <li><p><strong>Phase 1</strong>: Choose an initial value for threshold, start inserting the data points one by one into the tree as per the insertion algorithm.If, in the middle of the above step, the size of the CF tree exceeds the size of the available memory, increase the value of threshold.Convert the partially built tree into a new tree.</p>
            <ul>
            <li><p>The rebuild process is performed by building a new tree from the leaf nodes of the old tree.</p></li>
            <li><p>But there’s no need of rereading all the objects, we have all information in CFs.Repeat the above steps until the entire dataset is scanned and a full tree is built.</p></li>
            </ul>
            <p>We can also have Outlier Handling, because we fix the diameter and we manage them because we will have one outlier in one single clusters. This outlier will not be added to other clusters because the diameter will become too large.</p>
            <p>Depending on how we set the parameters we will handle this.</p>
            <p>If we set a big diameter this may not be true.</p></li>
            <li><p><strong>Phase 2</strong>: A bridge between phases 1 and 3.</p>
            <p>Builds a smaller CF tree by increasing the threshold.</p></li>
            <li><p><strong>Phase 3</strong>: Apply global clustering algorithm to the sub-clusters given by leaf entries of the CF tree.</p>
            <p>This improves clustering quality.</p></li>
            <li><p><strong>Phase 4</strong>: Scan the entire dataset to label the data points and we can do outlier handling.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="strengths-1">Strengths</h4>
            <ul>
            <li><p>finds a good clustering with a single scan and improves the quality with a few additional scans</p></li>
            <li><p>Complexity is <span class="math inline">\(O(n)\)</span></p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="weakness-2">Weakness</h4>
            <ul>
            <li><p>Handles only numeric data, and sensitive to the order of the data record. because we must calculate sum and mean.</p></li>
            <li><p>Sensitive to insertion order of data points, if I change the order of data the tree can change because the construction of the tree depends on this order.</p></li>
            <li><p>Since we fix the size of leaf nodes, so clusters may not be so natural</p></li>
            <li><p>Clusters tend to be spherical given the radius and diameter measures. Again, we can discover convex clusters, this is a limitation.</p></li>
            </ul>
            <h3 data-number="6.6.5" id="chameleon"><span class="header-section-number">6.6.5</span> CHAMELEON</h3>
            <p>This clustering algorithm takes in consideration different zones in the space in which we have different densities, by building this sparse graph, obtained by data objects.</p>
            <p>For each data object <span class="math inline">\(p\)</span> another data object <span class="math inline">\(q\)</span> is connected to <span class="math inline">\(p\)</span> if <span class="math inline">\(q\)</span> is among the top <span class="math inline">\(k\)</span> closest neighbors of <span class="math inline">\(p\)</span>.</p>
            <p><img src="../media/image397.png" /></p>
            <p>We generate this graph taking in consideration the <code>k nearest neighbors</code> of each object.</p>
            <p><code>CHAMELEON</code> build this graph by using this definition then apply a partition of the graph by generating the subgraph, and then, as final step, this subgraphs are merged to generate the final clusters.</p>
            <p>The merging phase is performed by exploiting the concept of relative interconnectivity and relative closeness.</p>
            <p>In terms of relative interconnectivity, two clusters <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> are merged if we have that the relative interconnectivity of each other is almost the same of the interconnectivity we have inside <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>.</p>
            <p>In terms of relative closeness, the closeness of <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> is considered over the internal closeness. We put them together if the clusters we are able to generate is characterized by a closeness similar to the closeness we have inside <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>.</p>
            <blockquote>
            <p>We’re looking for natural clusters and this is the approach we follow.</p>
            </blockquote>
            <p>The value of <span class="math inline">\(k\)</span> is used to generate the initial graph and we exploit <span class="math inline">\(k\)</span> because we connect two objects if one of them is among the <span class="math inline">\(k\)</span> closest neighbors.</p>
            <p>We connect two objects if we have a node among the <span class="math inline">\(k\)</span> nearest neighbors in fact.</p>
            <p><img src="../media/image398.png" /></p>
            <ul>
            <li><p>With <span class="math inline">\(k = 1\)</span> we have a sparse graph.</p></li>
            <li><p>With the increase of <span class="math inline">\(k\)</span> this graph is less sparse, we are considering more objects that can be connected and so we are increasing the possibility of interconnectivity.</p></li>
            </ul>
            <blockquote>
            <p>Each time we have to apply this algorithm we need to fix this parameter and the results will strongly depend on this parameter.</p>
            </blockquote>
            <p>The <code>k-nearest</code> neighbor captures the concept of neighborhood dynamically. The density of the region is recorded as <strong>the weight of the edges</strong>: the edges of a dense region tend to weigh more than those of a sparse region.</p>
            <p>I compute the k nearest neighbors but this <strong>connection</strong> between <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> is <strong>characterized by a weight</strong>, that will be higher if the distance is lower.</p>
            <blockquote>
            <p>When we partition the graph we have to take in consideration dense and sparse regions, we cut graphs in such a way to isolate dense regions so to preserve these natural clusters.</p>
            </blockquote>
            <blockquote>
            <p>The weight of each edge represents the closeness between two samples, that is, an edge will weigh more whether the two data samples are closer to each other. This allow us to generate natural clusters and they are more natural clusters than <code>DBSCAN</code>.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="graph-partitioning-algorithm">Graph-partitioning algorithm</h4>
            <ul>
            <li><p>A cluster <span class="math inline">\(C\)</span> is partitioned into subclusters <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span> so as to minimize the sum of the weight of the edges that would be cut should <span class="math inline">\(C\)</span> be bisected into <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>.</p>
            <p>We partition the graph cutting where we have a lowest edge, that means we are separating obejcts far to each other.</p></li>
            <li><p>Each one of these sub-clusters contains at least 25% of the nodes in <span class="math inline">\(C\)</span>. This is a constraint we enforce, we want to cut in a way we have subclusters.</p></li>
            <li><p>Edge cut <span class="math inline">\(EC_{\{C_i,C_j\}}\)</span> assesses the absolute interconnectivity between subclusters <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j \ \)</span>:</p>
            <p>the sum of the weight of the edges that connect vertices in <span class="math inline">\(C_i\)</span> to vertices in <span class="math inline">\(C_j\)</span>.</p>
            <blockquote>
            <p>We want to cut in such a way to limit this sum.</p>
            </blockquote>
            <p>Each edge is characterized by a weight that measures the closeness and we want to avoid to cut several edges and edge characterized by a high weight.</p></li>
            </ul>
            <p><code>CHAMELEON</code> obtains the initial set of sub-clusters as follows.</p>
            <ul>
            <li><p>Starts with all the points belonging to the same cluster.</p></li>
            <li><p>Then repeatedly selects the largest sub-cluster among the current set of sub-clusters and uses the graph-partitioning algorithm to bisect it.</p></li>
            <li><p>This process terminates when the larger sub-cluster contains fewer than a specified number of vertices (typically 1% to 5% of the overall number of data). In other words we start from the overall graph and we repeteatedly partition the graph to arrive at the termination condition we mentioned.Then we perform the agglomerative hierarchical clustering algorithm merges subclusters based on their similarity.</p></li>
            </ul>
            <p>The similarity is determined according to their <strong>relative interconnectivity</strong>, <span class="math inline">\(RI(C_i,C_j)\)</span> and their relative closeness, <span class="math inline">\(RC(C_i, C_j)\)</span>. Relative interconnectivity is defined as:</p>
            <p><span class="math display">\[
                RI(C_i. C_j) = \frac{|EC_{\{C_i,C_j\}}|}{\frac{1}{2}(|EC_{C_i}| + |EC_{C_j}|}
            \]</span></p>
            <ul>
            <li><p><span class="math inline">\(EC_{C_i}\)</span> is the internal inter-connectivity of a cluster <span class="math inline">\(C_i\)</span></p></li>
            <li><p>We have <span class="math inline">\(C_i\)</span> and if we assume to cut it by partitioning <span class="math inline">\(C_i\)</span> into roughly equal parts and compute the minimal sum of weights this corresponds to this interconnectivity.</p></li>
            </ul>
            <p>If we have that the relative interconnectivity is greater than a specified threshold, that means that if the interconnectivity is comparable of the interconnectivity inside <span class="math inline">\(C_i\)</span> and inside <span class="math inline">\(C_j\)</span>, we can consider that they can be merged.</p>
            <p>We want to consider also the <strong>relative closeness</strong> <span class="math inline">\(RC(C_i, C_j)\)</span> is defined as:</p>
            <p><span class="math display">\[
                RC(C_i, C_j) = \frac{\bar{S}_{EC_{\{C_i,C_j\}}}}{\frac{|C_i|}{|C_i| + |C_j|} \bar{S}_{EC_{C_i}} + \frac{|C_j|}{|C_i| + |C_j|} \bar{S}_{EC_{C_j}}}
            \]</span></p>
            <ul>
            <li><p><span class="math inline">\(\bar{S}_{EC_{\{C_i,C_j\}}}\)</span> is the average weight of the edges that connect vertices in <span class="math inline">\(C_i\)</span> to vertices in <span class="math inline">\(C_j\)</span></p></li>
            <li><p><span class="math inline">\(\bar{S}_{EC_{C_i}}\)</span> is the average weight of the edges that belong to the min-cut bisector of cluster <span class="math inline">\(C_i\)</span>.</p></li>
            </ul>
            <p>Also in this case we are considering if the connection between nodes when we put together <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span> is comparable to the connection we have inside <span class="math inline">\(C_i\)</span> and inside <span class="math inline">\(C_j\)</span>.</p>
            <blockquote>
            <p>The agglomerative hierarchical clustering merges only those pairs of clusters whose relative inter-connectivity and relative closeness are both above some user specified threshold <span class="math inline">\(T_{RI}\)</span> and <span class="math inline">\(T_{RC}\)</span>, respectively.</p>
            </blockquote>
            <p><code>CHAMELEON</code> visits each cluster <span class="math inline">\(C_i\)</span>, and checks to see if any one of its adjacent clusters <span class="math inline">\(C_j\)</span> satisfy the following two conditions:</p>
            <p>If more than one of the adjacent clusters satisfy the conditions, then <code>CHAMELEON</code> selects to merge <span class="math inline">\(C_i\)</span> with the cluster that it is most connected to; i.e., it selects the cluster <span class="math inline">\(C_j\)</span> such that the absolute inter-connectivity between these two clusters is the highest.</p>
            <p><span class="math display">\[
                RI(C_i, C_j) \geq T_{RI} \quad \land \quad RC(C_i, C_j) \geq T_{RC}
            \]</span></p>
            <h4 class="unnumbered" data-number="" id="conclusions-2">Conclusions</h4>
            <p><code>CHAMELEON</code> works well because it takes in consideration different density region, but it depends on k and the two thresholds.</p>
            <p>It also has a complexity <span class="math inline">\(O(n^2)\)</span> (in the worst case) where <span class="math inline">\(n\)</span> is the number of objects.</p>
            <p>We can produce clusters with different shapes. It has the power at discovering arbitrarily shaped clusters of high quality, because we take in consideration different density zones.</p>
            <p><img src="../media/image402.png" /></p>
            <p>They are not modellable form partitioning algorithms because they can just find convex clusters.</p>
            <p>It takes in consideration different zones with different densities, but the approach is quite complex.</p>
            <h2 data-number="6.7" id="density-based-methods"><span class="header-section-number">6.7</span> Density-based methods</h2>
            <p>Approaches analyzed so far are based on the concept of distance, <code>CHAMELEON</code> too because it computes the k-nearest neighbors.</p>
            <p>These clustering methods are based on density, such as density-connected points, to decide if instances belong to the same cluster.</p>
            <p>We will have different clusters when we met low dense regions, sparse regions.</p>
            <h4 class="unnumbered" data-number="" id="major-features">Major features:</h4>
            <ul>
            <li><p>Discover clusters of arbitrary shape, the reason is because we identify the cluster identifying dense regions connected.</p></li>
            <li><p>Handle noise</p></li>
            <li><p>They determine clusters just using one scan of the dataset</p></li>
            <li><p>But they need density parameters as termination condition. In these methods we do not need to fix the number of clusters we want to achieve but we need to set our definition of density, we have to define what’s dense for us and this will implicitly determine the number of clusters.</p></li>
            </ul>
            <p>We have to give as input our definition of dense regions. This is given by using two parameters:</p>
            <ul>
            <li><p><span class="math inline">\(\varepsilon\)</span> : Maximum radius of the neighborhood. Given one object we fix a radius that determine the neighborhood we are considering around one object.</p></li>
            <li><p><strong>MinPts</strong>: Minimum number of points in an <span class="math inline">\(\varepsilon\)</span>-neighborhood of that point.</p></li>
            </ul>
            <blockquote>
            <p>Density is the number of points we have in a volume. By specifying <span class="math inline">\(\varepsilon\)</span> and MinPts we are implying the density.</p>
            </blockquote>
            <p>If the <span class="math inline">\(\varepsilon\)</span>-neighborhood (neighborhood within radius <span class="math inline">\(\varepsilon\)</span> - NEps) of an object contains at least a minimum number, MinPts, of objects then the object is called core object.</p>
            <h4 class="unnumbered" data-number="" id="examples">Examples</h4>
            <ul>
            <li><p>ex 1</p>
            <p><img src="../media/image403.png" /></p>
            <p><span class="math inline">\(\varepsilon\)</span> = 1 cm, MinPts=3</p>
            <p>We have that <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> are core objects, <span class="math inline">\(q\)</span> is not because in the neighborhood we don’t have 3 objects.</p>
            <p>We define an object p as <strong>directly density-reachable</strong> from an object <span class="math inline">\(1\)</span> (respect to <span class="math inline">\(\varepsilon\)</span>, <span class="math inline">\(MinPts\)</span>) if <span class="math inline">\(p\)</span> belongs to <span class="math inline">\(N_{\varepsilon}(q)\)</span> (Neighborhood of q), <span class="math inline">\(q\)</span> is a core object, that is:</p>
            <p><span class="math display">\[
                  N_{$\varepsilon$}(q)| \geq MinPts
              \]</span></p>
            <p>(cardinality of neighborhood of q is greater or equal than minPts).</p></li>
            <li><p>ex 2</p>
            <p><img src="../media/image404.png" /></p>
            <p><span class="math inline">\(q\)</span> is directly density-reachable from <span class="math inline">\(m\)</span>, because <span class="math inline">\(m\)</span> is a core point and <span class="math inline">\(q\)</span> belongs to the neighborhood of <span class="math inline">\(m\)</span>.</p>
            <p>The viceversa is not true because <span class="math inline">\(q\)</span> is not a core object. <span class="math inline">\(m\)</span> is directly density-reachable from <span class="math inline">\(p\)</span> and vice versa.</p>
            <p>An object <span class="math inline">\(p\)</span> is <strong>density-reachable</strong> from an object <span class="math inline">\(q\)</span>, with respect to <span class="math inline">\(\varepsilon\)</span>, <span class="math inline">\(MinPts\)</span>, if there is a chain of objects <span class="math inline">\(p_1, \dots, p_n\)</span>, such that <span class="math inline">\(p_1 = q, p_n = p\)</span> such that <span class="math inline">\(p_i+1\)</span> is directly density-reachable from <span class="math inline">\(p_i\)</span>.</p>
            <p><img src="../media/image405.png" /></p></li>
            <li><p>ex 3:</p>
            <p>q is density-reachable from p because q is directly density-reachable from m and m is directly density-reachable from p.</p>
            <p>p is not density-reachable from q because q is not a core object.</p>
            <p>An object p is <strong>density-connected</strong> to an object q w.r.t. <span class="math inline">\(\varepsilon\)</span>, MinPts if there is an object o such that both p and q are density-reachable from o (w.r.t. <span class="math inline">\(\varepsilon\)</span> and MinPts).</p>
            <p><img src="../media/image406.png" /></p></li>
            <li><p>ex 4:</p>
            <p>p, q and m are all density-connected, because we have m for which q is density reachable and also p is density reachable.</p>
            <p>We want to determine cluster characterized by having objects inside density-connected.</p>
            <p>Object q is (indirectly) density-reachable from p because q is directly density reachable from m and m is directly density-reachable from p. However, p is not density reachable from q because q is not a core object. Similarly, r and s are density-reachable from o and o is density- reachable from r. Thus, o, r, and s are all density-connected.</p>
            <p><img src="../media/image407.png" /></p></li>
            </ul>
            <h3 data-number="6.7.1" id="dbscan"><span class="header-section-number">6.7.1</span> DBSCAN</h3>
            <p><strong>Density-Based Spatial Clustering of Applications with Noise</strong> relies on a density-based notion of cluster: A cluster is defined as a maximal set of density-connected points.</p>
            <p>We determine the cluster once we fix <span class="math inline">\(\varepsilon\)</span> and MinPts using this definition.</p>
            <p>The final cluster is just to have clusters of density connected points.</p>
            <p>This algorithm is able to discover clusters of arbitrary shape in spatial databases with noise.</p>
            <p><img src="../media/image408.png" /></p>
            <p>They can determine outliers, objects that do not belong to any cluster.</p>
            <p>We have objects classified in</p>
            <ul>
            <li><p>core objects</p></li>
            <li><p>outliers</p></li>
            <li><p>border objects (objects that belong to a cluster but aren’t core objects).</p></li>
            </ul>
            <p>We have them when they belong to a neighborhood of a core object without being a core object. It determines the border of the cluster we are determining.</p>
            <p>It searches for clusters by checking the <span class="math inline">\(\varepsilon\)</span>-neighborhood of each object in the database.</p>
            <p>If the <span class="math inline">\(\varepsilon\)</span>-neighborhood of an object <span class="math inline">\(p\)</span> contains more than <span class="math inline">\(MinPts\)</span>, a new cluster with a core object is created.</p>
            <p><code>DBSCAN</code> iteratively directly collects density reachable objects from these core objects: this may involve the merge of a few density-reachable clusters.</p>
            <p>The process terminated when no new point can be added to any cluster.</p>
            <p><code>DBSCAN</code> adopts the closure of density-connectedness to find connected dense regions as clusters. Each closed set is a density-based cluster.</p>
            <p>A subset <span class="math inline">\(C\)</span> in <span class="math inline">\(D\)</span> is a cluster if</p>
            <ul>
            <li><p>For any two objects <span class="math inline">\(o_1, o_2\)</span> in <span class="math inline">\(C\)</span>, <span class="math inline">\(o_1\)</span> and <span class="math inline">\(o_2\)</span> are density-connected,</p></li>
            <li><p>There does not exist an object <span class="math inline">\(o_i\)</span> in <span class="math inline">\(C\)</span> and another object <span class="math inline">\(o_j\)</span> in <span class="math inline">\((D-C)\)</span> such that <span class="math inline">\(o_i\)</span> and <span class="math inline">\(o_j\)</span> are density-connected</p></li>
            </ul>
            <p>The complexity is <span class="math inline">\(O(n^2)\)</span>.</p>
            <h4 class="unnumbered" data-number="" id="algoritm">Algoritm</h4>
            <p><img src="../media/image409.png" /></p>
            <p><img src="../media/image410.png" /></p>
            <p>We have to fix those two parameters because we are proposing our definition of dense region and depending on them, we can identify an object as outlier or not.</p>
            <p>If <span class="math inline">\(\varepsilon\)</span> is smaller, more points will be considered as outliers.</p>
            <blockquote>
            <p>The problem is to find the optimal value for <span class="math inline">\(\varepsilon\)</span> and <code>minPts</code>.</p>
            </blockquote>
            <blockquote>
            <p>If it doesn’t contain at least MinPts objects we mark it as outlier, but it can become a border object of an object. The fact that we denote it as noise do not mean it is an outlier surely.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="clarans-vs-dbscan">CLARANS vs DBSCAN</h4>
            <p><img src="../media/image411.png" /></p>
            <p>Since the shape is spherical, for how this algorithm works we obtain those clusters, the clusters found are not really natural because when we fix the number of clusters, we cannot tell the algorithm that one is big and others small. If we fix 4 we obtain those results because clusters are spherical. The distance fix the shape of the clusters.</p>
            <p>In the third case outliers are also included.</p>
            <p>With DBSCAN we follow dense regions, and if we fix the right <span class="math inline">\(\varepsilon\)</span> and minPts we can identify correctly these clusters. Here the density is important.</p>
            <p>We are also able to find outliers in the last example.</p>
            <p>It works well here because we have that density of clusters are similar to each other.</p>
            <p>If we have clusters of different densities, we are not able to find them, because if we set <span class="math inline">\(\varepsilon\)</span> and minPts they may be good for a particular density.</p>
            <p>It works very well if clusters have same densities.</p>
            <h4 class="unnumbered" data-number="" id="cluster-with-different-densities">CLUSTER WITH DIFFERENT DENSITIES</h4>
            <p><img src="../media/image412.png" /></p>
            <p>When we use <code>DBSCAN</code>, if we set minPts and we vary <span class="math inline">\(\varepsilon\)</span>:</p>
            <p><img src="../media/image413.png" /></p>
            <p>Using large values, we determine clusters characterized by low level of density, we consider very dense small clusters as part of the big cluster.</p>
            <p>If <span class="math inline">\(\varepsilon\)</span> is small we are able to identify dense clusters but in clusters with sparse densities we have all outliers now.</p>
            <p>If we have similar densities, it is also difficult to find the exact pair of values.</p>
            <blockquote>
            <p>The algorithm is really sensitive on how we set the parameters.</p>
            </blockquote>
            <p><img src="../media/image414.png" /></p>
            <p>In general cases, when we have high-dimensional data, we have no visual feedback, we have to experiment.</p>
            <p>Using <span class="math inline">\(\varepsilon\)</span>=0.5 and <span class="math inline">\(\varepsilon\)</span>=0.4 we have a big difference.</p>
            <p>By decreasing <span class="math inline">\(\varepsilon\)</span> of 0.1 we have that big clusters, characterized by not very dense regions, allow us to find a lot of clusters instead of just one.</p>
            <p>In the second example we have just one cluster but depending on the value of parameters we can have different situations.</p>
            <p>In the first case we determine one cluster because the connection between the two square is considered as dense region and the points there are considered density connected.</p>
            <p>A cluster is separated later because we are reducing minPts.</p>
            <p>We have to try to set them using an heuristic approach or with an input of the internal domain if we have a lot of dimensions.</p>
            <h4 class="unnumbered" data-number="" id="heuristic-approach-to-set-varepsilon-and-minpts">heuristic approach to set <span class="math inline">\(\varepsilon\)</span> and <code>minPts</code></h4>
            <p>For a given <span class="math inline">\(k\)</span> (that corresponds to minPts) we define a function k-dist, mapping each point to the distance from its k-th nearest neighbor.</p>
            <p><img src="../media/image415.png" /></p>
            <p>If we set <span class="math inline">\(k = 4\)</span>, the k-distance corresponds to the distance between the point we are considering and the fourth nearest neighbor. Lower this distance, higher the probability that the points will belong to a dense region.</p>
            <p>We sort the points in descending order of their k-dist values: the graph of this function gives some hints concerning the density distribution.</p>
            <p><img src="../media/image416.png" /></p>
            <p>If I set the distance of the kth nearest neighbor to <span class="math inline">\(\varepsilon\)</span> I obtain that this point is a core point, we have <span class="math inline">\(k\)</span> minPts inside the <span class="math inline">\(\varepsilon\)</span> neighborhood.</p>
            <p>The points in the top have a high 4-dist, which means that they have the 4th nearest neighbors quite far and they’re probably not in a dense region.</p>
            <p>Outliers, in fact, probably have a large k-dist.</p>
            <p>We have also a large number of points belonging to dense regions (the k-nearest neighbors are quite close).</p>
            <p>If we choose an arbitrary point p, set the parameter <span class="math inline">\(\varepsilon\)</span> to k-dist(p) and set the parameter MinPts to k, all points with an equal or smaller k-dist value will be core points.</p>
            <p>If we choose the value indicated by the arrow and we are sure all points in the right will be core points, while other in the left can be outliers or borders.</p>
            <p>If we set k-dist to that point all points in the right will be core point.</p>
            <p>If we choose a certain <span class="math inline">\(\varepsilon\)</span>, we will have few outliers in the left, not all of them.</p>
            <p>The threshold point is the first point in the first “valley” of the sorted k-dist graph.</p>
            <p>This is heuristic but it is a way to set <span class="math inline">\(\varepsilon\)</span> and minPts.</p>
            <p>Changing very few <span class="math inline">\(\varepsilon\)</span> we can have completely different results in terms of clusters.</p>
            <blockquote>
            <p>We have the problem that the intrinsic cluster structure cannot be characterized by global density parameters. It affects DBSCAN but also all density-based algorithms.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="alternatives-for-determine-the-cluster-structures">Alternatives for determine the cluster structures</h4>
            <p><img src="../media/image417.png" /></p>
            <ul>
            <li><p>The first alternative we can use is to use a <strong>hierarchical clustering algorithm</strong>, for instance the single-link method.</p>
            <p>Drawbacks:</p>
            <ul>
            <li><p>single-link effect, i.e. clusters which are connected by a line of few points having a small inter-object distance are not separated. We can put together clusters linked by this line.</p></li>
            <li><p>the results, i.e. the dendrograms, are hard to understand for more than a few hundred objects. With an high number of objects is hard to understand the final result.</p></li>
            </ul></li>
            <li><p>The second alternative is to use a <strong>density-based partitioning algorithm with different parameter settings</strong>.</p>
            <p>Drawbacks:</p>
            <ul>
            <li><p>there are an infinite number of possible parameter values.</p></li>
            <li><p>Even if we use a very large number of different values - which requires a lot of secondary memory to store the different cluster memberships for each point - it is not obvious how to analyze the results (we don’t have groung-truth) and we may still miss the interesting clustering levels.</p></li>
            </ul></li>
            </ul>
            <p>The <strong>solution</strong> is to run <strong>an algorithm which produces a special order of the database</strong> with respect to its density-based clustering structure containing the information about every clustering level of the data set.</p>
            <p>If we are able to produce this ordering, we can apply DBSCAN with different densities with specific parts of the space.</p>
            <p>The idea is to adopt an ordering algorithm that orders object respect to their density area.</p>
            <p>Determining the parameters of DBSCAN is quite hard. If we have a lot of dimensions we don’t have a visual feedback.</p>
            <p>When we set two parameters, also, we set them for all the space. We enforce the same definition of density for all the space, but in some parts we can have dense regions and in others we don’t.</p>
            <p>If we choose the pair in such a way to choose dense regions, points in which we don’t have dense zones are considered outliers.</p>
            <p>If we set them to identify not dense clusters, all points will belong to the same class at the end.</p>
            <p>We saw an approach to plot the k-dist on the values,</p>
            <h3 data-number="6.7.2" id="optics-ordering-points-to-identify-the-clustering-structure"><span class="header-section-number">6.7.2</span> OPTICS: Ordering Points To Identify the Clustering Structure</h3>
            <p>It will give us <strong>an idea to work with different regions</strong> with <strong>different densities</strong> and allow us to set parameters to investigate this specific zone.</p>
            <p>We won’t have clusters after the execution, we will have an ordering of points that allow us to know the combination of values to obtain good clusters.</p>
            <p>This cluster-ordering contains info equivalent to the density-based clustering corresponding to a broad range of parameter settings.</p>
            <p>It is good for both automatic and interactive cluster analysis, including finding intrinsic.</p>
            <p>The result can be represented graphically or using visualization techniques.</p>
            <p>We start with this observation: density-based clusters are monotonic with respect to the neighborhood threshold.</p>
            <p><strong>DBSCAN</strong>: for a constant <code>MinPts</code> value, density-based clusters with respect to a higher density are completely contained in density-connected sets obtained with respect to a lower density.</p>
            <h4 class="unnumbered" data-number="" id="example-8">Example</h4>
            <p><span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> are density-based clusters with respect to <span class="math inline">\(Eps_2 \leq eps_1\)</span> and <span class="math inline">\(C\)</span> is a density-based cluster with respect to <span class="math inline">\(Eps_1\)</span> completely containing the sets <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>.</p>
            <p><img src="../media/image418.png" /></p>
            <p><span class="math inline">\(Eps_1 \leq Eps_2\)</span> and that means that when we find <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> we are imposing the higher density. It tells us that clusters with lower density contains clusters with higher density. <span class="math inline">\(C\)</span> contains <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span>.</p>
            <p>The idea for <code>OPTICS</code> is to process a set of distance parameter values at the same time. In practice, to use <code>DBSCAN</code> for an infinite number of distance parameters <span class="math inline">\(Eps_i\)</span> which are smaller than a “generating distance <span class="math inline">\(\varepsilon\)</span>”.</p>
            <p>Unlike <code>DBSCAN</code>, <code>OPTICS</code> does not assign cluster memberships.</p>
            <p>Instead, it stores the order in which the objects are processed and the information which would be used by an extended <code>DBSCAN</code> algorithm to assign cluster memberships. This information consists of only two values for each object: - the core-distance - a reachability-distance.</p>
            <p>We don’t obtain at the end a partition but an ordering.</p>
            <h4 class="unnumbered" data-number="" id="definitions-1">Definitions</h4>
            <ul>
            <li><p><strong>Core-distance of an object p</strong> is the smallest <span class="math inline">\(\varepsilon\)</span> value that makes <span class="math inline">\(p\)</span> a core object. If <span class="math inline">\(p\)</span> is not a core object, the core-distance is undefined.</p></li>
            <li><p><strong>Reachability-distance of an object q from p</strong> is the minimum radius value that makes <span class="math inline">\(q\)</span> directly density-reachable from <span class="math inline">\(p\)</span>.</p></li>
            </ul>
            <blockquote>
            <p><span class="math inline">\(p\)</span> has to be a core object and <span class="math inline">\(q\)</span> must be in the neighborhood of <span class="math inline">\(p\)</span>. Therefore, the reachability-distance from <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span> is max (core distance(p), dist(p, q)).</p>
            </blockquote>
            <blockquote>
            <p>If <span class="math inline">\(p\)</span> is not a core object, the reachability-distance is undefined.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="example-9">Example</h4>
            <p><img src="../media/image419.png" /></p>
            <p>Here, the core distance of <span class="math inline">\(p\)</span> is the distance for which we include in the neighborhood the minPts that we need for the execution of DBSCAN.</p>
            <ul>
            <li><p>The reachability-distance <span class="math inline">\((p,q_1)\)</span> is <span class="math inline">\(\varepsilon^{&#39;}\)</span>, because <span class="math inline">\(q_1\)</span> is lower than the core distance.</p></li>
            <li><p>The reachability-distance <span class="math inline">\((p,q_2)\)</span> is really the distance because the core distance is lower.</p></li>
            </ul>
            <p>We use the second definition for the ordering.</p>
            <p>To construct the different partitions simultaneously, the objects have to be processed in a specific order:</p>
            <ul>
            <li><p><code>OPTICS</code> begins with an arbitrary object from the input database as the current object, <span class="math inline">\(p\)</span>. It retrieves the <span class="math inline">\(\varepsilon\)</span>-neighborhood of <span class="math inline">\(p\)</span>, determines the core-distance, and sets the reachability-distance to undefined.</p></li>
            <li><p>If <span class="math inline">\(p\)</span> is not a core object, <code>OPTICS</code> simply moves on to the next object in the <code>OrderSeeds</code> list.</p></li>
            <li><p>If <span class="math inline">\(p\)</span> is a core object, then for each object <span class="math inline">\(q\)</span> in the <span class="math inline">\(\varepsilon\)</span>-neighborhood of <span class="math inline">\(p\)</span>, <code>OPTICS</code> updates its reachability-distance from <span class="math inline">\(p\)</span> and inserts <span class="math inline">\(q\)</span> into <code>OrderSeeds</code> if q has not yet been processed.</p></li>
            </ul>
            <p>his <span class="math inline">\(\varepsilon\)</span>-neighborhood is not the <span class="math inline">\(\varepsilon\)</span> we use in the <code>DBSCAN</code>, but it’s just the larger distance that leads the search for points from the <code>OPTICS</code>.</p>
            <p>The objects contained in <code>OrderSeeds</code> are sorted by their reachability-distance to the closest core object from which they have been directly density reachable. In each step of the WHILE- loop, an object currentObject having the smallest reachability-distance in the seed-list is selected by the method <code>OrderSeeds:next()</code>.</p>
            <ul>
            <li><p>The <span class="math inline">\(\varepsilon\)</span>-neighborhood of this object and its core-distance are determined. Then, the object is simply written to the file OrderedFile with its core distance and its current reachability- distance.</p></li>
            <li><p>The iteration continues until the input is fully consumed and <code>OrderSeeds</code> is empty.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="conclusions-3">Conclusions</h4>
            <p>At the end of the execution of <code>OPTICS</code>, we have the list of objects in the order in which the objects themselves are processed.</p>
            <p>We have an ordering based on the reachability distance. These objects are processed considering the smallest reachability distance from the point we are considering and the next point.</p>
            <p>Complexity: in the worst case, <span class="math inline">\(O(n^2)\)</span>, where <span class="math inline">\(n\)</span> is the number of objects to be clustered.</p>
            <p>We have an ordered list, and we have an order of the points and we can plot the reachability distance. The <span class="math inline">\(\varepsilon\)</span> we see there is the <span class="math inline">\(\varepsilon\)</span> we use in <code>OPTICS</code> and limits the number of points we have to consider.</p>
            <p><img src="../media/image420.png" /></p>
            <p>After the execution of <code>OPTICS</code> I have this profile:</p>
            <ul>
            <li><p>Objects in the x-axis selected in such a way that close object are really close in terms of reachability distance, because when we select the next object we select the one with the smallest reachability distance with respect to t he current object. Considering two consecutive points they’re the closest in terms of reachability distance.</p></li>
            <li><p>If I consider two consecutive points, we are considering closest points in terms of reachability distance.</p></li>
            <li><p>The valley corresponds to the number of points really close to each other, because characterized by having the smallest reachability distance respect to all other points.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="usage-2">Usage</h4>
            <p>This plot can be useful to select <span class="math inline">\(\varepsilon\)</span> and <code>minPts</code> for <code>DBSCAN</code> or other density-based algorithms.</p>
            <p>Here <code>minPts</code> is fixed, when we defined the reachability distance, we start from the definition of core distance and to do that we have to fix <code>minPts</code>.</p>
            <p>This profile tells us what the reachability distance is. If we fix <span class="math inline">\(\varepsilon^{&#39;}\)</span> and use it for the <code>DBSCAN</code> we can see that with this, we can identify the clusters corresponding to valleys.</p>
            <p>The reachability distance for points in the valley are lower than the threshold <span class="math inline">\(\varepsilon^{&#39;}\)</span>.</p>
            <h4 class="unnumbered" data-number="" id="examples-of-usage">Examples of usage</h4>
            <ul>
            <li><p>ex 1</p>
            <p><img src="../media/image421.png" /></p>
            <p><code>OPTICS</code> produces the type of plot for that dataset. The deepest valley corresponds to very dense regions in our dataset.</p>
            <p>With that plot we can also decide different values for <span class="math inline">\(\varepsilon\)</span> and minPts depending on specific parts of the space.</p></li>
            <li><p>ex 2</p>
            <p><img src="../media/image422.png" /></p>
            <p>By analyzing the graph, we can understand if we have natural clusters in the dataset and their level of density.</p>
            <p>In very dense region like the cluster on the top, the valley is really deep. In the bottom we have not very dense cluster, we have a valley with the bottom level higher than other valleys.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="example-10">Example</h4>
            <p>On a 2 dimensional database with 16 points, starting with:</p>
            <p><span class="math display">\[
                \varepsilon = 44, \quad MinPts = 3
            \]</span>)</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image425.png" /></td>
            <td><img src="../media/image426.png" /></td>
            </tr>
            </tbody>
            </table>
            <ol type="1">
            <li><p>OPTICS tells us to select one object randomly, let’s select <code>A</code>.</p>
            <p>It has three points and in the seed list we add points in the neighborhood.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image427.png" /></td>
            <td><img src="../media/image428.png" /></td>
            </tr>
            </tbody>
            </table>
            <p><img src="../media/image429.png" /></p></li>
            <li><p>Now we consider <code>B</code>, and for <code>B</code> in its core distance we just have <code>C</code> with reachability distance <span class="math inline">\(40\)</span>.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image430.png" /></td>
            <td><img src="../media/image431.png" /></td>
            </tr>
            </tbody>
            </table>
            <p><img src="../media/image432.png" /></p>
            <p>We include <code>I</code> because we just select the points with the minimum reachability distance respect to the current object, <code>B</code>, and we just choose <code>I</code>.</p></li>
            <li><p>We consider <code>I</code> as current object and we determine all objects in the neighborhood.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image433.png" /></td>
            <td><img src="../media/image434.png" /></td>
            </tr>
            </tbody>
            </table>
            <p><img src="../media/image435.png" /></p>
            <p>When we consider as current object <code>I</code> we add in the seed-list these objects.</p></li>
            <li><p>We consider the object in the seed List with the minimum reachability distance, J, which becomes current.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!-- --></th>
            <th><!-- --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image436.png" /></td>
            <td><img src="../media/image437.png" /></td>
            </tr>
            </tbody>
            </table>
            <p><img src="../media/image438.png" /></p></li>
            <li><p>We then select L.</p>
            <p><img src="../media/image439.png" /></p></li>
            <li><p>At the end we have this situation:</p>
            <p><img src="../media/image440.png" /></p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="conclusion">Conclusion</h4>
            <p>If we analyze this profile, we can realize that some points are in a valley.</p>
            <p><img src="../media/image441.png" /></p>
            <p>If we cut choosing <span class="math inline">\(\varepsilon=44\)</span> we obtain two clusters.</p>
            <p>If we choose it higher we obtain just one cluster.</p>
            <h3 data-number="6.7.3" id="denclue-using-statistical-density-functions"><span class="header-section-number">6.7.3</span> DENCLUE: Using Statistical Density Functions</h3>
            <p>This is another density-based algorithm quite different from the others.</p>
            <p>The problem of DBSCAN and OPTICS is that <strong>density</strong> is calculated by <strong>counting the number of objects</strong> in a neighborhood defined by <span class="math inline">\(\varepsilon\)</span>. Such density estimates can be highly sensitive to the radius value used.</p>
            <p>A clustering method based on a set of density distribution functions is the solution provided by DENCLUE.</p>
            <p>Each observed object is treated as an indicator of high-probability density in the surrounding region. The probability density at a point depends on the distances from this point to the observed objects.</p>
            <p>If we have this dataset:</p>
            <p><img src="../media/image442.png" /></p>
            <p><code>DENCLUE</code> propose to assign to each object a function, typically a gaussian one, with center of the gaussian corresponding to the point.</p>
            <p>If we have points close to each other, each gaussian start to be overlapped. The sum of overlapping gaussians we obtain produces mountains. Higher the mountains and higher the probability to have dense regions.</p>
            <p>In far points we use the gaussian profile, while in dense regions the sum of these accumulates gaussians.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image443.png" /></td>
            <td><img src="../media/image444.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>We have to decide:</p>
            <ul>
            <li><p>how to cut hills to determine clusters, we need to set a parameter.</p></li>
            <li><p>how much the gaussian function is wide.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="example-11">Example</h4>
            <p>Let <span class="math inline">\(&lt;x_1, \dots, x_n\)</span> be an independent and identically distributed sample of a random variable <span class="math inline">\(f\)</span>. The kernel density approximation of the probability density function is:</p>
            <p><span class="math display">\[
                \hat{f_h} = \frac{1}{nh}\sum_{i=1}^n{K\frac{(x-x_i)}{h}}
            \]</span></p>
            <p>Kernel K() is a non-negative real-valued integrable function that should satisfy two requirements for all values of <span class="math inline">\(u\)</span>:</p>
            <p><span class="math display">\[
                \int_{-\infty}^{+\infty} K(u) du = 1 \text{ and } K(-u) = K(u)
            \]</span></p>
            <p>We have different types of kernel function, an example of kernel function is the gaussian function:</p>
            <p><span class="math display">\[
                K(\frac{x-x_i}{h}) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-x_i)^2}{2h^2}}
            \]</span></p>
            <p><span class="math inline">\(h\)</span> is the standard deviation and <span class="math inline">\(x_i\)</span> is the point we are considering where we are putting the kernel function.</p>
            <p>These are two examples of kernel functions:</p>
            <p><img src="../media/image448.png" /> <img src="../media/image449.png" /></p>
            <p>If we use squares, hills are not smoothing but dolomites,</p>
            <p>Clusters can be determined mathematically by identifying density attractors. We have to find the peaks of the mountains and are attractor for all points around the peak.</p>
            <p>To avoid trivial local maximum points, <code>DENCLUE</code> uses a noise threshold <span class="math inline">\(\xi\)</span> and only considers those density attractors <span class="math inline">\(x^*\)</span> such that:</p>
            <p><span class="math display">\[
                \hat{f}(x^*) \geq \xi
            \]</span></p>
            <p>We introduce it because we select as attractor also the peak of a gaussian for example, and if we don’t want it as local maxima we can exploit it.</p>
            <p>These attractors are the centers of clusters.</p>
            <p>Objects are assigned to clusters through density attractors using a step-wise hill-climbing procedure.</p>
            <p>If we consider a profile, we can realize that we have to perform an hill-climbing.</p>
            <h4 class="unnumbered" data-number="" id="algorithm-3">Algorithm</h4>
            <ul>
            <li><p>Until there exist samples in the data set,</p></li>
            <li><p>Select a sample <span class="math inline">\(x\)</span>. The density attractor for <span class="math inline">\(x\)</span> is computed by using the hill-climbing procedure.</p>
            <p><span class="math display">\[
                  x^0 = x \qquad x^{j+1} = x^j + \delta \frac{\nabla \hat{f}(x^j)}{|\nabla \hat{f}(x^j)|}
              \]</span></p></li>
            <li><p>We add to <span class="math inline">\(x_i\)</span> a delta multiplicated by the gradient,</p>
            <ul>
            <li><p>where <span class="math inline">\(\delta\)</span> is a parameter to control the speed of convergence and <span class="math inline">\(\delta\)</span> rules the speed for which we climb the speed. If it is too large, we have the risk to not identify the peak, jumping over it. If it is too small we need to spend a lot of time to identify the hill.</p></li>
            <li><p>and the gradient is</p>
            <p><span class="math display">\[
                  \nabla \hat{f}(x) = \frac{1}{h^{d+2}\sum_{i=1}^n{K\frac{x-x_i}{h}(x-x_i)}}
              \]</span></p>
            <p>where <span class="math inline">\(d\)</span>=space dimension and <span class="math inline">\(K\)</span> is a Gaussian.</p></li>
            </ul></li>
            <li><p>The hill-climbing procedure stops at step <span class="math inline">\(k&gt;0\)</span> if <span class="math inline">\(\hat{f}(x^{k+1}) &lt; \hat{f}(x^{k}\)</span> and assigns <span class="math inline">\(x\)</span> to the density attractor</p>
            <p><span class="math display">\[
                  x^* = x^k
              \]</span></p></li>
            </ul>
            <p>Each time we calculate <span class="math inline">\(x\)</span> we will have a higher of function higher than the previous value, because we are climbing the hill. We stop when we find a lower value.</p>
            <p>We have to find all possible attractors; we could start for each point in the dataset and try to apply the hill-climbing of procedure.</p>
            <p>But some groups of objects are attracted by the same attractor. If we start from any of those points, we find the same attractors.</p>
            <p>Instead of repeating the algorithm for each object, for efficiency reasons, the algorithm stores all points <span class="math inline">\(x^{&#39;}\)</span> with <span class="math inline">\(d(x_j, x^{&#39;}) \leq = h/2\)</span> for any step <span class="math inline">\(0 &lt; j &lt; k\)</span> during the hill-climbing procedure and attaches these points to the cluster of <span class="math inline">\(x^*\)</span> as well.</p>
            <p>We select <span class="math inline">\(x\)</span> and we consider if around we have other points, if we have them we consider those attracted by the attractor we are finding.</p>
            <p>Using these heuristics, all points which are located close to the path from <span class="math inline">\(x\)</span> to its density-attractor can be classified without applying the hill-climbing procedure to them.</p>
            <h4 class="unnumbered" data-number="" id="outliers-1">Outliers</h4>
            <p>DANCLUE is also able to determine outliers.</p>
            <p>An object x is an outlier or noise if it converges in the hill- climbing procedure to a local maximum <span class="math inline">\(x^*\)</span> with</p>
            <p><span class="math display">\[
                \hat{f}(x^*) &lt; \xi
            \]</span></p>
            <p>Probably single points will be considered outliers because the value of <span class="math inline">\(f\)</span> may be lower than <span class="math inline">\(\xi\)</span>.</p>
            <h4 class="unnumbered" data-number="" id="example-12">Example</h4>
            <p>Density attractors in a one-dimensional space</p>
            <p><img src="../media/image458.png" /></p>
            <p>When we sum we have these kinds of profile. We have some hills.</p>
            <p>Given attractors, that will be center of clusters, we must understand if we can merge clusters belonging to different attractors.</p>
            <p>We determined the attractors and to determine cluster we merge density attractors that are connected through paths of high density (<span class="math inline">\(&gt; threshold\)</span>).</p>
            <p>An arbitrary-shape cluster (with respect to two constants h and <span class="math inline">\(\xi\)</span>) for the set of density attractors <span class="math inline">\(X\)</span> is a subset <span class="math inline">\(C \subseteq D\)</span>, where: $$ x C   x^* X  |  (x^*) </p>
            <p>$$</p>
            <p><span class="math display">\[
                \forall x_1^*, x_2^* \in X \ \exists \text{ a path } P \in F^d \text{ from } x_1^* \text{ to } x_2^* \text{ with }  \hat{f}(p) \geq \xi \ \forall p \in P  
            \]</span></p>
            <p>We pickup two attractors and follow <span class="math inline">\(f\)</span> in such a value we have always the value of <span class="math inline">\(f\)</span> higher than <span class="math inline">\(\xi\)</span>.</p>
            <h4 class="unnumbered" data-number="" id="parameters">Parameters</h4>
            <p>We have <span class="math inline">\(h\)</span> and <span class="math inline">\(\xi\)</span> as parameters:</p>
            <p><img src="../media/image462.png" /></p>
            <ul>
            <li><p><span class="math inline">\(h\)</span> is the standar deviation of the gaussian function and depending on it the gaussian will be wide or narrow.</p>
            <p>The wideness of the gaussian impact the number of points that we include when we apply the hill-climbing in determining the attractors.</p>
            <p>h = 0.2 means a narrow kernel function, we have a number of peaks because we can’t attract points in wide regions. With the increase of h this phenomenon decreases, we attract an higher number of points.</p></li>
            <li><p><span class="math inline">\(\xi\)</span> fix the cut point of our hills, and depending on it we can have different clusters because if we select a high <span class="math inline">\(\xi\)</span> we select only points close to the peak.</p>
            <p>When we select lower value of <span class="math inline">\(\xi\)</span>, we just consider higher number of points included in clusters.</p>
            <p>Increasing the value of <span class="math inline">\(\xi\)</span> we cannot easily connect different attractors.</p></li>
            </ul>
            <p>We have a strong dependence in the two parameters in any case.</p>
            <blockquote>
            <p>Fixing these two values we determine the number of clusters we obtain and the possible outliers.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="major-features-1">Major features</h4>
            <ul>
            <li><p>Solid mathematical foundation</p></li>
            <li><p>Good for data sets with large amounts of noise</p></li>
            <li><p>Allows a compact mathematical description of arbitrarily shaped clusters in high-dimensional data sets</p></li>
            <li><p>Significant faster than existing algorithm (e.g., faster than DBSCAN by a factor of up to 45)</p></li>
            <li><p>Complexity (with some optimization) <span class="math inline">\(O(N \log(N))\)</span></p></li>
            </ul>
            <p>But needs an accurate choice of the parameters h and <span class="math inline">\(\xi\)</span>.</p>
            <ul>
            <li><p><span class="math inline">\(h\)</span> determines the influence of a point in its neighborhood and implicitly how many points relate to a specific attractor</p></li>
            <li><p><span class="math inline">\(\xi\)</span> describes whether a density-attractor is significant, allowing a reduction of the number of density-attractors and helping to improve how the parameters should be chosen to obtain good results</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="how-to-find-the-parameters">How to find the parameters</h4>
            <p>To determine the value of <span class="math inline">\(h\)</span> we consider different <span class="math inline">\(h\)</span> and determine the largest interval between hmax and hmin where the number of density attractors <span class="math inline">\(m(h)\)</span> remains constant.</p>
            <p>We use increasing value of <span class="math inline">\(h\)</span> and we select <span class="math inline">\(h\)</span> that is in the interval where the number of density attractors remain constant. This is the suggested choice for <span class="math inline">\(h\)</span>.</p>
            <p><img src="../media/image463.png" /></p>
            <p>Suggested choice for <span class="math inline">\(\xi\)</span>: if the database is noise free, all density attractors of <span class="math inline">\(D\)</span> are significant and <span class="math inline">\(\xi\)</span> should be chosen in</p>
            <p><span class="math display">\[
                0 \leq \xi \leq \min_{x^* \in X}{\{f^{D_C}(x^*)\}}
            \]</span></p>
            <p>If the database has noise as attractors we identify single objects.</p>
            <p>We have a possible optimization to speed-up the execution.</p>
            <h4 class="unnumbered" data-number="" id="denclue-implementation">DENCLUE implementation</h4>
            <p>Two steps:</p>
            <ol type="1">
            <li><p>Step</p>
            <p>Initial pre-clustering based on a grid to speed up the calculation of the density function</p>
            <ul>
            <li><p>We introduce the minimal bounding (hyper-)rectangle of the data set, and we divide it into d-dimensional hypercubes, with an edge length of 2h. We create a grid in our dataset.</p></li>
            <li><p>Only hypercubes which actually contain data points are determined. We reduce the exploration of the space focusing in hypercubes containing objects.</p></li>
            <li><p>The hypercubes are numbered depending on their relative position from a given origin. The keys of the populated cubes can be efficiently stored in a randomized search-tree or a B+ tree.</p></li>
            </ul>
            <p><img src="../media/image465.png" /></p>
            <p>Instead of considering all hypercubes we focus on the ones in which we have objects.</p>
            <ul>
            <li><p>For each populated cube c, in addition to the key, the number of points (Nc) which belong to c, pointers to those points, and the linear sum <span class="math inline">\(\sum_{x \in C} x\)</span> are stored.</p></li>
            <li><p>This information is used in the clustering step for a fast calculation of the mean of a cube (<span class="math inline">\(mean(c)\)</span>). Since clusters can spread over more than one cube, neighboring cubes which are also populated have to be accessed.</p></li>
            <li><p>To speed up this access, we connect neighboring populated cubes.</p>
            <p>More formally, two cubes <span class="math inline">\(c_1, c_2 \in C_p\)</span> are connected if <span class="math inline">\(d(mean(c_1), mean(c_2)) &lt; 4 \sigma\)</span> (we use a Gaussian kernel function and therefore <span class="math inline">\(h=\sigma\)</span>).</p></li>
            </ul></li>
            <li><p>Step : the actual clustering step</p>
            <ul>
            <li>We consider only the highly populated cubes <span class="math inline">\(C_{sp}\)</span> and cubes which are connected to a highly populated cube are considered in determining clusters. We speed-up the process limiting our search to cubes highly populated and cubes connected to them</li>
            </ul>
            <p>With this implementation the computational time in terms of CPU is lower than the one of standard DENCLUE and DBSCAN.</p>
            <p><img src="../media/image466.png" /></p>
            <p>It is almost constant with the size of the database. We can find clusters of different shapes in a reasonable time.</p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="attention">Attention</h4>
            <ul>
            <li><p>All the density based clustering methods are not fully effective when clustering high dimensional data, for the curse of dimensionality and objects appear far to each other.</p></li>
            <li><p>Methods that rely on near or nearest neighbor information do not work well on high dimensional spaces.</p></li>
            <li><p>In high dimensional data sets, it is very unlikely that data points are nearer to each other than the average distance between data points because of sparsely filled space. As a result, as the dimensionality of the space increases, the difference between the distance to the nearest and the farthest neighbors of a data object goes to zero. The solution is to apply attribute selection before using these algorithms or use different algorithms.</p></li>
            <li><p>We have to set parameters and if we choose in a bad way we are in trouble, we have a lot of differences with small changes. This is true independently on the density-based method. But we don’t have to choose the number of clusters we want to obtain, even if the pairs of parameters implicitly choose this number.</p></li>
            </ul>
            <h2 data-number="6.8" id="grid-based-clustering-method"><span class="header-section-number">6.8</span> Grid-Based Clustering Method</h2>
            <p>These methods use multi-resolution grid data structure.</p>
            <p>there are several interesting methods, with the characteristic of low computational cost, even if the results we obtain are less performing than other algorithms.</p>
            <p>We will analyze:</p>
            <ul>
            <li><p><code>STING</code> (a STatistical INformation Grid approach)</p></li>
            <li><p><code>CLIQUE</code>: which is both grid-based and subspace clustering</p></li>
            </ul>
            <h3 data-number="6.8.1" id="sting"><span class="header-section-number">6.8.1</span> STING</h3>
            <p>The spatial area is divided into rectangular cells.</p>
            <p>There are several levels of cells corresponding to different levels of resolution.</p>
            <p><img src="../media/image467.png" /></p>
            <p>Each cell at a high level is partitioned into a number of smaller cells in the next lower level.</p>
            <p>Statistical info of each cell is calculated and stored beforehand and is used to answer queries</p>
            <p>Parameters of higher-level cells can be easily calculated from parameters of lower level cell</p>
            <p>Parameters include:</p>
            <ul>
            <li><p>count, mean, standard deviation, min, max</p></li>
            <li><p>type of distribution—normal, uniform, exponential or none (if the distribution is unknown) – obtained by the user or by hypothesis tests</p></li>
            </ul>
            <p>Use a top-down approach to answer spatial data queries. We have an hierarchical structure with higher level with all information corresponding to the lower level.</p>
            <p><img src="../media/image468.png" /></p>
            <p>Statistical information regarding the attributes in each grid cell, for each layer are pre-computed and stored beforehand.</p>
            <h4 class="unnumbered" data-number="" id="idea-2">Idea</h4>
            <p>The statistical parameters for the cells in the lowest layer is computed directly from the values that are present in the table, when data are loaded into the database.</p>
            <p>The statistical parameters for the cells in all the other levels are computed from their respective children’s cells that are in the lower level. We start with the lowest level corresponding to the value we have and we split it in cells, we compute the statistics for each cell and when we just go to the next level we have a mapping from the cell in higher level with respect to the cell in the lower level, so that we compute statistics for cell at higher levels.</p>
            <p>Query types:</p>
            <ul>
            <li><p>SQL like language used to describe queries</p></li>
            <li><p>Two types of common queries found: one is to find region specifying certain constraints and other take in a region and return some attribute of the region</p></li>
            </ul>
            <p>We use a top-down approach to answer spatial data queries starting from a pre-selected layer-typically with a small number of cells.</p>
            <p>The pre-selected layer does not have to be the top most layer.</p>
            <p>For each cell in the current layer compute the confidence interval reflecting the cell’s relevance to the given query. The confidence interval is calculated by using the statistical parameters of each cell.</p>
            <p>From the interval calculated we label the cells as relevant or irrelevant for this query We remove the irrelevant cells from further consideration.</p>
            <p>When finished with the current layer, proceed to the next lower level.</p>
            <p>Processing of the next lower layer examines only the remaining relevant cells.</p>
            <p>Repeat this process until the bottom layer is reached.</p>
            <p>At this time if query specifications are met, the regions of relevant cells that satisfy the query are returned.</p>
            <p>Otherwise, the data that fall into the relevant cells are retrieved and further processed until they meet the requirement of the query.</p>
            <p>We speed-up the query processing so that we can concentrate on the data relevant to the specific query. This corresponds to have clusters inside specific grids.</p>
            <p>A typical query is the following:</p>
            <div class="sourceCode" id="cb17"><pre class="sourceCode sql"><code class="sourceCode sql"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="co">-- Select the maximal regions that have at least 100 houses per unit area and at least 70% of the house prices are abobe $400k and with total area at least 100 units with 90% confidence</span></span>
            <span id="cb17-2"><a href="#cb17-2" aria-hidden="true"></a></span>
            <span id="cb17-3"><a href="#cb17-3" aria-hidden="true"></a><span class="kw">SELECT</span> region</span>
            <span id="cb17-4"><a href="#cb17-4" aria-hidden="true"></a><span class="kw">FROM</span> house<span class="op">-</span>map</span>
            <span id="cb17-5"><a href="#cb17-5" aria-hidden="true"></a><span class="kw">WHERE</span> density <span class="kw">IN</span> (<span class="dv">100</span>, ∞)</span>
            <span id="cb17-6"><a href="#cb17-6" aria-hidden="true"></a>    <span class="kw">AND</span> price <span class="kw">RANGE</span> (<span class="dv">400000</span>, ∞)</span>
            <span id="cb17-7"><a href="#cb17-7" aria-hidden="true"></a>        <span class="kw">WITH</span> <span class="kw">percent</span> (<span class="fl">0.7</span>,<span class="dv">1</span>)</span>
            <span id="cb17-8"><a href="#cb17-8" aria-hidden="true"></a>    <span class="kw">AND</span> area (<span class="dv">100</span>, ∞)</span>
            <span id="cb17-9"><a href="#cb17-9" aria-hidden="true"></a>    <span class="kw">AND</span> <span class="kw">WITH</span> confidence <span class="op">=</span> <span class="fl">0.9</span></span></code></pre></div>
            <p>We can apply the query exploiting the previous approach that allow us to consider only cells relevant to each level.</p>
            <p><img src="../media/image470.png" /></p>
            <p>At the beginning we isolate some area, then we redefine are at the second level and then at the third level.</p>
            <p>From a region we explore the subsequent level and so on.</p>
            <p>At the end it’s like we determined a cluster putting together cells close to each other, and so it’s considered a clustering method.</p>
            <h4 class="unnumbered" data-number="" id="advantages">Advantages</h4>
            <ul>
            <li><p>Query-independent, easy to parallelize, incremental update</p></li>
            <li><p>Generation of the clusters complexity <span class="math inline">\(O(n)\)</span></p></li>
            <li><p>Query processing time <span class="math inline">\(O(g)\)</span>, where <span class="math inline">\(g\)</span> is the number of grid cells at the lowest level</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="disadvantages">Disadvantages</h4>
            <ul>
            <li>All the cluster boundaries are either horizontal or vertical, and no diagonal boundary is detected, because we build our hierarchy in this way</li>
            </ul>
            <h4 class="unnumbered" data-number="" id="considerations-6">Considerations</h4>
            <p>The regions returned by <code>STING</code> are an approximation of the result by <code>DBSCAN</code>. As the granularity approaches zero, the regions returned by <code>STING</code> approach the result of <code>DBSCAN</code></p>
            <p><img src="../media/image471.png" /></p>
            <h3 data-number="6.8.2" id="clique-clustering-in-quest"><span class="header-section-number">6.8.2</span> CLIQUE (Clustering In QUEst)</h3>
            <p><code>CLIQUE</code> automatically identify subspaces of a high dimensional data space that allow better clustering than original space.</p>
            <p><code>CLIQUE</code> can be considered as both density-based and grid-based.</p>
            <p>It is density based because we try to identify clusters basing on idea of density but density based too because we work with grid.</p>
            <p>It partitions each dimension into the same number of equal length intervals.</p>
            <p>Then it partitions an m-dimensional data space into non-overlapping rectangular units.</p>
            <p>A unit is dense if the fraction of total data points contained in the unit exceeds the input model parameter.</p>
            <h4 class="unnumbered" data-number="" id="idea-3">Idea</h4>
            <p>We generate a number of cells and consider they’re dense computing the number of data points we have inside the unit.</p>
            <p><img src="../media/image472.png" /></p>
            <p>The parameter we have here is present and allow us to determine density.</p>
            <p>A cluster is a maximal set of connected dense units within a subspace.</p>
            <p><img src="../media/image473.png" /></p>
            <p>We look for connection between dense units and generate clusters considering the maximum set of connected dense units.</p>
            <p>Partition the data space and find the number of points that lie inside each cell of the partition, is performed in this way:</p>
            <ul>
            <li><p>Identify the subspaces that contain clusters using the <strong>Apriori principle</strong></p>
            <p>In particular, if a k-dimensional unit is dense, then so are its projections in <span class="math inline">\((k-1)\)</span> dimensional space.</p>
            <p>If a square is dense if we project it on <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axis, the intervals we get are dense. Therefore, the candidate dense units in the k-th dimensional space are generated by the dense units found in <span class="math inline">\((k-1)\)</span> dimensional space</p></li>
            <li><p>To identify clusters we determine dense units in all subspaces of interest and determine connected dense units in all subspaces of interests.</p></li>
            <li><p>Generate minimal description for the clusters. We determine maximal regions that cover a cluster of connected dense units for each cluster.In this way we determine the minimal cover for each cluster.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="clustering-steps">Clustering steps</h4>
            <ol type="1">
            <li><p>Step</p>
            <p>partitions the <span class="math inline">\(d-dimensional\)</span> data space into non-overlapping rectangular units, identifying the dense units among these.</p>
            <p>In particular, <code>CLIQUE</code> partitions every dimension into intervals, and identifies intervals containing at least <span class="math inline">\(l\)</span> points, where <span class="math inline">\(l\)</span> is the density threshold.</p>
            <p><code>CLIQUE</code> then iteratively joins two <span class="math inline">\(k-dimensional\)</span> dense cells, <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span>, in subspaces <span class="math inline">\((D_{i1} , \dots , D_{ik})\)</span> and <span class="math inline">\((D_{j1} , \dots , D_{jk})\)</span>.</p>
            <p>If <span class="math inline">\(D_{i1} = D_{j1} , \dots , D_{ik} = D_{jk}\)</span>, and <span class="math inline">\(c_1\)</span> and <span class="math inline">\(c_2\)</span> share the same intervals in those dimensions. The join operation generates a new <span class="math inline">\((k+1)-dimensional\)</span> candidate cell <span class="math inline">\(c\)</span> in space <span class="math inline">\((D_{i1}, \dots, D_{ik} ,D_{jk})\)</span>.</p>
            <p><code>CLIQUE</code> checks whether the number of points in <span class="math inline">\(c\)</span> passes the density threshold. The iteration terminates when no candidates can be generated, or no candidate cells are dense.</p>
            <p>When we reach the join operation, we are sure that units we are joining are dense but the result is not sure to be dense, we have to count the number of instances we have in the <span class="math inline">\((k+1)\)</span> cell we have generated.</p></li>
            <li><p>Step</p>
            <p><code>CLIQUE</code> uses the dense cells in each subspace to assemble clusters, which can be of arbitrary shape.</p>
            <p>Each cluster can have an arbitrary shape beacause we follow the density referring to dense units. The only limitation we have is that the boundary of the cluster is rectangular because follow the shape of the unit.</p>
            <p><strong>Minimum Description Length (<code>MDL</code>) principle</strong>: use the maximal regions to cover connected dense cells, where a maximal region is a hyper-rectangle where every cell falling into this region is dense, and the region cannot be extended further in any dimension in the subspace.</p>
            <p>This is the idea we exploited in the <code>DBSCAN</code>, here we put together dense cells.</p>
            <p>Finding the best description of a cluster in general is NP-Hard, we should combine all possible cells. Thus, <strong>CLIQUE adopts a simple greedy approach</strong>. It starts with an arbitrary dense cell, finds a maximal region covering the cell, and then works on the remaining dense cells that have not yet been covered.</p>
            <p>The greedy method terminates when all dense cells are covered.</p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="example-13">Example</h4>
            <p>The CLIQUE algorithm is used to cluster a set of records that have three attributes: salary, vacation, and age.</p>
            <p><img src="../media/image474.png" /></p>
            <p><strong>steps</strong>:</p>
            <ul>
            <li>The first step is to split each attribute into intervals of equal length, creating a 3-dimensional grid on the space.</li>
            </ul>
            <blockquote>
            <p>The goal is to find the dense 3-dimensional rectangular units within this grid.</p>
            </blockquote>
            <ul>
            <li><p>To do this, the algorithm looks at:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr class="header">
            <th>salary-age plane</th>
            <th>Vacation-age plane</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>the salary-age plane and finds all the 2-dimensional rectangular units that are dense.</td>
            <td>It also finds the dense 2-dimensional units for the vacation-age plane.</td>
            </tr>
            </tbody>
            </table>
            <p><img src="../media/image475.png" /></p></li>
            <li><p>The next step is to find the dense units in the salary-vacation plane</p></li>
            <li><p>Now, we perform an intersection of the candidate search space with the extension of the dense units of the salary-vacation plane, in order to get all the 3-d dense units</p>
            <p><img src="../media/image476.png" /></p></li>
            </ul>
            <p>After finding the dense units, it is very easy to find clusters</p>
            <h4 class="unnumbered" data-number="" id="strength-1">Strength</h4>
            <ul>
            <li><p>automatically finds subspaces of the highest dimensionality such that high density clusters exist in those subspaces</p></li>
            <li><p>insensitive to the order of records in input and does not presume some canonical data distribution</p></li>
            <li><p>scales linearly with the size of input and has good scalability as the number of dimensions in the data increases. The reason is that we work using a grid and it scales good when the number of dimensions increase.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="weakness-3">Weakness</h4>
            <ul>
            <li><p>Obtaining a meaningful clustering is dependent on proper tuning of the grid size and the density threshold. It also depends on the size, the granularity we use in the grid, if we use small intervals we have precise clusters but we need more time</p></li>
            <li><p>The accuracy of the clustering result may be degraded at the expense of simplicity of the method</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="conclusions-4">Conclusions</h4>
            <p>The main advantage is the computational effort, typically we execute in reasonable time, but this is a trade-off between precision and speed. If we are interested in a curse clustering we can use a curse grid, intervals quite large, but we have curse boundaries.</p>
            <p>In many applications we may not be interested in precise clusters.</p>
            <h2 data-number="6.9" id="evaluation-of-clustering"><span class="header-section-number">6.9</span> Evaluation of Clustering</h2>
            <p>We can evaluate our clustering result focusing on:</p>
            <ul>
            <li><p><strong>Determining the number of clusters in a dataset</strong>, when we apply partition method</p></li>
            <li><p><strong>Measuring clustering quality</strong>: measures to assess how well the clusters fit the dataset. We want measures that score clustering and thus can compare two sets of clustering results on the same dataset.</p></li>
            <li><p><strong>Assessing clustering tendency</strong>: clustering analysis on a data set is meaningful only when there is a nonrandom structure in the data. It was the process we followed when we talked about measuring the tendency of clustering on the dataset. We need to evaluate if data has really some natural tendencies to clusters.</p></li>
            </ul>
            <p>Here we don’t have the ground-truth so it’s hard to introduce metrics.</p>
            <h3 data-number="6.9.1" id="determining-the-number-of-clusters"><span class="header-section-number">6.9.1</span> Determining the number of clusters</h3>
            <p>Determining the number of clusters is important, we need to set them before executing the algorithm.</p>
            <p>We don’t know how many natural clusters we have in the dataset typically, so we typically set a generic value for <span class="math inline">\(k\)</span> and find <span class="math inline">\(k\)</span> clusters.</p>
            <p>If we consider the cost function of <code>k-means</code> we can’t exploit the cost function to determine <span class="math inline">\(k\)</span>, because the minimum of the cost function is <span class="math inline">\(0\)</span> when we have $k = $ number of points.</p>
            <h4 class="unnumbered" data-number="" id="elbow-method">Elbow method</h4>
            <p>We can use the <strong>Elbow method</strong> which says that increasing the number of clusters can help to reduce the sum of within-cluster variance of each cluster. The marginal effect of reducing the sum of within-cluster variances may drop if too many clusters are formed, because splitting a cohesive cluster into two gives only a small reduction.</p>
            <ul>
            <li><p>If I increase <span class="math inline">\(k\)</span> until I don’t find natural clusters I have considerable decrease in the cost of function</p></li>
            <li><p>if I find a value for <span class="math inline">\(k\)</span> for which we have natural clusters I won’t have a strong reduction.</p></li>
            <li><p>If I split a natural cluster in two I don’t have a lot of advantages, because the distribution in the points in the cluster is the same that we have in the two clusters separated.</p></li>
            </ul>
            <p>We use the turning point in the curve of sum of within cluster variance with respect to the the number of clusters.</p>
            <p>I plot the cost function with the increasing of <span class="math inline">\(k\)</span> and I determine the biggest variation for which we have the turning point that give us the optimal value of <span class="math inline">\(k\)</span>.</p>
            <p><img src="../media/image477.png" /></p>
            <p>This method can work if I have separated clusters, if I don’t have separated and well-identifiable clusters the cost function decreases and I can’t identify the turning point, I don’t have natural clusters.</p>
            <h5 class="unnumbered" data-number="" id="steps-4">Steps</h5>
            <p>given a number <span class="math inline">\(k &gt; 0\)</span></p>
            <ul>
            <li><p>Form <span class="math inline">\(k\)</span> clusters on the data set in question using a clustering algorithm like <code>k-means</code>, and</p></li>
            <li><p>Calculate the sum of within-cluster variances, <span class="math inline">\(var(k)\)</span>, but we can use also the cost function.</p></li>
            <li><p>Plot the curve of <span class="math inline">\(var\)</span> with respect to <span class="math inline">\(k\)</span>.</p></li>
            <li><p>Choose <span class="math inline">\(k\)</span> as the first or most significant turning point of the curve.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="cross-validation-method">Cross-validation method</h4>
            <p>It divides a given data set into <span class="math inline">\(m\)</span> parts and use <span class="math inline">\(m - 1\)</span> parts to obtain a clustering model.</p>
            <p>We then use the remaining part to test the quality of the clustering.</p>
            <h5 class="unnumbered" data-number="" id="example-14">Example</h5>
            <ul>
            <li><p>For each point in the test set, we find the closest centroid, and use the sum of squared distance between all points in the test set and the closest centroids to measure how well the model fits the test set.</p></li>
            <li><p>For any <span class="math inline">\(k &gt; 0\)</span>, repeat it <span class="math inline">\(m\)</span> times, compare the overall quality measure different <span class="math inline">\(k\)</span>’s, and find the number of clusters that fits the data the best.</p></li>
            </ul>
            <h3 data-number="6.9.2" id="measuring-clustering-quality"><span class="header-section-number">6.9.2</span> Measuring Clustering Quality</h3>
            <p>This is not easy because we don’t have the ground-truth, and also with a lot of dimensions it’s not possible to do it graphically.</p>
            <p>We have two types of methods we can use: extrinsic vs. intrinsic.</p>
            <ul>
            <li><p><strong>Extrinsic</strong> are supervised, i.e., the ground truth is available.</p>
            <p>They are useful when we want to compare a clustering against the ground truth, using certain clustering quality measure. We can show that our algorithm works properly.</p>
            <p>Ex. BCubed precision and recall metric.</p></li>
            <li><p><strong>Intrinsic</strong> are unsupervised, i.e., the ground truth is unavailable.</p>
            <p>It evaluates the goodness of a clustering by considering how well the clusters are separated, and how compact the clusters are.</p>
            <p>The evaluation of the result is based on clusters’ characteristics, compactness and separation.</p>
            <p>Ex. Silhouette coefficient</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="extrinsic-methods">Extrinsic Methods</h4>
            <p>Let’s consider a clustering quality measure <span class="math inline">\(Q(C, Cg)\)</span>, for a clustering <span class="math inline">\(C\)</span> given the ground truth <span class="math inline">\(Cg\)</span>.</p>
            <p><span class="math inline">\(Q\)</span> is good if it satisfies the following 4 essential criteria:</p>
            <ul>
            <li><p><strong>Cluster homogeneity</strong>: the purer, the better. We can consider purity because we know the ground truth, the idea is to have clusters that group instances belonging to the same class.</p></li>
            <li><p><strong>Cluster completeness</strong>: should assign objects belong to the same category in the ground truth to the same cluster.</p></li>
            <li><p><strong>Rag bag</strong>: putting a heterogeneous object into a pure cluster should be penalized more than putting it into a rag bag (i.e., “miscellaneous” or “other” category). The idea is that it’s better to have objects in miscellaneous cluster rather than others.</p></li>
            <li><p><strong>Small cluster preservation</strong>: splitting a small category into pieces is more harmful than splitting a large category into pieces.</p></li>
            </ul>
            <h5 class="unnumbered" data-number="" id="bcubed-precision-and-recall">Bcubed precision and recall</h5>
            <p>It is an extrinsic method that evaluates the precision and recall for every object in a clustering on a given data set according to ground truth. The precision of an object indicates how many other objects in the same cluster belong to the same category as the object.</p>
            <p>Ideally we want clusters containing only objects belonging to the same class. The recall of an object reflects how many objects of the same category are assigned to the same cluster.</p>
            <p>They’re defined because we would like to have each cluster corresponding to one class, this is the ideal situation.</p>
            <p><img src="../media/image478.png" /> <img src="../media/image479.png" /></p>
            <p><span class="math inline">\(L\)</span> identify the class of <span class="math inline">\(o_i\)</span>. If the two objects belong to the same class and cluster the correctness is equal to <span class="math inline">\(1\)</span>.</p>
            <p><img src="../media/image480.png" /> <img src="../media/image481.png" /></p>
            <p>If we put in the same cluster all objects belonging to the same class our precision and recall will assume the highest value, being close to 1.</p>
            <p>The algorithm with highest precision and recall is considered as the best.</p>
            <h4 class="unnumbered" data-number="" id="intrinsic-methods">Intrinsic Methods</h4>
            <p>Intrinsic methods evaluate a clustering by examining how well the clusters are separated and how compact the clusters are, we exploit the definition of good clusters; we have no other possibility.</p>
            <h5 class="unnumbered" data-number="" id="silhouette-coefficient">Silhouette coefficient</h5>
            <p>For each object <span class="math inline">\(o\)</span> in <span class="math inline">\(D\)</span>, we compute <span class="math inline">\(a(o)\)</span> as the average distance between <span class="math inline">\(o\)</span> and all the other objects in the cluster to which <span class="math inline">\(o\)</span> belongs to.</p>
            <p><span class="math display">\[
                a(o) = \frac{\sum_{o^{&#39;}\in C_i, o \neq o^{&#39;}}{dist(o,o^{&#39;})}}{|C_i| - 1}
            \]</span></p>
            <p>It measures compactness of the cluster in which <span class="math inline">\(o\)</span> is located. Low value is preferable, in that case the compactness is higher.</p>
            <p>Compute <span class="math inline">\(b(o)\)</span> as the minimum average distance from o to all clusters to which o does not belong to.</p>
            <p><span class="math display">\[
                b(o) = \min_{c_j | 1 \leq j \leq k, j\neq i}{\{\frac{\sum_{o^{&#39;}\in C_j}{dist(o, o^{&#39;})}}{|C_j|}\}}
            \]</span></p>
            <p>We would like to have <span class="math inline">\(b\)</span> high, because it is a measure of the separation between clusters.</p>
            <p>The <strong>silhouette coefficient</strong> of <span class="math inline">\(o\)</span> will be:</p>
            <p><span class="math display">\[
                s(o) = \frac{b(o)-a(o)}{\max{\{a(o), b(o)\}}}
            \]</span></p>
            <p>The value of the silhouette coefficient is between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. The smaller the value of <span class="math inline">\(a(o)\)</span>, the more compact the cluster.</p>
            <h5 class="unnumbered" data-number="" id="conclusions-5">Conclusions</h5>
            <ul>
            <li><p>When <span class="math inline">\(s(o)\)</span> approaches <span class="math inline">\(1\)</span>, the cluster containing <span class="math inline">\(o\)</span> is compact and <span class="math inline">\(o\)</span> is far away from other clusters, which is the preferable case.</p>
            <p><span class="math inline">\(a(o)\)</span> will be low and <span class="math inline">\(b(o)\)</span> will be high, <span class="math inline">\(\max{\{a(o), b(o)\}} = b(o)\)</span> and then the coefficient will be close to 1.</p></li>
            <li><p>When the silhouette coefficient value is negative $ b(o) &lt; a(o)$, this means that, in expectation, <span class="math inline">\(o\)</span> is closer to the objects in another cluster than to the objects in the same cluster as <span class="math inline">\(o\)</span>.</p>
            <p>This is a bad situation and should be avoided.</p>
            <blockquote>
            <p>The worst situation is <span class="math inline">\(-1\)</span>.</p>
            </blockquote></li>
            </ul>
            <p>To measure a cluster’s fitness within a clustering, we can compute the average silhouette coefficient value of all objects in the cluster.</p>
            <p>To measure the quality of a clustering, we can use the average silhouette coefficient value of all objects in the data set.</p>
            <p>With this value we can evaluate if this is a good clustering in terms of compactness and separation.</p>
            <p>This coefficient was defined for partitioning clustering and works very well if our clusters are convex.</p>
            <p>In the situation below it doesn’t work very well, these clusters are closer than the average distance we have in the cluster.</p>
            <p><img src="../media/image485.png" /></p>
            <p>It cannot be applied for measuring the quality of density-based algorithms.</p>
            <h1 data-number="7" id="clustering-of-high-dimensional-data"><span class="header-section-number">7</span> Clustering of high-dimensional data</h1>
            <p>The traditional distance measures which are frequently used in low-dimensional cluster analysis aren’t also effective on high-dimensional data. We have the problem of curse of dimensionality, objects appear very sparse.</p>
            <p>If we consider a problem of customers and products purchased:</p>
            <table>
            <thead>
            <tr class="header">
            <th>Customer</th>
            <th><span class="math inline">\(P_1\)</span></th>
            <th><span class="math inline">\(P_2\)</span></th>
            <th><span class="math inline">\(P_3\)</span></th>
            <th><span class="math inline">\(P_4\)</span></th>
            <th><span class="math inline">\(P_5\)</span></th>
            <th><span class="math inline">\(P_6\)</span></th>
            <th><span class="math inline">\(P_7\)</span></th>
            <th><span class="math inline">\(P_8\)</span></th>
            <th><span class="math inline">\(P_9\)</span></th>
            <th><span class="math inline">\(P_{10}\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>Ada</td>
            <td>1</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            </tr>
            <tr class="even">
            <td>Bob</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>1</td>
            </tr>
            <tr class="odd">
            <td>Cathy</td>
            <td>1</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>1</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>0</td>
            <td>1</td>
            </tr>
            </tbody>
            </table>
            <p>We can represent each customer with a vector of 1 if he purchased the corresponding product, 0 otherwise.</p>
            <p>If we compute the euclidian distance, we realize that distances are all the same.</p>
            <p>Most of the clustering algorithms are based on the concept of distance or density, but these have problems with an high number of features, they suffer for the curse of dimensionality problem. Also densities because we consider in it closeness of objects. We have to find a way to apply clustering dealing with curse of dimensionality.</p>
            <p>Major challenges:</p>
            <ul>
            <li><p>Many irrelevant dimensions may mask clusters</p></li>
            <li><p>Distance measure becomes meaningless—due to equi-distance, they may appear at the same distance</p></li>
            <li><p>Clusters may exist only in some subspaces</p></li>
            </ul>
            <p>Two major <strong>kinds of methods</strong> to deal with high-dimensional clustering:</p>
            <ul>
            <li><p><em>Subspace-clustering</em>: Search for clusters existing in subspaces of the given high dimensional data space. We don’t consider all dimensions but we identify clusters in subspaces.</p></li>
            <li><p><em>Dimensionality reduction approaches</em>: Construct a much lower dimensional space and search for clusters there (may construct new dimensions by combining some dimensions in the original data). We have to create them in a reasonable way.</p></li>
            </ul>
            <h2 data-number="7.1" id="the-curse-of-dimensionality"><span class="header-section-number">7.1</span> The curse of dimensionality</h2>
            <p>Data in only one dimension is relatively packed. Adding a dimension “stretch” the points across that dimension, making them further apart. Adding more dimensions will make the points further apart—high dimensional data is extremely sparse.</p>
            <p>Distance measure becomes meaningless due to equi-distance.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image487.png" /></td>
            <td><img src="../media/image488.png" /></td>
            </tr>
            </tbody>
            </table>
            <h3 data-number="7.1.1" id="subspace-clustering"><span class="header-section-number">7.1.1</span> Subspace Clustering</h3>
            <p>Clusters may exist only in some subspaces. Subspace-clustering consists in finding clusters in all the subspaces. Some dimensions can act as noise, but we may have some projection of subspaces in which we identify natural clusters.</p>
            <p>In 3 dimensions it’s not easy to identify clusters:</p>
            <p><img src="../media/image489.png" /></p>
            <p>If I project data objects along dimensions:</p>
            <p><img src="../media/image490.png" /></p>
            <p>We have that projecting on dimension b two clusters are merged.</p>
            <p>If we project along two dimensions, we can isolate some clusters, in the first case red and green, in the second violet and blue, and so on. When we have an high number of dimensions we shouldn’t try to discover clusters with the original space but it’s more suited to consider subspaces. We have subspace search methods that search various subspaces to find clusters.</p>
            <h2 data-number="7.2" id="subspace-clustering-methods"><span class="header-section-number">7.2</span> Subspace Clustering Methods</h2>
            <p>They can be Bottom-up approaches or Top-down approaches. An approach is correlation-based clustering methods (E.g., PCA based approaches). Another approach are Bi-clustering methods, they perform subspace selection and clustering concurrently. We have two types of approaches:</p>
            <ul>
            <li><p><em>Optimization-based methods</em></p></li>
            <li><p><em>Enumeration methods</em></p></li>
            </ul>
            <h3 data-number="7.2.1" id="subspace-search-methods"><span class="header-section-number">7.2.1</span> Subspace Search Methods</h3>
            <p>Search various subspaces to find clusters:</p>
            <ul>
            <li><p><strong>Bottom-up approaches</strong></p>
            <p>Start from low-D subspaces and search higher-D subspaces only when there may be clusters in such subspaces.</p>
            <p>Various pruning techniques to reduce the number of higher-D subspaces to be searched.</p>
            <p>Ex. CLIQUE</p></li>
            <li><p><strong>Top-down approaches</strong></p>
            <p>Start from full space and search smaller subspaces recursively Effective only if the locality assumption holds: restricts that the subspace of a cluster can be determined by the local neighborhood.</p>
            <p>Ex. PROCLUS: a k-medoid-like method</p></li>
            </ul>
            <h3 data-number="7.2.2" id="correlation-based-methods"><span class="header-section-number">7.2.2</span> Correlation-Based Methods</h3>
            <p>Subspace search method can be <strong>similarity based on distance or density</strong>. Correlation-based method are based on <strong>advanced correlation models</strong>.</p>
            <p>Ex. PCA based approach: We apply PCA (for Principal Component Analysis) to derive a set of new, uncorrelated dimensions, then mine clusters in the new space or its subspaces. We can do it using eigenvectors associated with high eigenvalues.</p>
            <h3 data-number="7.2.3" id="bi-clustering-methods"><span class="header-section-number">7.2.3</span> Bi-Clustering Methods</h3>
            <p><strong>Bi-clustering</strong> approaches cluster both objects and attributes simultaneously (we treat objects and attributes in symmetric way). We have clusters of attributes and clusters of objects.</p>
            <p><img src="../media/image491.png" /></p>
            <p>We want to obtain a cluster of objects in a cluster of attributes. At the end we will have clusters of objects described by a subset of attributes.</p>
            <p>Four requirements:</p>
            <ul>
            <li><p>Only a small set of objects participate in a cluster</p></li>
            <li><p>A cluster only involves a small number of attributes</p></li>
            <li><p>An object may participate in multiple clusters, or does not participate in any cluster at all. This because one object can belong to different clusters in different subspaces.</p></li>
            <li><p>An attribute may be involved in multiple clusters, or is not involved in any cluster at all</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="types-of-bi-clusters">Types of Bi-clusters</h4>
            <p>We have to define the type of bi-cluster when we apply it.</p>
            <p>Ex 1. Gene expression or microarray data: a gene sample/condition matrix.</p>
            <p>Each element in the matrix, a real number, records the expression level of a gene under a specific condition Let <span class="math inline">\(A = {a_1, \dots, a_n}\)</span> be a set of genes, <span class="math inline">\(B = {b_1, \dots, b_n}\)</span> a set of conditions. Let <span class="math inline">\(E=[e_{ij}]\)</span> be a gene expression data matrix.</p>
            <p>A bi-cluster is a submatrix where genes and conditions follow some consistent patterns, a <strong>submatrix where objects and attributes follow consistent patterns</strong>.</p>
            <table>
            <thead>
            <tr class="header">
            <th></th>
            <th><span class="math inline">\(\dots\)</span></th>
            <th><span class="math inline">\(b_6\)</span></th>
            <th><span class="math inline">\(\dots\)</span></th>
            <th><span class="math inline">\(b_{12}\)</span></th>
            <th><span class="math inline">\(\dots\)</span></th>
            <th><span class="math inline">\(b_{36}\)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><span class="math inline">\(a_1\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(a_{33}\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            </tr>
            <tr class="even">
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            </tr>
            <tr class="odd">
            <td><span class="math inline">\(a_{86}\)</span></td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            <td><span class="math inline">\(\dots\)</span></td>
            <td>60</td>
            </tr>
            </tbody>
            </table>
            <p>We have 4 types of bi-clusters patterns (ideal cases):</p>
            <ul>
            <li><p><strong>Bi-clusters with constant values</strong>:</p>
            <p>for any <span class="math inline">\(i\)</span> in <span class="math inline">\(I\)</span> and <span class="math inline">\(j\)</span> in <span class="math inline">\(J\)</span>, <span class="math inline">\(e_{ij} = c\)</span></p>
            <p>We have a subset of objects and a subset of dimensions for which we have a constant value.</p>
            <p>We have to find a set of objects and attributes for which we have the same value.</p></li>
            <li><p><strong>Bi-clusters with constant values on rows:</strong></p>
            <p>We have constant values along the single row. This means that:</p>
            <p><span class="math display">\[
                  e_{ij}=c + \alpha_i
              \]</span></p>
            <p>where αi is the adjustment for row i. Also, it can be constant values on columns</p>
            <p>We start with a constant value and for each row we have just a difference due to this ai.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>10</td>
            <td>10</td>
            <td>10</td>
            <td>10</td>
            <td>10</td>
            </tr>
            <tr class="even">
            <td>20</td>
            <td>20</td>
            <td>20</td>
            <td>20</td>
            <td>20</td>
            </tr>
            <tr class="odd">
            <td>50</td>
            <td>50</td>
            <td>50</td>
            <td>50</td>
            <td>50</td>
            </tr>
            </tbody>
            </table>
            <p>We could have also constant values on columns.</p></li>
            <li><p><strong>Bi-clusters with coherent values</strong> (aka. pattern-based clusters).</p>
            <p>Rows change in a synchronized way with respect to the columns and vice versa</p>
            <p><span class="math inline">\(e_{ij} = c + \alpha_i + \Beta j\)</span></p>
            <p>We have an adjustment for the row and an adjustment for the column. $A <span class="math inline">\(I \times J\)</span>$ is a bi-cluster with coherent values if and only if for any:</p>
            <p><span class="math display">\[
                  i_1, i_2 \in I \text{ and } j_1, j_2 \in J \text{, then } e_{i_1 j_1} - e_{i_2 j_2} = e_{i_2 j_1} e_{i_1 j_2}  
              \]</span></p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>10</td>
            <td>50</td>
            <td>30</td>
            <td>70</td>
            <td>20</td>
            </tr>
            <tr class="even">
            <td>20</td>
            <td>60</td>
            <td>40</td>
            <td>80</td>
            <td>30</td>
            </tr>
            <tr class="odd">
            <td>50</td>
            <td>90</td>
            <td>70</td>
            <td>110</td>
            <td>60</td>
            </tr>
            <tr class="even">
            <td>0</td>
            <td>40</td>
            <td>20</td>
            <td>60</td>
            <td>10</td>
            </tr>
            </tbody>
            </table>
            <p>with <span class="math inline">\(I\)</span> a set of rows and <span class="math inline">\(J\)</span> a set of attributes.</p>
            <p>For any pairs of rows and columns we have that those differences are equal, difference between elements in the same row and different coumn must be equal to the difference between different row but same column.</p>
            <p>We have the same variation for each attribute we consider in the cluster, as we can see in the example.</p></li>
            <li><p><strong>Bi-clusters with coherent evolutions on rows</strong> i.e., only interested in the up- or down- regulated changes across genes or conditions without constraining on the exact values. We want to observe the tendency and not precise values.</p>
            <table>
            <thead>
            <tr class="header">
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            <th><!--  --></th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>10</td>
            <td>50</td>
            <td>30</td>
            <td>70</td>
            <td>20</td>
            </tr>
            <tr class="even">
            <td>20</td>
            <td>100</td>
            <td>50</td>
            <td>1000</td>
            <td>30</td>
            </tr>
            <tr class="odd">
            <td>50</td>
            <td>100</td>
            <td>90</td>
            <td>120</td>
            <td>80</td>
            </tr>
            <tr class="even">
            <td>0</td>
            <td>80</td>
            <td>20</td>
            <td>100</td>
            <td>10</td>
            </tr>
            </tbody>
            </table>
            <p>The tendency is the same but the values are different.</p>
            <p><span class="math display">\[
                  \forall \ i_1, i_2 \in I \text{ and } j_1, j_2 \in J \text{, then } (e_{i_1 j_1} - e_{i_2 j_2}) (e_{i_2 j_1} e_{i_1 j_2}) \geq 0  
              \]</span></p>
            <p>Pairs of rows and columns we have that the difference between elements with same column multiplied for the difference between elements in the same row, is positive.</p>
            <p>The tendency is always the same, if the differences are both positive/negative the product will be positive.</p></li>
            </ul>
            <h2 data-number="7.3" id="bi-clustering-methods-1"><span class="header-section-number">7.3</span> Bi-Clustering Methods</h2>
            <p>These are ideal definitions of bi-clusters, but real-world data is noisy. We try to find approximate bi-clusters.</p>
            <p>We have two types of Methods: <em>Optimization-based methods</em> vs. <em>enumeration methods</em>.</p>
            <ul>
            <li><p><strong>Optimization-based methods</strong></p>
            <p>Try to find a submatrix at a time that achieves the best significance as a bi-cluster. Due to the cost in computation, greedy search is employed to find local optimal bi-clusters.</p>
            <p>One example is the <span class="math inline">\(\delta\)</span>-Cluster Algorithm.</p></li>
            <li><p><strong>Enumeration methods</strong></p>
            <p>Use a tolerance threshold to specify the degree of noise allowed in the bi-clusters to be mined, then try to enumerate all submatrices as bi-clusters that satisfy the requirements.</p>
            <p>We start from attributes 2 by 2 submatrix and we try increase this submatrix.</p>
            <p>Ex. <span class="math inline">\(\delta\)</span>-pCluster Algorithm.</p></li>
            </ul>
            <h3 data-number="7.3.1" id="bi-clustering-for-micro-array-data-analysis"><span class="header-section-number">7.3.1</span> Bi-Clustering for Micro-Array Data Analysis</h3>
            <p>Let’s consider the three objects in the left characterized by three features. Left figure: Micro-array “raw” data shows 3 genes and their values in a multi-D space: It’s difficult to find their patterns. I conclude that I don’t have any pattern. The reason is that we have some attributes acting as noise.</p>
            <p>If we focus on some attributes and consider their subspace (second plot), we find out that these three objects in that subspace are in a bi-cluster with coherent values, because the difference between corresponding values in the attributes are the same.</p>
            <p>If I analyze my objects in the overall space I cannot realize it but focusing on a subspace I realize a pattern for the three objects. In the third subspace I’m considering f,d,a,g,i and we can discover I have a bi-cluster with coherent evolution on rows, for each object we have the same tendency in the attributes of the bi-cluster.</p>
            <p><img src="../media/image497.png" /></p>
            <h4 class="unnumbered" data-number="" id="delta-bi-cluster"><span class="math inline">\(\delta\)</span>-Bi-Cluster</h4>
            <p>We work using an optimization method.</p>
            <p><img src="../media/image498.png" /></p>
            <p>If we are in the case of coherent values, H tends to be very small.</p>
            <p>A submatrix <span class="math inline">\(I \times J\)</span> is <strong><span class="math inline">\(\delta\)</span>-bi-cluster</strong> if <span class="math inline">\(H(I \times J) \leq \delta\)</span> where <span class="math inline">\(\delta \geq 0\)</span> is a threshold fixed by the user.</p>
            <p>When <span class="math inline">\(\delta = 0\)</span>, <span class="math inline">\(I \times J\)</span> is a perfect bi-cluster with coherent values. By setting <span class="math inline">\(\delta &gt; 0\)</span>, a user can specify the tolerance of average noise per element against a perfect bi-cluster.</p>
            <p><span class="math display">\[
                residue(e_{ij}) = e_{ij} − e_{iJ} − e_{Ij} + e_{IJ} 16
            \]</span></p>
            <p>When we increase the size of <span class="math inline">\(I\)</span> and the size of <span class="math inline">\(J\)</span> we want to obtain on average a bi-cluster with this H value, lower than or equal to delta. This delta is a tolerance that each bi-cluster we generate on average.</p>
            <h4 class="unnumbered" data-number="" id="the-delta-cluster-algorithm">The <span class="math inline">\(\delta\)</span>-Cluster Algorithm</h4>
            <p>Maximal <span class="math inline">\(\delta\)</span>-bi-cluster is a <span class="math inline">\(\delta\)</span>-bi-cluster <span class="math inline">\(I \times J\)</span> such that there does not exist another <span class="math inline">\(\delta\)</span>-bi-cluster <span class="math inline">\(I^{&#39;} \times J^{&#39;}\)</span> which contains <span class="math inline">\(I \times J\)</span>.</p>
            <p>Two phase computation: <em>deletion phase</em> and <em>additional phase</em>.</p>
            <ul>
            <li><p><strong>Deletion phase:</strong></p>
            <p>Start from the whole matrix, iteratively remove rows and columns while the mean squared residue of the matrix is over <span class="math inline">\(\delta\)</span>. At each iteration, for each row/column, compute the mean squared residue:</p>
            <p><img src="../media/image499.png" /></p>
            <p>and we remove the row or column of the largest mean squared residue, it is the residual that impact more on the computation.</p></li>
            <li><p><strong>Addition phase:</strong></p>
            <p>Expand iteratively the <span class="math inline">\(\delta\)</span>-bi-cluster <span class="math inline">\(I \times J\)</span> obtained in the deletion phase as long as the <span class="math inline">\(\delta\)</span>-bi-cluster requirement is maintained. Consider all the rows/columns not involved in the current bi-cluster Ix J by calculating their mean squared residues. A row/column of the smallest mean squared residue is added into the current <span class="math inline">\(\delta\)</span>-bi-cluster. This approach finds only one <span class="math inline">\(\delta\)</span>-bi-cluster, thus needs to run multiple times: replacing the elements in the output bi-cluster by random numbers. We can have different clusters of objects in different subspaces, so we need to apply this approach multiple times, but if we execute the approach without changing anything we find the same cluster. This technique allows us to find a number of bi-clusters. That approach doesn’t guarantee to find the local optimum.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="delta-pcluster"><span class="math inline">\(\delta\)</span>-pCluster</h4>
            <p>It consists in enumerating all bi-clusters (<span class="math inline">\(\delta\)</span>-pClusters). Since a submatrix <span class="math inline">\(I \times J\)</span> is a bi-cluster with (perfect) coherent values if <span class="math inline">\(e_{i_1 j_1} − e_{i_2 j_1} = e_{i_1 j_2} − e_{i_2 j_2}\)</span>.</p>
            <p>For any 2 x 2 submatrix of <span class="math inline">\(I \times J\)</span>, define <em>p-score</em>:</p>
            <p><img src="../media/image500.png" /></p>
            <p>It’s the difference between the pairs between two rows in the first column and the pairs between two rows in the second column. When we have perfect coherent values this p-score is equal to 0.</p>
            <p>A submatrix <span class="math inline">\(I \times J\)</span> is a <strong><span class="math inline">\(\delta\)</span>-pCluster</strong> (pattern-based cluster) if the p-score of every 2 x 2 submatrix of <span class="math inline">\(I \times J\)</span> is at most <span class="math inline">\(\delta\)</span>, where <span class="math inline">\(\delta\)</span> ≥ 0 is a threshold specifying a user's tolerance of noise against a perfect bi-cluster. We fix delta and compute the p-score.</p>
            <p>The p-score controls the noise on every element in a bi-cluster, while the mean squared residue captures the average noise.</p>
            <p>For the <strong>monotonicity</strong>: If <span class="math inline">\(I \times J\)</span> is a <span class="math inline">\(\delta\)</span>-pClusters, every x x y (x,y ≥ 2) submatrix of <span class="math inline">\(I \times J\)</span> is also a <span class="math inline">\(\delta\)</span>-pClusters. A <span class="math inline">\(\delta\)</span>-pCluster is <strong>maximal</strong> if no more row or column can be added into the cluster and retain <span class="math inline">\(\delta\)</span>-pCluster: We only need to compute all maximal <span class="math inline">\(\delta\)</span>-pClusters. We increase the 2x2 submatrix adding one row or column verifying the p-score is under the threshold we fixed.</p>
            <p><strong>MaPle: Efficient Enumeration of <span class="math inline">\(\delta\)</span>-pClusters</strong></p>
            <p>For each condition combination <span class="math inline">\(J\)</span>, find the maximal subsets of genes <span class="math inline">\(I\)</span> such that <span class="math inline">\(I \times J\)</span> is a <span class="math inline">\(\delta\)</span>-pClusters.</p>
            <p>If <span class="math inline">\(I \times J\)</span> is not a submatrix of another <span class="math inline">\(\delta\)</span>- pClusters then <span class="math inline">\(I \times J\)</span> is a maximal <span class="math inline">\(\delta\)</span>-pCluster. The algorithm is very similar to mining frequent closed itemsets.</p>
            <p>We are determining bi-clusters with approximatively coherent values, approximate for the tollerance we have.</p>
            <h3 data-number="7.3.2" id="dimensionality-reduction-methods"><span class="header-section-number">7.3.2</span> Dimensionality-Reduction Methods</h3>
            <p>Another approach we have tries to reduce the dimensions. We construct a new space of the original space and try to identify clusters in the new space.</p>
            <p>Dimensionality reduction: <strong>In some situations, it is more effective to construct a new space instead of using some subspaces of the original data.</strong></p>
            <p>Ex. To cluster the points in the figure, any subspace of the original one, X and Y, cannot help, since all the three clusters will be projected into the overlapping areas in X and Y axes.</p>
            <p><img src="../media/image501.png" /></p>
            <p>If we construct a new dimension as the dashed one, if we project along this new dimension we can identify the cluster very easily, the three clusters become apparent when the points projected into the new dimension.</p>
            <p>We have different <strong>dimensionality reduction methods</strong>:</p>
            <ul>
            <li><p><em>Feature selection and extraction</em>: But may not focus on clustering structure finding. When we transform the space with PCA we find a dimension for which we have high variability, while for clustering it’s useful to use the spectrum of the similarity matrix of the data.</p></li>
            <li><p><em>Spectral clustering</em>: Combining feature extraction and clustering (i.e., use the spectrum of the similarity matrix of the data to perform dimensionality reduction for clustering in fewer dimensions)</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="spectral-clusteringthe-ng-jordan-weiss-njw-algorithm">Spectral Clustering(The Ng-Jordan-Weiss (<code>NJW</code>) Algorithm)</h4>
            <p>We generate the affinity matrix that takes in consideration the affinity between objects.</p>
            <p>Given a set of objects <span class="math inline">\(o_1, \dots, o_n\)</span>, and the distance between each pair of objects, <span class="math inline">\(dist(o_i, o_j)\)</span>, find the desired number <span class="math inline">\(k\)</span> of clusters Calculate an affinity matrix <span class="math inline">\(W\)</span>, where <span class="math inline">\(\sigma\)</span> is a scaling parameter that controls how fast the affinity <span class="math inline">\(W_{ij}\)</span> decreases as <span class="math inline">\(dist(o_i, o_j)\)</span> increases.</p>
            <p>Each element <span class="math inline">\(i,j\)</span> in this matrix corresponds to:</p>
            <p><span class="math display">\[
                W_{ij} = e^{-\frac{dist(o_i, o_j)}{\sigma^2}}
            \]</span></p>
            <p>It takes in consideration the affinity, the closeness between objects.</p>
            <p>In <code>NJW</code>, set <span class="math inline">\(W_{ii} = 0\)</span>.</p>
            <p>Then we derive a matrix <span class="math inline">\(A = f(W)\)</span>, generated as following.</p>
            <p><code>NJW</code> defines a matrix, <span class="math inline">\(D\)</span>, as a diagonal matrix such that <span class="math inline">\(D_{ii}\)</span> is the sum of the i-th row of <span class="math inline">\(W\)</span>, that is:</p>
            <p><span class="math display">\[
                D_{ii} = \sum_{j=1}^n{W_{ij}}
            \]</span></p>
            <p>Then, <span class="math inline">\(A\)</span> is set to:</p>
            <p><span class="math display">\[
                A = D^{-\frac{1}{2}}W D^{-\frac{1}{2}}
            \]</span></p>
            <p>The we find the <span class="math inline">\(k\)</span> leading eigenvectors of <span class="math inline">\(A\)</span> and we project the original data into the new space defined by these k eigen-vectors.</p>
            <p>A vector <span class="math inline">\(v\)</span> is an eigenvector of matrix <span class="math inline">\(A\)</span> if <span class="math inline">\(Av = \lambda v\)</span>, where <span class="math inline">\(\lambda\)</span> is the corresponding eigen-value Using the k leading eigenvectors, project the original data into the new space defined by the k leading eigenvectors, and run a clustering algorithm, such as k-means, to find k clusters on the transformed space formed by the k-leading eigenvectors.Assign the original data points to clusters according to how the transformed points are assigned in the clusters obtained.</p>
            <p><img src="../media/image505.png" /></p>
            <p>We reduce the dimensions, apply the clustering in the new space and project back to cluster the original data.</p>
            <h4 class="unnumbered" data-number="" id="example-15">Example</h4>
            <p>An example of application: 200 data in each half moon.</p>
            <p><img src="../media/image506.png" /></p>
            <ul>
            <li><p>If I apply the classical k-means we don’t obtain this type of clusters.</p>
            <p>The similarity is:</p>
            <p><img src="../media/image507.png" /></p>
            <p>K = 2</p></li>
            <li><p>Spectral embedding given by the first two eigenvectors:</p>
            <p><img src="../media/image508.png" /></p></li>
            <li><p>We can determine clusters in this way.</p>
            <p>Partition obtained by NJW:</p>
            <p><img src="../media/image509.png" /></p>
            <p>There are examples where Spectral Clustering result is really different from the optimal one.</p>
            <p><img src="../media/image510.png" /></p></li>
            <li><p>The solution given by NJW with three classes, its normalized cut is 0.16.</p>
            <p>We find a higher number of clusters, if we change the parameters of cutting we find in any case different results than the original.</p></li>
            </ul>
            <h1 data-number="8" id="clustering-with-constraints"><span class="header-section-number">8</span> Clustering with Constraints</h1>
            <p>Need user feedback: Users know their applications the best.</p>
            <p>Less parameters but more user-desired constraints.</p>
            <p>A bank manager wishes to locate four ATMs in the area in the figure on the left: obstacle and desired clusters. We want to locate ATMs to serve approximately the same population. Ignoring the obstacles will result in the clusters on the right, approximately we will have the same size of population covered by each ATM.</p>
            <table>
            <thead>
            <tr class="header">
            <th>Obstacle view</th>
            <th>Ignoring the obstacles</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image511.png" /></td>
            <td><img src="../media/image512.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>But with C1 people in the other part of the river need to find a bridge to cross the river.</p>
            <p>Constraints: presence of the hill and the river.</p>
            <p>We have to find clustering algorithms that allow us to fix constraints.</p>
            <h2 data-number="8.1" id="categorization-of-constraints"><span class="header-section-number">8.1</span> Categorization of Constraints</h2>
            <h3 data-number="8.1.1" id="constraints-on-instances"><span class="header-section-number">8.1.1</span> <strong>Constraints on instances</strong>:</h3>
            <p>Specifies how a pair or a set of instances should be grouped in the cluster analysis.</p>
            <h4 class="unnumbered" data-number="" id="two-types"><strong>Two types</strong>:</h4>
            <ul>
            <li><p><strong>must-link(x, y)</strong>: x and y should be grouped into one cluster</p></li>
            <li><p><strong>cannot-link(x, y)</strong> are constraints that can be defines if they cannot be linked together, for example if distance(x, y) &gt; d</p></li>
            </ul>
            <p>If we apply the clustering without taking in consideration constraints, the output is the third.</p>
            <p><img src="../media/image513.png" /></p>
            <p>If we impose constraints, like must-link constraints if some must belong to the same cluster and cannot link constraints if they cannot link together.</p>
            <h3 data-number="8.1.2" id="constraints-on-clusters"><span class="header-section-number">8.1.2</span> <strong>Constraints on clusters</strong>:</h3>
            <p>Specify a requirement on the clusters.</p>
            <p>E.g., specify the min number of objects in a cluster, the max diameter of a cluster, the shape of a cluster (e.g., a convex), number of clusters (e.g., k).</p>
            <ul>
            <li><p><span class="math inline">\(\delta-constraint\)</span> (Minimum separation):</p>
            <p>For any two clusters <span class="math inline">\(S_i\)</span>, <span class="math inline">\(S_j\)</span>, <span class="math inline">\(∀i, j\)</span> for each two instances <span class="math inline">\(s_p \in S_i\)</span>, <span class="math inline">\(s_q \in S_j\)</span>, <span class="math inline">\(∀p, q:\)</span></p>
            <p><span class="math display">\[
                D(sp,sq) \ge \delta
            \]</span></p>
            <p>The minimum separation between clusters must be higher than delta.</p></li>
            <li><p><span class="math inline">\(\varepsilon-constraint\)</span></p>
            <p>For any cluster <span class="math inline">\(S_i, |S_j| &gt; 1\)</span>, <span class="math inline">\(∀p, s_p \in S_i, \exists s_q \in S_i:\)</span></p>
            <p><span class="math display">\[
                \varepsilon \ge D(s_p,s_q), s_p \ne s_q
            \]</span></p>
            <p>Takes in consideration the compactness of clusters.</p></li>
            </ul>
            <p>Constraints on clusters can be converted to instance level constraints.</p>
            <ul>
            <li><p><span class="math inline">\(\delta-constraint\)</span> (Minimum separation)</p>
            <p>For every point x, must-link all points y such the <span class="math inline">\(D(x,y) &lt; \delta\)</span>, i.e., conjunction of must link (ML) constraints. .</p>
            <p>Can be imposed to instances belonging to the different cluster.</p>
            <p><img src="../media/image514.png" /></p></li>
            <li><p><span class="math inline">\(\varepsilon-constraint\)</span></p>
            <p>For every point x, must link to at least one point y such that <span class="math inline">\(D(x,y) \le \varepsilon\)</span>, i.e. disjunction of ML constraints.</p>
            <p>Can be imposed to instances belonging to the same cluster.</p>
            <p><img src="../media/image515.png" /></p></li>
            </ul>
            <p>Points inside clusters is connected with a point inside the cluster with a distance below epsilon.</p>
            <p>Will generate many instance level constraints.</p>
            <p>If we impose constraints, considering some instances in clusters, we can arrive to have the clusters we expect.</p>
            <h3 data-number="8.1.3" id="constraints-on-similarity-measurements"><span class="header-section-number">8.1.3</span> <strong>Constraints on similarity measurements</strong>:</h3>
            <p>Specifies a requirement that the similarity calculation must respect</p>
            <p>E.g., to cluster people as moving objects in a plaza, while Euclidean distance is used to give the walking distance between two points, a constraint on similarity measurement is that the trajectory implementing the shortest distance cannot cross a wall.</p>
            <p>If we consider only the euclidian distance we are in trouble because we do not consider possible obstacles. We need to reconsider the concept of similarity.</p>
            <h3 data-number="8.1.4" id="hard-vs.-soft-constraints"><span class="header-section-number">8.1.4</span> <strong>Hard vs. soft constraints</strong>:</h3>
            <ul>
            <li><p>A constraint is <strong>hard</strong> if a clustering that violates the constraint is unacceptable. If we are not able to satisfies the constraint we won’t generate the cluster.</p></li>
            <li><p>A constraint is <strong>soft</strong> if a clustering that violates the constraint is not preferable but acceptable when no better solution can be found.</p>
            <p>Soft constraints are also called <em>preferences</em>. This is more an optimization problem, if you satisfy it it’s better but in any case the cluster is acceptable.</p></li>
            </ul>
            <h2 data-number="8.2" id="clustering-with-constraints-1"><span class="header-section-number">8.2</span> Clustering with constraints</h2>
            <p>Partition unlabeled data into clusters and use constraints to aid and bias clustering. Our goal is to have examples in same cluster similar, separate clusters different and constraints are maximally respected.</p>
            <h3 data-number="8.2.1" id="enforcing-constraints"><span class="header-section-number">8.2.1</span> Enforcing Constraints:</h3>
            <ul>
            <li><p><strong>Strict enforcement</strong>: find best feasible clustering respecting all constraints</p></li>
            <li><p><strong>Partial enforcement</strong>: find best clustering maximally respecting constraints</p></li>
            </ul>
            <p>These are possible constraints in the data set:</p>
            <p><img src="../media/image516.png" /></p>
            <p>These are not hard or soft, we need to establish it.</p>
            <p><strong>Must-link</strong> means that the two has to belong to the same cluster while <strong>cannot-link</strong> the opposite.</p>
            <p>We can also use labeled data and create must-link exploiting the class, objects belonging to the same class will be linked. We can exploit this partial knowledge to achieve what we want to obtain.</p>
            <p>We will have at the end:</p>
            <p><img src="../media/image517.png" /></p>
            <p>In this situation constraints are respected.</p>
            <p>We can also have <strong>Conflicting or redundant constraints.</strong></p>
            <p>For example:</p>
            <p><span class="math inline">\(must-link(x, y)\ if\ dist(x, y) &lt; 5\)</span></p>
            <p><span class="math inline">\(cannot-link(x, y)\ if\ dist(x, y) &gt; 3\)</span></p>
            <p>If a data set has two objects, x, y, such that dist(x, y) = 4, then no clustering can satisfy both constraints simultaneously.</p>
            <p>if these are hard we are in trouble, if soft we can perform a choice.</p>
            <h3 data-number="8.2.2" id="how-can-we-measure-the-quality-and-the-usefulness-of-a-set-of-constraints"><span class="header-section-number">8.2.2</span> How can we measure the quality and the usefulness of a set of constraints?</h3>
            <ul>
            <li><p><strong>Informativeness</strong>: the amount of information carried by the constraints that is beyond the clustering model. Given a data set, D, a clustering method, A, and a set of constraints, C, the informativeness of C with respect to A on D can be measured by the fraction of constraints in C that are unsatisfied by the clustering computed by A on D.</p>
            <p>We reason on the fraction of constraints that are unsatisfied.</p></li>
            <li><p><strong>Coherence of a set of constraints</strong>: the degree of agreement among the constraints themselves, which can be measured by the redundancy among the constraints.</p></li>
            </ul>
            <p>Constraints divide the set of all plausible solutions into two sets: feasible and infeasible: <span class="math inline">\(S = S_F \cup S_I\)</span>.</p>
            <p>Constraints effectively reduce the search space to <span class="math inline">\(S_F\)</span>.</p>
            <p><span class="math inline">\(S_F\)</span> all have a common property, respecting the constraints. So it’s not unexpected that we find solutions with a desired property and find them quickly, because our search space is reduced focusing only on feasible solutions.</p>
            <h3 data-number="8.2.3" id="handling-hard-constraints"><span class="header-section-number">8.2.3</span> <strong>Handling hard constraints</strong>:</h3>
            <p>Strictly respect the constraints in cluster assignments</p>
            <p>The COP-k-means algorithm in which we take in consideration constraints. The idea is to generate super-instances for must-link constraints.</p>
            <p>To do this we compute the transitive closure of the must-link constraints and to represent such a subset, we replace all those objects in the subset by the mean.</p>
            <p>We consider all objects linked by must-link and instead of considering them separately we create super-instances and in this way we enforce that all instances has to belong to the same cluster.</p>
            <p>The super-instances also carries a weight, which is the number of objects it represents. Important because when we apply the k-means that super-instance represents a number of instances, they must attracts more the centroid of cluster than other instances because they represent group of instances.</p>
            <p>We conduct modified k-means clustering to respect cannot-link constraints</p>
            <p>We modify the center-assignment process in k-means to a nearest feasible center assignment.</p>
            <p>An object is assigned to the nearest center so that the assignment respects all cannot-link constraints</p>
            <p>We create these super-instances and in terms of cannot-link we modify the center assignment to the nearest center assignment.</p>
            <p><img src="../media/image518.png" /></p>
            <p>We apply this approach when we have a set of unlabeled data but also a set of labeled data.</p>
            <p>We are supposing to have a large amount of data but some ara characterized by labels and we create must-links with objects belonging to the same class.</p>
            <p>With also cannot-link constraints we can apply it the same.</p>
            <p>The output is a set of clusters where constraints are respected.</p>
            <p>If the labels are the same we put them in the must-link, otherwise in cannot-link constraints.</p>
            <p>In 4 we replace subset of points with a super-point, that is an average of points connected by must-link constraints. We associate a weight corresponding to the points we have in the must-link.</p>
            <p>In this way we respect must-link and adjust points to respect cannot-links.</p>
            <p><img src="../media/image519.png" /></p>
            <p>We just replace with an instance the two objects connected by a must-link and the cannot-link remains.</p>
            <p><img src="../media/image520.png" /></p>
            <p>We perform the execution of k-means using the cannot-link to move the membership of points to the cluster which is different than the cluster of the other point.</p>
            <p><img src="../media/image521.png" /></p>
            <h3 data-number="8.2.4" id="handling-soft-constraints"><span class="header-section-number">8.2.4</span> <strong>Handling Soft Constraints</strong></h3>
            <p><strong>Treated as an optimization problem</strong>: When a clustering violates a soft constraint, a penalty is imposed on the clustering. If we satisfy them, we are reaching an optimal solution.</p>
            <p><strong>Overall objective</strong>: Optimizing the clustering quality and minimizing the constraint violation penalty.</p>
            <p>Ex. CVQE (Constrained Vector Quantization Error) algorithm: Conduct k-means clustering while enforcing constraint violation penalties.</p>
            <p>If we have some penalties, we can increase the distance between an object and the centroid, increasing the cost function.</p>
            <p>For instance, we can exploit as <strong>objective function</strong> the sum of distance used in k-means, adjusted by the constraint violation penalties.</p>
            <ul>
            <li><p>Penalty of a must-link violation</p>
            <p>If objects x and y must-be-linked but they are assigned to two different centers, c1 and c2, dist(c1, c2) is added to the objective function as the penalty, we increase the cost function.</p></li>
            <li><p>Penalty of a cannot-link violation</p>
            <p>If objects x and y cannot-be-linked but they are assigned to a common center c, dist(c, c′), between c and c′ is added to the objective function as the penalty, where c′ is the closest cluster to c that can accommodate x or y</p></li>
            </ul>
            <p>All solutions are feasible and we just have an optimization problem.</p>
            <h2 data-number="8.3" id="speeding-up-constrained-clustering"><span class="header-section-number">8.3</span> Speeding Up Constrained Clustering</h2>
            <p>It is costly to compute some constrained clusteringAn interest application is to perform clustering with obstacle objects: Spatial clustering in the presence of obstacles.</p>
            <p>Let’s suppose we want to cluster people as moving objects in a plaza. Euclidean distance is used to measure the walking distance. However, constraint on similarity measurement is that the trajectory implementing the shortest distance cannot cross a wall.</p>
            <p>If we consider only the Euclidian distance we cannot considerate obstacles.Distance has to be derived by geometric computations: the computational cost is high if a large number of objects and obstacles are involved.</p>
            <p>A point p is visible from another point q if the straightline joining p and q does not intersect any obstacles.</p>
            <p>We can create a <strong>visibility graph</strong> to perform this type of clustering and it is the graph, VG = (V,E), such that each vertex of the obstacles has a corresponding node in V and two nodes, v1 and v2, in V are joined by an edge in E if and only if the corresponding vertices they represent are visible to each other. Let VG’ = (V’,E’) be a visibility graph created from VG by adding two additional points, p and q, in V’. E’ contains an edge joining two points in V’ if the two points are mutually visible. The shortest path between two points, p and q, will be a subpath of VG’.</p>
            <p><img src="../media/image522.png" /></p>
            <p>We just consider all vertexes visible to each other, so that if we want to compute the shortest path we can create it just considering this graph.</p>
            <p>To reduce the cost of distance computation between any two pairs of objects or points, several pre-processing and optimization techniques can be used. One method groups points that are close together into microclusters.</p>
            <p>This can be done by first triangulating the region R into triangles, and then grouping nearby points in the same triangle into microclusters, using a method similar to BIRCH or DBSCAN.</p>
            <p><img src="../media/image523.png" /></p>
            <p>By processing microclusters rather than individual points, the overall computation is reduced. After that, precomputation can be performed to build two kinds of join indices based on the computation of the shortest paths:</p>
            <ol type="1">
            <li><p>VV indices, for any pair of obstacle vertices, and</p></li>
            <li><p>MV indices, for any pair of microcluster and obstacle vertex.</p></li>
            </ol>
            <p>Use of the indices helps further optimize the overall performance.</p>
            <p>Using such precomputation and optimization strategies, the distance between any two points (at the granularity level of a microcluster) can be computed efficiently.</p>
            <p>Thus, the clustering process can be performed in a manner similar to a typical efficient k-medoids algorithm, such as CLARANS, and achieve good clustering quality for large data sets.</p>
            <p>This pre-computation is useful to speed-up the computation of similarity between points and then we can apply a clustering algorithm.</p>
            <p>Here is the comparison of not taking obstacles into account and taking them.</p>
            <table>
            <thead>
            <tr class="header">
            <th><strong>Not</strong> taking obstacles into account</th>
            <th>Taking obstacles into account</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image524.png" /></td>
            <td><img src="../media/image525.png" /></td>
            </tr>
            </tbody>
            </table>
            <h1 data-number="9" id="outlier-analysis-1"><span class="header-section-number">9</span> Outlier Analysis</h1>
            <p>Maintenance is performed with repetition or in front of some conditions.</p>
            <p>Conditional based maintenance is becoming popular, if we realize that somethings are changing we need to predict possible faults.</p>
            <p>The first step is to detect faults thinking to achieve possible deviation of the normal behavior so to stop the machine before arriving to the fault.</p>
            <p><strong>Outlier</strong>: A data object that deviates significantly from the normal objects as if it were generated by a different mechanism.</p>
            <p>Ex.: Unusual credit card purchase, sports: Michael Jordon, …</p>
            <p>Outliers are different from the noise data.</p>
            <p>Noise is random error or variance in a measured variable.</p>
            <p>Noise should be removed before outlier detection.</p>
            <p>If we use smoothing filter with wide window we have the risk to let an outlier disappear, it can avoid us to find the outlier.</p>
            <p><strong>Outliers are interesting</strong>: They violate the mechanism that generates the normal data</p>
            <p><strong>Outlier detection vs. novelty detection</strong>: early stage, outlier; but later merged into the model. We can have a new trend that starts at the beginning as an outlier but then become a trend. It’s needed to understand when this novel start.</p>
            <h2 data-number="9.1" id="types-of-outliers"><span class="header-section-number">9.1</span> Types of Outliers</h2>
            <p><strong>Three kinds of outliers</strong>: global, contextual and collective outliers.</p>
            <h3 data-number="9.1.1" id="global-outlier-or-point-anomaly"><span class="header-section-number">9.1.1</span> <strong>Global outlier</strong> (or point anomaly):</h3>
            <p>Object is <span class="math inline">\(O_g\)</span> if it significantly deviates from the rest of the data set.</p>
            <p>Ex. Intrusion detection in computer networks, fault detection in industry. The behavior is completely different from the normal behavior.</p>
            <p>Issue: We need to find an <strong>appropriate measurement of deviation</strong>.</p>
            <p>How significantly must it deviates?</p>
            <h3 data-number="9.1.2" id="contextual-outlier-or-conditional-outlier"><span class="header-section-number">9.1.2</span> <strong>Contextual outlier</strong> (or conditional outlier)</h3>
            <p>Object is <span class="math inline">\(O_c\)</span> if it deviates significantly based on a selected context</p>
            <p>Ex. 35<sup>o</sup> C in Pisa: without telling what is the season it cannot be an outlier (depending on summer or winter?)</p>
            <h4 class="unnumbered" data-number="" id="attributes-of-data-objects-should-be-divided-into-two-groups.">Attributes of data objects should be divided into two groups.</h4>
            <p>We have:</p>
            <ul>
            <li><p><strong>Contextual attributes</strong>: defines the context, e.g., time &amp; location.</p></li>
            <li><p><strong>Behavioral attributes</strong>: characteristics of the object, used in outlier evaluation, e.g., temperature. We need to identify what is the context and what is the behavior.</p></li>
            </ul>
            <p>Contextual outliers can be viewed as a generalization of local outliers—whose density significantly deviates from its local area.</p>
            <p>Issue: <strong>How to define or formulate meaningful context?</strong></p>
            <p><img src="../media/image526.png" /></p>
            <h3 data-number="9.1.3" id="collective-outliers"><span class="header-section-number">9.1.3</span> <strong>Collective Outliers</strong></h3>
            <p>They consist on a subset of data objects collectively deviate significantly from the whole data set, even if the individual data objects may not be outliers.</p>
            <p>We consider a cluster of objects that deviates significantly from all other objects.</p>
            <p>We have objects with a similar behavior but completely different from the normality.</p>
            <p>Applications: E.g., intrusion detection: When a number of computers keep sending denial-of-service packages to each other.</p>
            <p>Detection of collective outliers has to consider not only behavior of individual objects, but also that of groups of objects. We need to have the background knowledge on the relationship among data objects, such as a distance or similarity measure on objects.</p>
            <p><img src="../media/image527.png" /></p>
            <p>A dataset may have multiple types of outliers and one object may belong to more than one type of outlier.</p>
            <h2 data-number="9.2" id="challenges-of-outlier-detection"><span class="header-section-number">9.2</span> Challenges of Outlier Detection</h2>
            <p>Challenges of outlier detection are:</p>
            <ul>
            <li><p><strong>Modeling normal objects and outliers properly</strong> <span class="math inline">\(\rightarrow\)</span> Hard to enumerate all possible normal behaviors in an application. The border between normal and outlier objects is often a gray area, it’s not easy to define a threshold in distance to define an object as outlier.</p></li>
            <li><p><strong>Application-specific outlier detection</strong> <span class="math inline">\(\rightarrow\)</span> Choice of distance measure among objects and the model of relationship among objects are often application-dependent. E.g., clinic data: a small deviation could be an outlier; while in marketing analysis, we need to have larger fluctuations</p></li>
            <li><p><strong>Handling noise in outlier detection</strong> <span class="math inline">\(\rightarrow\)</span> Noise may distort the normal objects and blur the distinction between normal objects and outliers. It may help hide outliers and reduce the effectiveness of outlier detection. This is complex because performing noise reduction we need to balance the reduction of noise and the reduction of outliers.</p></li>
            <li><p><strong>Understandability</strong> <span class="math inline">\(\rightarrow\)</span> Understand why these are outliers: Justification of the detection. We should also specify the degree of an outlier: the unlikelihood of the object being generated by a normal mechanism</p></li>
            </ul>
            <p>The result we will obtain will depend on parameters, for example eps and minPts with DBSCAN.</p>
            <h2 data-number="9.3" id="outlier-detection"><span class="header-section-number">9.3</span> Outlier Detection</h2>
            <p>We have two ways to categorize outlier detection methods:</p>
            <ul>
            <li><p>Based on <strong>whether user-labeled examples of outliers can be obtained</strong>: Supervised, semi-supervised vs. unsupervised methods. We can use supervision if we have labelled objects.</p></li>
            <li><p>Categorize based on <strong>assumptions about normal data and outliers</strong>: Statistical, proximity-based, and clustering-based methods.</p></li>
            </ul>
            <h3 data-number="9.3.1" id="supervised-methods"><span class="header-section-number">9.3.1</span> <strong>Supervised Methods</strong></h3>
            <p>Since we have labelled data, we try to modeling outlier detection is considered as a classification problem.Samples are examined by domain experts and used for training &amp; testing.</p>
            <p>Methods for Learning a classifier for outlier detection effectively:</p>
            <ul>
            <li><p>Model normal objects &amp; report those not matching the model as outliers, or</p></li>
            <li><p>Model outliers and treat those not matching the model as normal</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="challenges"><strong>Challenges</strong>:</h4>
            <ul>
            <li><p>We need to work with <strong>imbalanced classes</strong>, i.e., <strong>outliers are rare</strong>. The number of normal objects is typically higher than the number of outliers. We will boost the outlier class and make up some artificial outliers to avoid the bias</p></li>
            <li><p><strong>Catch as many outliers as possible</strong>, so recall is more important than accuracy (i.e., not mislabeling normal objects as outliers)</p></li>
            </ul>
            <h3 data-number="9.3.2" id="unsupervised-methods"><span class="header-section-number">9.3.2</span> <strong>Unsupervised Methods</strong></h3>
            <p>We assume the normal objects are somewhat clustered into multiple groups, each having some distinct features.</p>
            <p>An outlier is expected to be far away from any groups of normal objects. We generate custers and if an object is different than all clusters we can assume it as outliers.Weakness:</p>
            <ul>
            <li><p><strong>Weakness</strong>: Cannot detect collective outlier effectively, we collect clusters</p></li>
            <li><p>Normal objects may not share any strong patterns, but the collective outliers may share high similarity in a small area, and identify them as clusters</p></li>
            <li><p><strong>Unsupervised methods may have a high false positive rate</strong> but still miss many real outliers.</p></li>
            <li><p><strong>Supervised methods can be more effective</strong>, e.g., identify attacking some key resources</p></li>
            </ul>
            <p>Many clustering methods can be adapted for unsupervised methods:</p>
            <ul>
            <li><p><strong>Find clusters, then outliers</strong>: not belonging to any cluster</p></li>
            <li><p><strong>Problem 1</strong>: Hard to distinguish noise from outliers</p></li>
            <li><p><strong>Problem 2</strong>: Costly since first clustering and then determine outliers: but far less outliers than normal objects.</p>
            <ul>
            <li>Newer methods: <strong>tackle outliers directly</strong>, for example DBSCAN</li>
            </ul></li>
            </ul>
            <h3 data-number="9.3.3" id="semi-supervised-methods"><span class="header-section-number">9.3.3</span> <strong>Semi-supervised methods</strong></h3>
            <p>In many applications, <strong>the number of labeled data is often small</strong>: labels could be on outliers only, normal objects only, or both.</p>
            <p>Semi-supervised outlier detection: Regarded as applications of semi-supervised learning.</p>
            <p>We exploit unsupervised methods but also labeled data.</p>
            <p>If some labeled normal objects are available.</p>
            <p>Use the labeled examples and the proximate unlabeled objects to train a model for normal objects.</p>
            <p>Those not fitting the model of normal objects are detected as outliers.</p>
            <p>We try to classify only one class, and what’s not classified as belonging to the class is considered outlier.</p>
            <p>If only some labeled outliers are available, a small number of labeled outliers many do not cover the possible outliers well.</p>
            <p>To improve the quality of outlier detection, one can get help from models for normal objects learned from unsupervised methods.</p>
            <h3 data-number="9.3.4" id="statistical-techniques-also-known-as-model-based-methods"><span class="header-section-number">9.3.4</span> <strong>Statistical techniques</strong> (also known as model-based methods)</h3>
            <p><strong>We assume that the normal data follow some statistical model</strong> (a stochastic model) and the data not following the model are outliers.</p>
            <p>We determine the distribution of normal data where we think we have the normal behavior and if objects are in the skew of a gaussian distribution are considered outliers.</p>
            <p>Example:</p>
            <ul>
            <li><p>First use Gaussian distribution to model the normal data</p></li>
            <li><p>For each object y in region R, estimate g<sub>D</sub>(y), the probability of y fits the Gaussian distribution</p></li>
            <li><p>If g<sub>D</sub>(y) is very low, y is unlikely generated by the Gaussian model, thus it is an outlier</p></li>
            </ul>
            <p><img src="../media/image528.png" /></p>
            <p>Effectiveness of statistical methods:</p>
            <p>highly depends on whether the assumption of statistical model holds in the real data.</p>
            <p>There are rich alternatives to use various statistical models</p>
            <p>E.g., parametric vs. non-parametric</p>
            <p>This is parametric, we assume the distribution is gaussian, we compute mean and std with the data available and consider a point as an outlier if it has a low probability to belong to the distribution.</p>
            <p>We have also nonparametric methods.</p>
            <h3 data-number="9.3.5" id="proximity-based-methods"><span class="header-section-number">9.3.5</span> <strong>Proximity-based methods</strong></h3>
            <p>An object is an outlier if the <strong>nearest neighbors of the object are far away</strong>, i.e., the proximity of the object significantly deviates from the proximity of most of the other objects in the same data set</p>
            <p><img src="../media/image528.png" /></p>
            <p>Example: We model the proximity of an object using its 3 nearest neighbors.</p>
            <p>Objects in region R are substantially different from other objects in the data set.</p>
            <p>Thus the objects in R are outliers.</p>
            <p>We consider here 3 neighbors computing the proximity with the 3 nearest neighbors.</p>
            <p>The effectiveness of proximity-based methods highly relies on the proximity measure.</p>
            <p>In some applications, proximity or distance measures cannot be obtained easily.</p>
            <p>Often have a difficulty in finding a group of outliers which stay close to each other.</p>
            <p>Two major types of proximity-based outlier detection are distance-based vs. density-based.</p>
            <h3 data-number="9.3.6" id="clustering-based-methods"><span class="header-section-number">9.3.6</span> <strong>Clustering-based methods</strong></h3>
            <p><strong>Normal data belong to large and dense clusters</strong>, whereas outliers belong to small or sparse clusters, or do not belong to any clusters.</p>
            <p>Example: two clusters</p>
            <p><img src="../media/image528.png" /></p>
            <p>All points not in R form a large cluster, the two points in R form a tiny cluster, thus are outliers.</p>
            <p>Since there are many clustering methods, there are many clustering-based outlier detection methods as well.</p>
            <p>We have to determine a threshold, when I consider if a point is a member of the cluster or an outlier.</p>
            <p>In k-means all points will belong to one cluster, but when we have an outlier, we expect it will be far away from the center of the cluster.</p>
            <p>Executing clustering algorithms is expensive: straightforward adaptation of a clustering method for outlier detection can be costly and does not scale up well for large data sets.</p>
            <h2 data-number="9.4" id="statistical-approaches"><span class="header-section-number">9.4</span> Statistical approaches</h2>
            <p>In statistical approaches we assume that the objects in a dataset are generated by a stochastic process (a generative model).</p>
            <p>The idea is to learn a generative model fitting the given dataset, and then identify the objects in low probability regions of the model as outliers.</p>
            <p>We learn the model using data and consider objects in low probability regions as outliers, they will be far from normal distribution.</p>
            <p>Methods are divided into two categories: parametric vs. non-parametric.</p>
            <ul>
            <li><p>Parametric method: we assume some distribution and we use the data to determine the parameters of the distribution</p>
            <p>For example we assume that the normal data is generated by a parametric distribution with parameter θ.</p>
            <p>The probability density function of the parametric distribution f(x, θ) gives the probability that object x is generated by the distribution, the smaller this value, the more likely x is an outlier.</p></li>
            <li><p>Non-parametric method: We do not assume an a-priori statistical model and determine the model from the input data. Not completely parameter free but consider the number and nature of the parameters are flexible and not fixed in advance.</p>
            <p>Examples: histogram and kernel density estimation They are not completely parameter free because in an histogram we have to fix the width of the bar, while in kernel we have to choose the kind of kernel to use.</p></li>
            </ul>
            <h3 data-number="9.4.1" id="univariate-data-dataset-involving-only-one-attribute-or-variable"><span class="header-section-number">9.4.1</span> <strong>Univariate data</strong> (dataset involving only one attribute or variable)</h3>
            <p>Often assume that data are generated from a normal distribution, learn the parameters from the input data, and identify the points with low probability as outliers.</p>
            <p>How is it possible to determine whether a distribution is normal?</p>
            <ul>
            <li><p><strong>Shapiro-Wilk Test</strong>: test for verifying whether a random sample comes from a normal distribution</p></li>
            <li><p><strong>Q-Q (Quantile-Quantile) plot</strong>: in general, this plot can be used to estimate whether a random sample comes from a continuous distribution (for instance, a normal distribution) as long as the quantiles can be calculated</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="q-q-plot"><strong>Q-Q plot</strong></h4>
            <p>Plots of two <strong>quantiles</strong> against each other.</p>
            <p>Recall: a quantile is a fraction where certain values fall below that quantile.</p>
            <p>To verify whether a random sample comes from a normal distribution we plot the quantiles corresponding to the random samples against the quantile of a normal distribution.</p>
            <p>If the Q-Q plot is along the line the distributions are similar and I can conclude the distribution under test is normal.</p>
            <h4 class="unnumbered" data-number="" id="an-example"><strong>An example</strong></h4>
            <p>Let’s assume that we have to determine whether the following samples from a normal distribution: 7.19, 6.31, 5.89, 4.5, 3.77, 4.25, 5.19, 5.79, 6.79</p>
            <p>We have an univariate domain. We adopt the following procedure:</p>
            <p><strong>Step 1</strong>. Sort the values from the smallest to the largest:</p>
            <p>3.77, 4.25, 4.50, 5.19, 5.79, 5.89, 6.31, 6.79, 7.19</p>
            <p><strong>Step 2</strong>. Draw a normal distribution curve and divide it into n+1 segments, where n is the number of values. In our example</p>
            <p><img src="../media/image530.png" /></p>
            <p>If this is the distribution we want to analyze if it’s similar to a gaussian distribution. We split the normal distribution curve and each segment in this case is 10% of the total area.</p>
            <p><strong>Step 3</strong>. Find the z-value(cut-off point for each segment).Recall the z-value is the number of standard deviations from the mean a data point is:</p>
            <p><img src="../media/image530.png" /></p>
            <p>If we select a value we have x% of the values in that side.</p>
            <p><strong>Step 4</strong>. Plot the dataset values against the normal distribution cut-off points.</p>
            <p><img src="../media/image531.png" /></p>
            <p>A (almost) straight line on the Q-Q plot indicates that the data distribution is approximately normal.</p>
            <p>We have a strong correlation between our values and the distribution we obtain using the normal distribution, so an high probability our distribution is close to the normal one.</p>
            <p>If we have one point vary far from the mean the probability it is an outlier is higher.</p>
            <h4 class="unnumbered" data-number="" id="methods"><strong>Methods</strong></h4>
            <p><strong>Method 1</strong>: model the normal distribution and consider the probability of the points to belong to this distribution</p>
            <p>Ex: Average temperature: {24.0, 28.9, 28.9, 29.0, 29.1, 29.1, 29.2, 29.2, 29.3, 29.4} Use the maximum likelihood method to estimate μ and σ</p>
            <p>The probability that a point xi is generated by the model is</p>
            <p><span class="math display">\[
                \Large{P(x_i|\sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}e^{-\dfrac{(x_i-\mu)^2}{2\sigma^2}}}
            \]</span></p>
            <p>Consequently, the likelihood that X is generated by the model is</p>
            <p><span class="math display">\[
                \Large{L(\mathcal{N}(\mu, \sigma^2):X) = P(X|\mu, \sigma^2) = \prod_{i = 1}^{n}{\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}}}
            \]</span></p>
            <p>We can compute mean and std to fix parameters of our std.</p>
            <p>Since we have an univariate distribution the outlier must have max or min value.</p>
            <p>For the above data with n = 10, we have</p>
            <p><span class="math display">\[
                \large{\hat{\mu} = 28.61 \hspace{2cm} \hat{\sigma} = \sqrt{2.29} = 1.51}
            \]</span></p>
            <p>We know that the region between <span class="math inline">\(\mu \pm 3\sigma\)</span> contains 99.7% of data</p>
            <p>Then <span class="math inline">\(\dfrac{24 - 28.61}{1.51} = - 3.04 &lt; -3\)</span>. Thus, 24 is an outlier since it is far then <span class="math inline">\(3\sigma\)</span>, it is in the skew of gaussian distribution.</p>
            <p>If the distribution is normal, we use the data to compute mean and std and then we just consider the probability of each point on the distribution or we just subtract mean and divide for std. If we find a value higher/lower our threshold we know that it is an outlier,</p>
            <p><strong>Method 2</strong>: The Grubb's test (maximum normed residual test) ─ another statistical method under normal distribution. The test finds whether a minimum value or a maximum value is an outlier. In an univariate we know that an outlier must be there if there is one.</p>
            <p>The test checks for outliers by looking for the maximum of the absolute differences between the values and the mean. Basically, the steps are:</p>
            <ol type="1">
            <li><p>Find the G test statistic.</p>
            <ol type="a">
            <li><p> Order the data points from smallest to largest.</p></li>
            <li><p> Find the mean (<span class="math inline">\(\bar{Y}\)</span>) and standard deviation (s) of the data set.</p></li>
            </ol>
            <p>c)  Calculate the G test statistic using one of the following equations: The Grubbs’ test statistic for a two-tailed test is:</p>
            <p><span class="math display">\[
                 G = \dfrac{\max\limits_{i=1,...,N}{\left|Y_i-\bar{Y}\right|}}{s}
             \]</span></p>
            <p>We determine the maximum and with it we just consider the following table.</p></li>
            <li><p>Find the G Critical Value.</p>
            <p>Several tables exist for finding the critical value for Grubbs’ test. The table is a partial table for several G critical values and alpha levels (confidence values).</p>
            <table>
            <thead>
            <tr class="header">
            <th>N</th>
            <th>0.1</th>
            <th>0.075</th>
            <th>0.05</th>
            <th>0.025</th>
            <th>0.01</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>3</td>
            <td>1.15</td>
            <td>1.15</td>
            <td>1.15</td>
            <td>1.15</td>
            <td>1.15</td>
            </tr>
            <tr class="even">
            <td>4</td>
            <td>1.42</td>
            <td>1.44</td>
            <td>1.46</td>
            <td>1.48</td>
            <td>1.49</td>
            </tr>
            <tr class="odd">
            <td>5</td>
            <td>1.6</td>
            <td>1.64</td>
            <td>1.67</td>
            <td>1.71</td>
            <td>1.75</td>
            </tr>
            <tr class="even">
            <td>6</td>
            <td>1.73</td>
            <td>1.77</td>
            <td>1.82</td>
            <td>1.89</td>
            <td>1.94</td>
            </tr>
            <tr class="odd">
            <td>7</td>
            <td>1.83</td>
            <td>1.88</td>
            <td>1.94</td>
            <td>2.02</td>
            <td>2.1</td>
            </tr>
            <tr class="even">
            <td>8</td>
            <td>1.91</td>
            <td>1.96</td>
            <td>2.03</td>
            <td>2.13</td>
            <td>2.22</td>
            </tr>
            <tr class="odd">
            <td>9</td>
            <td>1.98</td>
            <td>2.04</td>
            <td>2.11</td>
            <td>2.21</td>
            <td>2.32</td>
            </tr>
            <tr class="even">
            <td>10</td>
            <td>2.03</td>
            <td>2.1</td>
            <td>2.18</td>
            <td>2.29</td>
            <td>2.41</td>
            </tr>
            <tr class="odd">
            <td>11</td>
            <td>2.09</td>
            <td>2.14</td>
            <td>2.23</td>
            <td>2.36</td>
            <td>2.48</td>
            </tr>
            <tr class="even">
            <td>12</td>
            <td>2.13</td>
            <td>2.2</td>
            <td>2.29</td>
            <td>2.41</td>
            <td>2.55</td>
            </tr>
            <tr class="odd">
            <td>13</td>
            <td>2.17</td>
            <td>2.24</td>
            <td>2.33</td>
            <td>2.46</td>
            <td>2.61</td>
            </tr>
            <tr class="even">
            <td>14</td>
            <td>2.21</td>
            <td>2.28</td>
            <td>2.37</td>
            <td>2.51</td>
            <td>2.66</td>
            </tr>
            <tr class="odd">
            <td>15</td>
            <td>2.25</td>
            <td>2.32</td>
            <td>2.41</td>
            <td>2.55</td>
            <td>2.71</td>
            </tr>
            <tr class="even">
            <td>16</td>
            <td>2.28</td>
            <td>2.35</td>
            <td>2.44</td>
            <td>2.59</td>
            <td>2.75</td>
            </tr>
            <tr class="odd">
            <td>17</td>
            <td>2.31</td>
            <td>2.38</td>
            <td>2.47</td>
            <td>2.62</td>
            <td>2.79</td>
            </tr>
            </tbody>
            </table>
            <p>Manually, you can find the G critical value</p>
            <p><span class="math display">\[
                \large{G &gt; \dfrac{N-1}{\sqrt{N}}\sqrt{\dfrac{t^2_{\alpha/2N,N-2}}{N-2+t^2_{\alpha/2N,N-2}}}}
            \]</span></p>
            <p>Where <span class="math inline">\(t^2_{\alpha/2N,N-2}\)</span> is the <strong>value taken by a t-distribution with (N-2) degrees of freedom at a significance level of α/(2N)</strong>. We consider the confidence level we want to have and N and if the G-value computed is higher than the one in the table, the point is an outlier.</p></li>
            <li><p>Compare the Gtest statistic to the G critical value</p>
            <ol type="a">
            <li><p> <span class="math inline">\(G_{test} &lt; G_{critical}\)</span>: keep the sample in the data set; it is not an outlier for the specific confidence level we set.</p></li>
            <li><p> <span class="math inline">\(G_{test} \ge G_{critical}\)</span>: reject the sample as an outlier</p></li>
            </ol></li>
            <li><p>Reject the point as an outlier if the test statistic is greater than the critical value.</p></li>
            </ol>
            <h4 class="unnumbered" data-number="" id="example-16">Example</h4>
            <p><img src="../media/image540.png" /></p>
            <p>In this example we have different values and the summarizing values in the table.</p>
            <p>We computed G for the specific dataset and we have alpha and N.</p>
            <p>In the table, we find a <span class="math inline">\(G_{crit} = 2.23\)</span>. Thus, <span class="math inline">\(G &gt; G_{crit}\)</span> and therefore G is an outlier.</p>
            <p>All this with a normal distribution.</p>
            <h3 data-number="9.4.2" id="multivariate-outliers"><span class="header-section-number">9.4.2</span> <strong>Multivariate outliers</strong></h3>
            <p>Multivariate data is a data set involving two or more attributes or variables.</p>
            <p>We have to transform the multivariate outlier detection task into a univariate outlier detection problem</p>
            <h4 class="unnumbered" data-number="" id="methods-1"><strong>Methods</strong></h4>
            <p><strong>Method 1</strong>. Use the <strong>Mahalanobis distance</strong></p>
            <p>Mahalanobis’ distance is the distance between a point and a distribution. And not between two distinct points. It is effectively a <strong>multivariate equivalent of the Euclidean distance</strong>.</p>
            <p>Actually, the Euclidean distance work fine as long as the dimensions are equally weighted and are independent of each other.</p>
            <p>It weights more some dimensions respect to other.</p>
            <p><img src="../media/image541.png" /></p>
            <p>Equally weighted. The two following tables show the ‘area’ and ‘price’ of the same objects. Only the units of the variables change. Nevertheless, the distances between any two rows are different just changing units.</p>
            <p>The problem can be overcome by scaling the variables, by computing the <span class="math inline">\(z\text{-}score = \dfrac{x - mean}{\sigma}\)</span> or making it vary within a specific range (for instance, between 0 and 1).</p>
            <p>In this way the distance we compute is the same for the two columns.</p>
            <p><strong>Independent of each other</strong>. The Euclidean distance between a point and the center of the points (distribution) can give little or misleading information about how close a point really is to the cluster.</p>
            <p><img src="../media/image542.png" /></p>
            <p>When A and B are correlate the Euclidian distance makes sense, while it doesn’t in the second figure. If I consider the center of the center and point 1 and 2 the distance are similar. But differently from the first case point 1 is in the distribution while point 2 is not.</p>
            <p>I weight in the same way all dimensions. When the variables X and Y are correlated Point 1 is closer to the cluster than point 2</p>
            <p><strong>How is the Mahalanobis distance different from the Euclidean distance?</strong></p>
            <ul>
            <li><p>Transforms the columns into uncorrelated variables</p></li>
            <li><p>Scale the columns to make their variance equal to 1</p></li>
            <li><p>Finally, it calculates the Euclidean distance</p></li>
            </ul>
            <p>We transform in uncorrelated distances so that the euclidian distance makes sense.</p>
            <p>Let ō be the mean vector for a multivariate data set. The Mahalanobis distance for an object o to ō is defined as</p>
            <p><span class="math display">\[
                MDist^2(o,\bar{o}) = (o-\bar{o})^TS^{-1}(o-\bar{o})
            \]</span></p>
            <p>where <span class="math inline">\(S\)</span> is the <strong>covariance matrix</strong>.</p>
            <p>To divide by the covariance matrix is essentially a multivariate equivalent of the regular standardization <span class="math inline">\(z = \dfrac{x - mean}{\sigma}\)</span> , they will vary in all dimensions in the same way.</p>
            <p>If the variables are strongly correlated, then the covariance will be high and the distance will be reduced; otherwise, the covariance will be low and the distance will not be reduced.</p>
            <p>The multivariate outlier detection problem is transformed as follows.</p>
            <ol type="1">
            <li>Calculate the mean vector from the multivariate data set.</li>
            <li>For each object <span class="math inline">\(o\)</span>, calculate <span class="math inline">\(MDist(o,\bar{o})\)</span>, the Mahalanobis distance from <span class="math inline">\(o\)</span> to <span class="math inline">\(\bar{o}\)</span>.</li>
            <li>Detect outliers in the transformed univariate data set, {<span class="math inline">\(MDist(o, \bar{o})|o \in D\)</span>}.</li>
            <li>if <span class="math inline">\(MDist(o,\bar{o})\)</span> is determined to be an outlier, then <span class="math inline">\(o\)</span> is regarded as an outlier as well.</li>
            </ol>
            <p>The distance involves mean vector of all objects.</p>
            <p>We replace a set of objects by considering the set of distances of each object considering the mean vector. In this way we transform the multivariate problem of outlier detection in a univariate problem of outlier detection.</p>
            <p>The outlier is the one with the largest distance, it is completely different from others and so it’s very far from the mean vector.</p>
            <p>With this we replace all objects with the corresponding distances going to an univariate problem from a multivariate one, we can use this distance to pass to univariate outliers.</p>
            <p>We can use the <strong>Grubb's test</strong> on this measure to detect outliers now.</p>
            <p>If we have a normal distribution we can apply the test seen before.</p>
            <p><strong>Weaknesses</strong>:</p>
            <ul>
            <li><p>Computationally heavy (covariance matrix and its inverse)</p></li>
            <li><p>Needs to store the covariance matrix and its inverse</p></li>
            </ul>
            <p><strong>Method 2</strong>. Use χ2-statistic:</p>
            <p>Assumption: the population of O follows a multivariate distribution with the mean vector ō and the covariance matrix S.</p>
            <p>The method exploits a distance measure based on the chi-square test statistic</p>
            <p><span class="math display">\[
                \chi^2 = \sum_{i=1}^n{\dfrac{(o_i - E_i)^2}{E_i}}
            \]</span></p>
            <p>where <span class="math inline">\(o_i\)</span> and <span class="math inline">\(E_i\)</span> are the observed value, and the expected value of the ith variable and <span class="math inline">\(n\)</span> is the number of variables. Using the average values as estimates of the expectation, we have</p>
            <p><span class="math display">\[
                X^2 = \sum_{i=1}^n{\dfrac{(X_i-\bar{X_i})^2}{\bar{X_i}}}
            \]</span></p>
            <p>According to the <strong>central limit theorem</strong>, when the number of variables is large enough (i.e., greater than 30), <span class="math inline">\(\chi^2\)</span> as the sum of squared differences between the observed and the expected values of those variables has approximately a normal distribution. Since we are interested in detecting significantly large <span class="math inline">\(\chi^2\)</span> values for intrusion detection, we need to set only the upper control limit <span class="math inline">\(\bar{\chi}^2 + 3S_{X^2}\)</span>, that is, if the computed <span class="math inline">\(\chi^2\)</span> for an observation is greater than <span class="math inline">\(\bar{\chi}^2 + 3S_{X^2}\)</span> we signal an anomaly, where <span class="math inline">\(X^2\)</span> is the mean. We transform again the multivariate problem to a univariate problem. The expected value is the mean one in the formula, so it’s similar to the Mahalaobis distance.</p>
            <h3 data-number="9.4.3" id="non-parametric-methods"><span class="header-section-number">9.4.3</span> <strong>Non-parametric methods</strong></h3>
            <p>Here, the model of normal data is learned from the input data without any a priori structure.</p>
            <p>We don’t assume the type of distribution.</p>
            <p>Often makes fewer assumptions about the data, and thus can be applicable in more scenarios.</p>
            <h4 class="unnumbered" data-number="" id="outlier-detection-using-histogram">Outlier detection using <strong>histogram</strong></h4>
            <p><img src="../media/image547.png" /></p>
            <p>The figure shows the histogram of purchase amounts in transactions.</p>
            <p>A transaction in the amount of $7,500 is an outlier, since only 0.2% transactions have an amount higher than $5,000.</p>
            <p>We can create this histogram because we know the distribution of data. Problem: Hard to choose an appropriate bin size for histogram.</p>
            <p>Too small bin size <span class="math inline">\(\rightarrow\)</span> normal objects in empty/rare bins, false positive</p>
            <p>Too big bin size <span class="math inline">\(\rightarrow\)</span> outliers in some frequent bins, we will have false negative.</p>
            <p><strong>Solution</strong>: Adopt kernel density estimation to estimate the probability density distribution of the data. If the estimated density function is high, the object is likely normal. Otherwise, it is likely an outlier.</p>
            <p>We don’t fix any distribution as in parametric methods but we make some assumptions in the size of the bin.</p>
            <p>We can also use <strong>Kernel Density Estimation</strong>, where we fix kernel function and we determine the distribution using it.</p>
            <p><span class="math display">\[
                \hat f_h(x) = \dfrac{1}{n}\sum_{i=1}^{n}{K_h\left(x-x_i\right)} = \dfrac{1}{nh}\sum_{i=1}^{n}{K_h\left(\dfrac{x-x_i}{h}\right)}
            \]</span></p>
            <p>where <span class="math inline">\(K(•)\)</span> is the kernel — a symmetric but not necessarily positive function that integrates to one — and <span class="math inline">\(h &gt; 0\)</span> is a smoothing parameter called the bandwidth.</p>
            <p><img src="../media/image549.png" /></p>
            <h4 class="unnumbered" data-number="" id="computational-cost"><strong>Computational Cost</strong></h4>
            <p>The computational cost of statistical methods depends on the models.</p>
            <p>Simple parametric models (e.g., a Gaussian) have linear time.</p>
            <p>More sophisticated models (e.g., mixture models, where the Expectation Maximization (EM) algorithm is used in learning) several iterations.</p>
            <p>Each iteration, however, is typically linear considering the data set’s size. For kernel density estimation, the model learning cost can be up to quadratic. Once the model is learned, the outlier detection cost is often very small per object.</p>
            <h2 data-number="9.5" id="proximity-based-approaches"><span class="header-section-number">9.5</span> Proximity-Based Approaches</h2>
            <p>Intuition: <strong>Objects that are far away from the others are outliers</strong>.</p>
            <p>Assumption of proximity-based approach: The proximity of an outlier deviates significantly from that of most of the others in the data set.</p>
            <p>Two types of proximity-based outlier detection methods:</p>
            <ul>
            <li><p><strong>Distance-based outlier detection</strong>: An object o is an outlier if its neighborhood does not have enough other points, we set this parameter.</p></li>
            <li><p><strong>Density-based outlier detection</strong>: An object o is an outlier if its density is relatively much lower than that of its neighbors. We reason about proximity in relation with the neighbors.</p>
            <p>Using the idea of density if we hare in a sparse region we cannot expect points are close to each other but it doesn’t mean points are outliers. Deciding if a point is an outlier depends on what we have around. Outliers are identified in regions with different densities.</p></li>
            </ul>
            <h3 data-number="9.5.1" id="distance-based-outlier-detection"><span class="header-section-number">9.5.1</span> Distance-Based Outlier Detection</h3>
            <p>For each object o, we examine the number of o the r objects in the r-neighborhood of o, where r is a user-specified distance threshold</p>
            <p> An object o is an outlier if most (taking <span class="math inline">\(\pi\)</span> as a fraction threshold) of the objects in D are far away from o, i.e., not in the r-neighborhood of o.</p>
            <p>An object o is a DB(r,<span class="math inline">\(\pi\)</span>) outlier if:</p>
            <p><span class="math display">\[
                \dfrac{\|o&#39;|dist(o,o&#39;) \le r\|}{\|D\|} \le \pi
            \]</span></p>
            <p>We consider an outlier such considering the neighbors of an outlier but we also have two parameters, pi and r.If the number of objects in the neighborhood is lower than a threshold, we have an outlier.</p>
            <p>Equivalently, one can check the distance between o and its k-thnearest neighbor ok, where <span class="math inline">\(k = \bigl\lceil \pi\|D\| \bigr\rceil\)</span>. o is an outlier if <span class="math inline">\(dist(o, o_k) &gt; r\)</span>.</p>
            <p>I compute the number of objects we consider in the neighborhood and use their distance.</p>
            <p>This approach is quite expensive.</p>
            <p>To speed-up computation we adopt a nested loop algorithm.</p>
            <p>For any object oi, calculate its distance from other objects, and count the number of other objects in the r-neighborhood.</p>
            <p>If <span class="math inline">\(\pi\)</span>∙n other objects are within r distance, terminate the inner loop</p>
            <p>Otherwise, <span class="math inline">\(o_i\)</span> is a DB(r, <span class="math inline">\(\pi\)</span>) outlier.</p>
            <p>Instead of calculating distances we limit the computation.</p>
            <p><strong>Efficiency</strong>: Actually, CPU time is not <span class="math inline">\(O(n^2)\)</span> but linear to the data set size since for most non-outlier objects, the inner loop terminates early. With most of the objects this occur, because they’re not outliers.</p>
            <p><img src="../media/image552.png" /></p>
            <p>In the outer loop we just count the inner points with a distance lower than a threshold.</p>
            <p>If we achieve the number, we stop the loop to save computational time.</p>
            <p>The efficiency is still a concern when the complete set of objects cannot be held into main memory, we have the cost I/O swapping.</p>
            <p>We have two observations for the major cost</p>
            <ol type="1">
            <li>each object tests against the whole dataset, why not only its close neighbor? Each objects has to compute the distance with neighbors why not checking only the closer.</li>
            <li>check objects one by one, why not group by group?</li>
            </ol>
            <p>Grid-based method (CELL): Data space is partitioned into a multi-D grid. Each cell is a hyper cube with diagonal length <span class="math inline">\(\dfrac{r}{2}\)</span>.</p>
            <p><img src="../media/image553.png" /></p>
            <h4 class="unnumbered" data-number="" id="pruning-using-the-level-1-level-2-cell-properties">Pruning using the level-1 &amp; level-2 cell properties:</h4>
            <ul>
            <li><p><strong>Level-1</strong>: For any possible point x in cell C and any possible point y in a level-1 cell (one cell away from C) <span class="math inline">\(dist(x,y) \le r\)</span>. Since we choose the diagonal lenght <span class="math inline">\(\dfrac{r}{2}\)</span> we consider just the first level around C.</p></li>
            <li><p><strong>Level-2</strong>: For any possible point x in cell C and any point y in a level-2 cell (two cells away from C) <span class="math inline">\(dist(x,y) \ge r\)</span></p></li>
            </ul>
            <p>We compute three values to speed-up the computation.</p>
            <p>Let <span class="math inline">\(a\)</span> be the number of objects in cell C, <span class="math inline">\(b_1\)</span> be the total number of objects in the level-1 cells, and <span class="math inline">\(b_2\)</span> be the total number of objects in the level-2 cells. We can apply the following rules.</p>
            <ul>
            <li><p><strong>Level-1 cell pruning rule</strong>: Based on the level-1 cell property, if <span class="math inline">\(a+b_1 &gt; [\pi n]\)</span>, then every object o in <span class="math inline">\(C\)</span> is not a DB(r, <span class="math inline">\(\pi\)</span>)-outlier because all those objects in <span class="math inline">\(C\)</span> and the level-1 cells are in the r-neighborhood of o, and there are at least <span class="math inline">\([\pi n]\)</span> such neighbors. We consider the level 1, we know the number of points in <span class="math inline">\(C\)</span> and level-1 cell. Since the diagonal length is <span class="math inline">\(\dfrac{r}{2}\)</span> points in consideration are far at most <span class="math inline">\(r\)</span>.</p></li>
            <li><p><strong>Level-2 cell pruning rule</strong>: Based on the level-2 cell property, if <span class="math inline">\(a+b_1+b_2 &lt; [\pi n] +1\)</span>, then all objects in <span class="math inline">\(C\)</span> are DB(r, <span class="math inline">\(\pi\)</span>)-outliers because each of their r-neighborhoods has less than <span class="math inline">\([\pi n]\)</span> other objects.</p></li>
            </ul>
            <p>Thus, we only need to check the objects that cannot be pruned, and even for such an object o, only need to compute the distance between o and the objects in the level-2 cells (since beyond level-2, the distance from o is more than <span class="math inline">\(r\)</span>).</p>
            <p>We reduce the computation because we can conclude using the two rules when objects in <span class="math inline">\(C\)</span> are certainly and are not outliers.</p>
            <p>It reduces the computation of the distance between the object and other objects in the dataset. We have to prepare the structure with grid and compute sum of objects we have in each cell and have for each the first and second level cells.</p>
            <h4 class="unnumbered" data-number="" id="problem-with-different-densities"><strong>Problem with different densities</strong></h4>
            <p>A problem we may have is that we don’t have equal densities in every part of the space, we work with different densities:</p>
            <p><img src="../media/image554.png" /></p>
            <p>Outlier <span class="math inline">\(o_2\)</span> has similar density as elements of cluster <span class="math inline">\(C_1\)</span>, but <span class="math inline">\(o_2\)</span> is an outlier.</p>
            <p>Solution: approach that <strong>takes in consideration the densities of single points</strong> in the space.</p>
            <p>We can have sparse or very dense zones, and this affect our ability to identify outliers.</p>
            <p><span class="math inline">\(o_1\)</span> is an outlier of course, but it’s not so obvious that <span class="math inline">\(o_2\)</span> is an outlier.</p>
            <p>It is if we observe the distribution of points but if we fix the number of points in the neighborhood thinking about the sparse zone, we include <span class="math inline">\(o_2\)</span> in the cluster.</p>
            <p>If we set parameters thinking about dense zone all <span class="math inline">\(C_1\)</span> points are outliers.</p>
            <p>A solution to take in consideration the neighborhood of a points is the concept of <strong>outlierness</strong>.</p>
            <p>Is point relatively far away from its neighbors?</p>
            <p>Let <span class="math inline">\(N_k(x_i)\)</span> be the k-nearest neighbors of <span class="math inline">\(x_i\)</span>. Let <span class="math inline">\(D_k(x_i)\)</span> be the average distance to k-nearest neighbors</p>
            <p><span class="math display">\[
                \large{D_k(x_i) = \frac{1}{k}\sum_{j \in N_k(x_i)}{\|x_i - x_j\|}}
            \]</span></p>
            <p>We compute the distance between the point and points in its neighborhood, and compute the average.</p>
            <p>Outlierness is the ratio of <span class="math inline">\(D_k(x_i)\)</span> to average <span class="math inline">\(D_k(x_i)\)</span> for its neighbors <span class="math inline">\(j\)</span>:</p>
            <p><span class="math display">\[
                \large{O_k(x_i) = \dfrac{D_k(x_i)}{\dfrac{1}{k}\sum_{j \in N_k(x_i)}{D_k(x_j)}}}
            \]</span></p>
            <p>We compare the average of distance with the average of distance I have for the objects in the neighborhood of <span class="math inline">\(x_i\)</span>.</p>
            <p>If the average distance of the point and its neighbors is higher than the average distance of points in the neighborhood, so if outlierness &gt; 1, <span class="math inline">\(x_i\)</span> is further away from neighbors than expected.</p>
            <p>I can deduce that this can be an outlier and we do it just looking at the neighborhood.</p>
            <p><img src="../media/image557.png" /></p>
            <p>In fact, the average distance of <span class="math inline">\(o_1\)</span> and <span class="math inline">\(o_2\)</span> is greater than the one of their neighbors.</p>
            <p>We can detect outliers in different part of the space considering the points around it, so their context.</p>
            <p>We have to establish a <strong>threshold</strong>; we need to fix the value of outlierness that let us conclude something.</p>
            <p>If clusters are close, outlierness gives unintuitive results:</p>
            <p><img src="../media/image558.png" /></p>
            <p>p has an higher outlierness than q and r, because the objects in the neighborhood of p are object in dense zone, one point is not an outlier because it is in a sparse zone but it is close to a dense zone.</p>
            <p>We have boundary points close to very dense zone.</p>
            <p>The green points are not part of the KNN list of p for small k.</p>
            <h3 data-number="9.5.2" id="density-based-outlier-detection"><span class="header-section-number">9.5.2</span> Density-based Outlier Detection</h3>
            <p><strong>Local outliers</strong>: Outliers comparing to their local neighborhoods, instead of the global data distribution.</p>
            <p><img src="../media/image559.png" /></p>
            <p>In figure, <span class="math inline">\(o_1\)</span> and <span class="math inline">\(o_2\)</span> are local outliers to <span class="math inline">\(C_1\)</span>, <span class="math inline">\(o_3\)</span> is a global outlier, but <span class="math inline">\(o_4\)</span> is not an outlier, because <span class="math inline">\(o_4\)</span> is in a sparse zone. However, proximity-based clustering cannot find that <span class="math inline">\(o_1\)</span> and <span class="math inline">\(o_2\)</span> are outliers (e.g., comparing with <span class="math inline">\(o_4\)</span>).</p>
            <p>Intuition (density-based outlier detection): <strong>The density around an outlier object is significantly different from the density around its neighbors</strong>. Instead of exploiting outlierness in which we work with distance we use also density.</p>
            <p><strong>Method</strong>: Use the relative density of an object against its neighbors as the indicator of the degree of the object being outlier.</p>
            <h4 class="unnumbered" data-number="" id="definitions-2"><strong>Definitions</strong></h4>
            <p>To exploit this concept we introduce:</p>
            <ul>
            <li><p><strong>k-distance of an object o</strong>, <span class="math inline">\(dist_k(o)\)</span>: distance between o and its k-th NN after finding its k nearest neighbors.</p></li>
            <li><p><strong>k-distance neighborhood of o</strong>, <span class="math inline">\(N_k(o) = \{o&#39;|o&#39;\ in\ D,\ dist(o,o&#39;) \le dist_k(o)\}\)</span></p>
            <p>The cardinality of <span class="math inline">\(N_k(o)\)</span> could be bigger than k since multiple objects may have identical distance to o.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="local-outlier-factor"><strong>Local Outlier Factor</strong></h4>
            <p>First, we compute the <strong>reachability distance</strong> from <span class="math inline">\(o&#39;\)</span> to <span class="math inline">\(o\)</span>: <span class="math display">\[
                reachdist_k(o \leftarrow o&#39;) = \max\{dist_k(o), dist(o,o&#39;)\}
            \]</span> where <span class="math inline">\(k\)</span> is a <strong>user-specified parameter</strong>.</p>
            <p>If <span class="math inline">\(dist(o,o&#39;)\)</span> is lower than <span class="math inline">\(dist_k(o)\)</span> we exploit this last metric.</p>
            <p><strong>Local reachability density</strong> of o:</p>
            <p><span class="math display">\[
                lrd_k(o) = \dfrac{\|N_k(o)\|}{\sum_{o&#39; \in N_k(o)}{reachdist_k(o&#39; \leftarrow o)}}
            \]</span></p>
            <p>This is a density because we consider the number of objects and a sum of distances in the denominator.</p>
            <p>The denominator is the distance between <span class="math inline">\(o\)</span> and <span class="math inline">\(o&#39;\)</span>, while if we are in dense zone this is equal to <span class="math inline">\(dist_k(o)\)</span> while not in dense zone this distance is really the distance between <span class="math inline">\(o\)</span> and <span class="math inline">\(o&#39;\)</span>.</p>
            <p><strong>LOF (Local outlier factor)</strong> of an object o is the average of the ratio of local reachability of o and those of o’s k-nearest neighbors:</p>
            <p><span class="math display">\[
                LOF_k(o) = \dfrac{\sum_{o&#39; \in N_k(o)}{\dfrac{lrd_k(o&#39;)}{lrd_k(o)}}}{\|N_k(o)\|} = \sum_{o&#39; \in N_k(o)}{lrd_k(o&#39;) \cdot \sum_{o&#39; \in N_k(o)}{reachdist_k(o&#39; \leftarrow o)}}
            \]</span></p>
            <p>The lower the local reachability density of o, and the higher the local reachability density of the kNN of o, the higher LOF.</p>
            <p>If the local density of o is lower than the local reachability density of the KNN of o, this value of LOF is high and this is an indication that probably o is an outlier.</p>
            <p>This captures a local outlier whose local density is relatively low comparing to the local densities of its kNN.</p>
            <p>We move from the idea of distance to the idea of local density.</p>
            <p>We can identify outliers in different zones with different densities.</p>
            <p>We have to decide the LOF threshold to consider and also the value of k.</p>
            <p><strong>LOF captures the relative distance of an object</strong>, it is <strong>computationally heavy</strong> but <strong>allow us to find local outliers</strong>.</p>
            <h5 class="unnumbered" data-number="" id="example-17">Example</h5>
            <p>Consider the following 4 data points a(0,0), b(0,1), c(1,1), d(3,0)</p>
            <p>Compute the distance between the four points (Manhattan distance)</p>
            <p>dist(a,b) = 1 , dist(a,c) = 2, dist(a,d) = 3, dist(b,c) = 1, dist(b,d) = 4 and dist(c,d) = 3.</p>
            <p>Let us consider k=2.</p>
            <p>Then we compute distk for each object:</p>
            <p>dist2(a) = dist(a,c) = 2, the two-nearest neighbor for a is c.</p>
            <p>dist2(b) = dist(b,a) = 1</p>
            <p>dist2(c) = dist(c,a) = 2</p>
            <p>dist2(d) = dist(d,a) = 3</p>
            <p>So we have the neighborhood2:</p>
            <p>N2(a) = {b,c}</p>
            <p>N2(b) = {a,c}</p>
            <p>N2(c) = {b,a}</p>
            <p>N2(d) = {a,c}</p>
            <p>We compute the local reachability distance two for a,b,c and d:</p>
            <p><span class="math display">\[
                lrd_2(a) = \dfrac{\|N_2(a)\|}{reachdist_2(b \leftarrow a) + reachdist_2(c \leftarrow a)}
            \]</span> <span class="math display">\[
                reachdist_2(b \leftarrow a) = \max\{dist_2(b), dist(b,a)\} = \max\{1,1\} = 1
            \]</span> <span class="math display">\[
                reachdist_2(c \leftarrow a) = \max\{dist_2(c), dist(c,a)\} = \max\{2,2\} = 2
            \]</span> Thus <span class="math display">\[
                lrd_2(a) = \dfrac{\|N_2(a)\|}{reachdist_2(b \leftarrow a) + reachdist_2(c \leftarrow a)} = \dfrac{2}{1+2} = 0.667
            \]</span> <span class="math display">\[
                lrd_2(b) = \dfrac{\|N_2(b)\|}{reachdist_2(a \leftarrow b) + reachdist_2(c \leftarrow b)} = \dfrac{2}{2+2} = 0.5
            \]</span> <span class="math display">\[
                lrd_2(c) = \dfrac{\|N_2(c)\|}{reachdist_2(b \leftarrow c) + reachdist_2(a \leftarrow c)} = \dfrac{2}{1+2} = 0.667
            \]</span> <span class="math display">\[
                lrd_2(d) = \dfrac{\|N_2(d)\|}{reachdist_2(a \leftarrow d) + reachdist_2(c \leftarrow d)} = \dfrac{2}{3+3} = 0.33
            \]</span></p>
            <p>Now we compute the LOF.</p>
            <p><span class="math display">\[
                LOF_2(a) = (lrd_2(b)+lrd_2(c))(reachdist_2(b \leftarrow a) + reachdist_2(c \leftarrow a)) = (0.5 + 0.667)(1 + 2) = 3.501
            \]</span> <span class="math display">\[
                LOF_2(b) = (lrd_2(a)+lrd_2(c))(reachdist_2(a \leftarrow b) + reachdist_2(c \leftarrow b)) = (0.667 + 0.667)(2 + 2) = 5.336
            \]</span> <span class="math display">\[
                LOF_2(c) = (lrd_2(b)+lrd_2(a))(reachdist_2(b \leftarrow c) + reachdist_2(a \leftarrow c)) = (0.5 + 0.667)(1 + 2) = 3.501
            \]</span> <span class="math display">\[
                LOF_2(d) = (lrd_2(a)+lrd_2(c))(reachdist_2(a \leftarrow d) + reachdist_2(c \leftarrow d)) = (0.667 + 0.667)(3 + 3) = 8.004
            \]</span></p>
            <p><span class="math inline">\(LOF_2(d)\)</span> is an outlier corresponding to other LOFs, in fact d is (3,0) quite far from others.</p>
            <p><img src="../media/image566.png" /></p>
            <h2 data-number="9.6" id="clustering-based-outlier-detection"><span class="header-section-number">9.6</span> <strong>Clustering-Based Outlier Detection</strong></h2>
            <p>Using clustering an object is an outlier if (1) <strong>it does not belong to any cluster</strong>, (2) there is a <strong>large distance between the object and its closest cluster</strong>, or (3) it <strong>belongs to a small or sparse cluster</strong>.</p>
            <p>We need to fix different parameters as we’ve seen.</p>
            <p><strong>Case 1</strong>: Not belong to any cluster</p>
            <p>Example: Identify animals not part of a flock:</p>
            <p>We can use a density-based clustering method such as <strong>DBSCAN</strong></p>
            <p><img src="../media/image567.png" /></p>
            <p>But we need to find the best choices for <em>eps</em> and <em>minPts</em> to define what’s dense for us, changing them will change the results.</p>
            <p><strong>Case 2</strong>: Far from its closest cluster</p>
            <p>We can use k-means to partition data points into clusters.</p>
            <p>For each object o, assign an outlier score based on its distance from its closest center.</p>
            <p>If <span class="math inline">\(\dfrac{dist(o,c_o)}{avg\_dist(c_o)}\)</span> is large, likely this point is an outlier.</p>
            <p><img src="../media/image568.png" /></p>
            <p>We need to fix the value of k, and the threshold we need to consider the ratio large.</p>
            <p>Example: Intrusion detection <span class="math inline">\(\rightarrow\)</span> Consider the similarity between data points and the clusters in a training data set.</p>
            <p>Use a training set to find patterns of “normal” data, e.g., frequent itemsets in each segment, and cluster similar connections into groups.</p>
            <p>Compare new data points with the clusters mined—Outliers are possible attacks (TCP connection data example)</p>
            <p>Each of the approaches seen so far <strong>detects only individual objects as outliers</strong>. In a large dataset, some outliers may form a small cluster. In intrusion detection, for example, hackers who use similar tactics to attack a system. For instance, in the following figure C1 and C3 should be regarded as outliers. We have collective outliers.</p>
            <p><img src="../media/image569.png" /></p>
            <p>LOF in fact takes in consideration the density of points in respect to the density of neighbors, with k = 3 we cannot identify outliers because we have a small cluster and so each object have 3 neighbors with similar density respect to the analyzed object.</p>
            <p>To identify the physical significance of the definition of an outlier, an outlier factor, namely <strong>CBLOF (Cluster-Based Local Outlier Factor)</strong> is associated with each object.</p>
            <p>CBLOF <strong>measures both the size of the cluster</strong> the object belongs to and the <strong>distance between the object and its closest cluster</strong>.</p>
            <p>Given two parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we define <span class="math inline">\(b\)</span> as the boundary of large and small clusters if one of the following formulas holds</p>
            <p><span class="math display">\[
                \bigg \{
                    \begin{array}{rl}
                        |C_1| + |C_2| + ... + |C_b| \ge |D| \alpha \hspace{1cm} (1)\\
                        |C_b|/|C_b+1| \ge \beta \hspace{2cm}(2)\\
                    \end{array}  
            \]</span></p>
            <ol type="1">
            <li><p>most data points in the dataset are not outliers (for instance <span class="math inline">\(\alpha\)</span> = 90%). The sum of the cardinality of clusters is higher than a percentage of the cardinality of D.</p></li>
            <li><p>large and small clusters should have significant differences in size (for instance <span class="math inline">\(\beta\)</span> = 5). We want to identify small clusters as outliers.</p></li>
            </ol>
            <p>The set of large clusters is defined as <span class="math inline">\(LC=\{C_i|i \le b\}\)</span> and the set of small clusters is defined as <span class="math inline">\(SC=\{C_j|j &gt; b\}\)</span>.</p>
            <p>We will have a cluster with not outliers and a cluster with outliers.</p>
            <p>For each object o, <strong>CBLOF</strong> (Cluster-Based Local Outlier Factor) is defined as:</p>
            <p><span class="math display">\[
                CBLOF(o)\bigg \{
                    \begin{array}{rl}
                        |C_i| \cdot \min(distance(o,C_j)) \hspace{1cm} o \in C_i,C_j \in SC\ and\ C_j \in LC\\
                        |C_j| \cdot \min(distance(o,C_j)) \hspace{2.46cm}o \in C_i\ and\ C_i \in LC\\
                    \end{array}  
            \]</span></p>
            <p>For the computation of the distance between the object and the cluster, it is sufficient to adopt the similarity measure used in the clustering algorithm.</p>
            <p>To find <strong>CBLOF</strong> we use this algorithm:</p>
            <ul>
            <li><p>Cluster the dataset</p></li>
            <li><p>Compute the value of CBLOF for each object</p></li>
            </ul>
            <p>It let us find objects in small cluster and let use understand how far them from large clusters are.</p>
            <h3 data-number="9.6.1" id="strengths-2"><span class="header-section-number">9.6.1</span> <strong>Strengths</strong></h3>
            <ul>
            <li><p>Detect outliers <strong>without requiring any labeled data</strong>, clusters are unsupervised</p></li>
            <li><p><strong>Work for many types of data</strong></p></li>
            <li><p>Clusters can be regarded as <strong>summaries of the data</strong></p></li>
            <li><p>Once the clusters are obtained, need <strong>only compare any object against the clusters to determine whether it is an outlier</strong> (fast because we assume that we computed clusters before, but we also have to generate clusters)</p></li>
            </ul>
            <h3 data-number="9.6.2" id="weaknesses"><span class="header-section-number">9.6.2</span> <strong>Weaknesses</strong></h3>
            <ul>
            <li><p><strong>Effectiveness depends highly on the clustering method</strong> used—they may not be optimized for outlier detection. Depending on the clustering algorithm we can also have different shapes of clusters and it affects our conclusion.</p></li>
            <li><p><strong>High computational cost</strong>: Need to first find clusters</p></li>
            <li><p>A method to reduce the cost: <strong>Fixed-width clustering</strong></p>
            <ul>
            <li><p>A point is assigned to a cluster if the center of the cluster is within a pre-defined distance threshold from the point</p></li>
            <li><p>If a point cannot be assigned to any existing cluster, a new cluster is created, and the distance threshold may be learned from the training data under certain conditions.</p></li>
            </ul></li>
            </ul>
            <h2 data-number="9.7" id="classification-approaches"><span class="header-section-number">9.7</span> <strong>Classification Approaches</strong></h2>
            <p>Idea: <strong>Train a classification model that can distinguish “normal” data from outliers</strong>.</p>
            <p>But <strong>the dataset will be imbalanced</strong>, the number of outliers is lower.</p>
            <p>A <strong>brute-force approach</strong> resolves this problem.</p>
            <p>We consider a training set that contains samples labeled as “normal” and others labeled as “outlier”.</p>
            <p>But the training set is typically heavily biased: number of “normal” samples likely far exceeds number of outlier samples. We have the necessity to re-balance by using undersampling or oversampling.</p>
            <p><img src="../media/image572.png" /></p>
            <p>This approach cannot detect unseen anomaly.</p>
            <h3 data-number="9.7.1" id="one-class-model"><span class="header-section-number">9.7.1</span> <strong>One-class model</strong></h3>
            <p>A classifier is built to describe only the normal class. We learn the normal class, so we learn the decision boundary of the normal class, <strong>it classifies one only class</strong>.</p>
            <p>Any samples that do not belong to the normal class (not within the decision boundary) are declared as outliers.</p>
            <p><img src="../media/image573.png" /></p>
            <h4 class="unnumbered" data-number="" id="support-vector-data-description">Support Vector Data Description</h4>
            <p>The most popular approach is <em>Support Vector Data Description</em> (<strong>SVDD</strong>).</p>
            <p>It constructs a hyper-sphere around the positive class data that encompassed almost all points in the data set with the minimum radius.</p>
            <p>The SVDD classifier rejects a given test point as outlier if it falls outside the hyper-sphere. However, SVDD <strong>can reject some fraction of positively labelled data when the volume of the hyper-sphere decreases</strong>.</p>
            <h4 class="unnumbered" data-number="" id="nearest-neighbor-description">Nearest Neighbor Description</h4>
            <p>Another approach is the <em>Nearest Neighbor Description</em> (<strong>NN-d</strong>), a variant of the Nearest Neighbor method.</p>
            <p>A test object z is accepted as a member of target class provided that its local density is greater than or equal to the local density of its nearest neighbor in the training set. The following <strong>acceptance function</strong> is used:</p>
            <p><span class="math display">\[
                f_{NN^{tr}}(z) = I\left(\dfrac{\|z - NN^{tr}(z)\|}{NN^{tr}(z)-NN^{tr}(NN^{tr}(z))} \right)
            \]</span></p>
            <p>which presents that the distance from object z to its nearest neighbor in the training set and <span class="math inline">\(NN^{tr}(z)\)</span> is compared to the distance from its nearest neighbor <span class="math inline">\(NN^{tr}(z)\)</span> to its nearest neighbor.</p>
            <p><img src="../media/image575.png" /></p>
            <p>If this is higher than a threshold, we can reject z.</p>
            <h3 data-number="9.7.2" id="semi-supervised-learning"><span class="header-section-number">9.7.2</span> <strong>Semi-supervised learning</strong></h3>
            <p><strong>Combining classification-based and clustering-based methods</strong>.</p>
            <h4 class="unnumbered" data-number="" id="method">Method:</h4>
            <ul>
            <li><p>Using a clustering-based approach, find a large cluster, <span class="math inline">\(C\)</span>, and a small cluster, <span class="math inline">\(C_1\)</span></p></li>
            <li><p>Since some objects in <span class="math inline">\(C\)</span> carry the label “normal”, treat all objects in <span class="math inline">\(C\)</span> as normal.</p></li>
            <li><p>Use the one-class model of this cluster to identify normal objects in outlier detection</p></li>
            <li><p>Since some objects in cluster <span class="math inline">\(C_1\)</span> carry the label “outlier”, declare all objects in <span class="math inline">\(C_1\)</span> as outliers</p></li>
            <li><p>Any object that does not fall into the model for C (such as <span class="math inline">\(a\)</span>) is considered an outlier as well</p></li>
            </ul>
            <p>We exploit clustering but also the ground-truth.</p>
            <p><img src="../media/image576.png" /></p>
            <h4 class="unnumbered" data-number="" id="comments-on-classification-based-outlier-detection-methods">Comments on classification-based outlier detection methods</h4>
            <ul>
            <li><p><strong>Strength</strong>: Outlier detection is fast, we just need to see if it’s out of the boundary</p></li>
            <li><p><strong>Bottleneck</strong>: Quality heavily depends on the availability and quality of the training set, but often difficult to obtain representative and high-quality training data, we don’t have a sufficient number of outliers to train reliable classifiers.</p></li>
            </ul>
            <h2 data-number="9.8" id="mining-contextual-and-collective-outliers"><span class="header-section-number">9.8</span> <strong>Mining Contextual and Collective Outliers</strong></h2>
            <h3 data-number="9.8.1" id="transformation-into-conventional-outlier-detection"><span class="header-section-number">9.8.1</span> Transformation into Conventional Outlier Detection</h3>
            <p>If the contexts can be clearly identified, <strong>transform the contextual outlier detection problem to conventional outlier detection</strong>:</p>
            <ul>
            <li><p><strong>Identify the context of the object</strong> using the contextual attributes</p></li>
            <li><p>Calculate the <strong>outlier score</strong> for the object in the context using a conventional outlier detection method.</p></li>
            </ul>
            <p>Ex. Detect outlier customers in the context of customer groups</p>
            <ul>
            <li><p><strong>Contextual attributes</strong>: age group, postal code</p></li>
            <li><p><strong>Behavioral attributes</strong>: number of transations/yr, annual total transations amount</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="steps-5">Steps:</h4>
            <ol type="1">
            <li>locate customer’s context,</li>
            <li>compare c with the other customers in the same group, and</li>
            <li>use a conventional outlier detection method.</li>
            </ol>
            <p>If the context contains very few customers, we need to <strong>generalize contexts</strong>.</p>
            <p>For instance, for a customer c, if the corresponding context contains very few or even no other customers, the evaluation of whether c is an outlier using the exact context is unreliable or even impossible. I found the context but i don’t have enough samples to deduce if an object is an outlier.</p>
            <p>To overcome this challenge, we can assume that customers of similar age and who live within the same area should have similar normal behavior.</p>
            <p>We learn a <strong>mixture model</strong> <span class="math inline">\(U\)</span> on the contextual attributes, and another <strong>mixture model</strong> <span class="math inline">\(V\)</span> of the data on the behavior attributes.</p>
            <p>We learn a mapping <span class="math inline">\(p(V_i|U_j)\)</span>: the probability that a data object o belonging to cluster <span class="math inline">\(U_j\)</span> on the contextual attributes is generated by cluster <span class="math inline">\(V_i\)</span> on the behavior attributes.</p>
            <p>The outlier is determined by using the <strong>outlier score</strong>:</p>
            <p><span class="math display">\[
                S(o) = \sum_{U_j}{p(o \in U_j) \sum_{V_i}{p(o \in V_i)p(V_i|U_j)}}
            \]</span></p>
            <h3 data-number="9.8.2" id="modeling-normal-behavior-considering-contexts"><span class="header-section-number">9.8.2</span> Modeling Normal Behavior considering Contexts</h3>
            <p>In some applications, one cannot clearly partition the data into contexts.</p>
            <p>Ex. if a customer suddenly purchased a product that is unrelated to those she recently browsed, it is unclear how many products browsed earlier should be considered as the context.</p>
            <p>Model the “normal” behavior considering contexts can be performed:</p>
            <ul>
            <li><p>Using a training data set, <strong>train a model that predicts the expected behavior attribute values</strong> considering the contextual attribute values</p></li>
            <li><p>An object is a <strong>contextual outlier</strong> if its behavior attribute values <strong>significantly deviate from the values predicted</strong> by the model</p></li>
            </ul>
            <p>If what we have predicted is far from what we observed, we can deduce it is an outlier.</p>
            <p>Using a prediction model that links the contexts and behavior, these methods avoid the explicit identification of specific contexts.</p>
            <p>Methods: <strong>A number of classification and prediction techniques can be used to build such models</strong>, such as regression, Markov Models, and Finite State Automaton.</p>
            <h3 data-number="9.8.3" id="structured-objects"><span class="header-section-number">9.8.3</span> Structured Objects</h3>
            <p>Collective outliers are identified if objects as a group deviate significantly from the entire data.</p>
            <p>Need to examine the <strong>structure of the dataset</strong>, i.e, the relationships between multiple data objects. Each of these structures is inherent to its respective type of data.</p>
            <ul>
            <li><p>For <strong>temporal data</strong> (such as time series and sequences), we explore the structures formed by time, which occur in segments of the time series or subsequences.</p></li>
            <li><p>For <strong>spatial data</strong>, explore local areas</p></li>
            <li><p>For <strong>graph and network data</strong>, we explore subgraphs</p></li>
            </ul>
            <p>Difference from the contextual outlier detection: the <strong>structures are often not explicitly defined</strong> and have to be discovered as part of the outlier detection process. It strongly depends on types of data and structure.</p>
            <h4 class="unnumbered" data-number="" id="collective-outlier-detection-methods-two-categories.">Collective outlier detection methods: two categories.</h4>
            <ol type="1">
            <li><p>In the first category we reduce the problem to conventional outlier detection.</p>
            <p>We identify structure units, treat each structure unit (e.g., subsequence, time series segment, local area, or subgraph) as a data object, and extract features</p>
            <p>Then outlier detection on the set of “structured objects” constructed as such using the extracted features</p></li>
            <li><p>The second category models the expected behavior of structure units directly.</p></li>
            </ol>
            <p>Ex. 1. Detect collective outliers in online social network of customers</p>
            <ul>
            <li><p>Treat each possible subgraph of the network as a structure unit</p></li>
            <li><p>Collective outlier: An outlier subgraph in the social network.</p>
            <ul>
            <li><p>Small subgraphs that are of very low frequency</p></li>
            <li><p>Large subgraphs that are surprisingly frequent</p></li>
            </ul></li>
            </ul>
            <p>Ex.2. Detect collective outliers in temporal sequences</p>
            <ul>
            <li><p>Learn a Markov model from the sequences</p></li>
            <li><p>A subsequence can then be declared as a collective outlier if it significantly deviates from the model</p></li>
            </ul>
            <p>Collective outlier detection is <strong>subtle</strong> due to the challenge of <strong>exploring the structures in data</strong>.</p>
            <ul>
            <li><p>The exploration typically uses heuristics, and thus may be application dependent.</p></li>
            <li><p>The computational cost is often high due to the sophisticated mining process.</p></li>
            </ul>
            <p>We have to deal with the specific application domain.</p>
            <h2 data-number="9.9" id="outlier-detection-in-high-dimensional-data"><span class="header-section-number">9.9</span> Outlier Detection in <strong>High Dimensional Data</strong></h2>
            <p>We are strongly affected by the <strong>curse of dimensionality</strong> problem, increasing dimensions the space becomes very sparse. We need techniques to work with sparse data.</p>
            <p>Reducing the dimensionality is not a good approach, we need to find the best one.</p>
            <h3 data-number="9.9.1" id="challenges-1"><span class="header-section-number">9.9.1</span> Challenges</h3>
            <ul>
            <li><p><strong>Interpretation of outliers</strong> is fundamental.</p>
            <p>Detecting outliers without saying why they are outliers is not very useful in high-D due to many features (or dimensions) are involved in a high-dimensional data set.</p>
            <p>E.g., which subspaces that manifest the outliers or an assessment regarding the “outlier-ness” of the objects.</p></li>
            <li><p>We need to work with <strong>data sparsity</strong>. Data in high-D spaces are often sparse.</p>
            <p>The distance between objects becomes heavily dominated by noise as the dimensionality increases.</p></li>
            <li><p>We can use <strong>data subspaces</strong>. Adaptive to the subspaces signifying the outliers.</p>
            <p>We can in this way capture the local behavior of data.</p></li>
            <li><p>It must be <strong>scalable considering dimensionality</strong>, where the number of subspaces increases exponentially.</p></li>
            </ul>
            <h3 data-number="9.9.2" id="approaches-to-cope-with-the-curse-of-dimensionality"><span class="header-section-number">9.9.2</span> <strong>Approaches to cope with the curse of dimensionality</strong></h3>
            <p><strong>Method 1</strong>: Detect outliers in the full space, we don’t reduce the dimensions. e.g., <strong>HilOut Algorithm</strong></p>
            <p>Find distance-based outliers, but use the ranks of distance instead of the absolute distance in outlier detection.</p>
            <p>For each object o, find its k-nearest neighbors: <span class="math inline">\(nn_1(o)\)</span>, … , <span class="math inline">\(nn_k(o)\)</span>.</p>
            <p>The weight of object o: <span class="math display">\[
                \large{w(o) = \sum_{i = 1}^{k}{dist(o,nn_i(o))}}
            \]</span></p>
            <p>All objects are ranked in <strong>weight-descending order</strong>.</p>
            <p>We just select the top-<span class="math inline">\(l\)</span> objects in weight are output as outliers (<span class="math inline">\(l\)</span>: user-specified parameters).</p>
            <p>We work in ranks and not in distance and order is established by using distance put we don’t put a threshold based on distance.</p>
            <p><strong>Method 2</strong>: Approach the problem using <strong>dimensionality reduction</strong>.</p>
            <p>Works only when in lower-dimensionality, normal instances can still be distinguished from outliers</p>
            <p>We can <strong>exploit PCA</strong>: Heuristically, the principal components with low variance are preferred because, on such dimensions, normal objects are likely close to each other and outliers often deviate from the majority.</p>
            <h3 data-number="9.9.3" id="finding-outliers-in-subspaces"><span class="header-section-number">9.9.3</span> Finding Outliers in Subspaces</h3>
            <p>Then after reduced the space we analyze outliers in the transformed space.</p>
            <p>Extending conventional outlier detection to subspace instead of the original space is <strong>possible but hard for outlier interpretation</strong>, in fact we’ll work in subspaces.</p>
            <p>Finding outliers in much lower dimensional subspaces is easy to interpret the motivation and to what extent the object is an outlier.</p>
            <h4 class="unnumbered" data-number="" id="example-18">Example</h4>
            <p>Find outlier customers in certain subspace: average transaction amount &gt;&gt; avg. and purchase frequency &lt;&lt; avg.</p>
            <p>An approach is to exploit <strong>grid-based subspace outlier detection</strong> method. The idea is to project data onto various subspaces to find an area whose density is much lower than average.</p>
            <p>This means we have to discretize the data into a grid with <span class="math inline">\(\varphi\)</span> equi-depth regions.</p>
            <p>The equi-depth grid takes in consideration equal frequency.</p>
            <p>Using this approach we search for regions that are significantly sparse.</p>
            <p>Consider a k-d cube: k ranges on k dimensions, with n objects.</p>
            <p>If objects are independently distributed, the expected number of objects falling into a k-dimensional region is <span class="math inline">\((1/\varphi)^kn = f^kn\)</span>, where <span class="math inline">\(1/\varphi\)</span> is the probability to find dense region in our space, and <span class="math inline">\(k\)</span> is the number of dimensions, the standard</p>
            <p>deviation is <span class="math inline">\(\sqrt{f^k(1-f^k)n}\)</span></p>
            <p>The <strong>sparsity coefficient</strong> of cube <span class="math inline">\(C\)</span>:</p>
            <p><span class="math display">\[
                S(C) = \dfrac{n(C)-f^kn}{\sqrt{f^k(1-f^k)n}}
            \]</span></p>
            <p><span class="math inline">\(n(C)\)</span> is the number of objects present in the cell.</p>
            <p><span class="math inline">\(f^kn\)</span> is the probability of having objects multiplied by the number of objects, the number of objects we expect in the cell.</p>
            <p>if <span class="math inline">\(n(C) &gt; f^kn\)</span> it contains more objects than expected, few possibility to have outliers. Otherwise, we have density lower than expected and we expect there are outliers.</p>
            <p>If <span class="math inline">\(S(C) &lt; 0\)</span>, <span class="math inline">\(C\)</span> contains less objects than expected.</p>
            <p>The more negative, the sparser <span class="math inline">\(C\)</span> is and the more likely the objects in <span class="math inline">\(C\)</span> are outliers in the subspace. We have a lot less density than expected.</p>
            <p><img src="../media/image581.png" /></p>
            <h4 class="unnumbered" data-number="" id="algorithm-4"><strong>Algorithm</strong></h4>
            <ul>
            <li><p><strong>Find the m grid cells</strong> (projections) with the <strong>lowest sparsity</strong> coefficients</p></li>
            <li><p>Brute-force algorithm is in <span class="math inline">\(O(\varphi k)\)</span></p></li>
            <li><p>Evolutionary algorithm (input: m and the dimensionality of the cells). We need to fix m and the dimensionality of the cell.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="discussion"><strong>Discussion</strong></h4>
            <ul>
            <li><p>Results need not be the points from the optimal cells</p></li>
            <li><p><strong>Very coarse model</strong> (all objects that are in cell with less points than to be expected)</p></li>
            <li><p><strong>Quality depends on grid resolution and grid position</strong></p></li>
            <li><p><strong>Implements a global approach</strong> (key criterion: globally expected number of points within a cell)</p></li>
            </ul>
            <h3 data-number="9.9.4" id="modelling-high-dimensional-outliers"><span class="header-section-number">9.9.4</span> Modelling High-Dimensional Outliers</h3>
            <p>Develop new models for high-dimensional outliers directly.</p>
            <p><strong>Avoid proximity measures</strong> and adopt new heuristics that do not deteriorate in high-dimensional data, that does not depend a lot on distance.</p>
            <p>We identify outliers using the idea of <strong>angles</strong>.</p>
            <p>Angles are more stable than distances in high dimensional spaces(e.g. the popularity of cosine-based similarity measures for text data).</p>
            <p><strong>Object o is an outlier if most other objects are located in similar directions</strong>.</p>
            <p><strong>Object o is no outlier if many other objects are located in varying directions</strong>.</p>
            <p><img src="../media/image582.png" /></p>
            <p>If I connect a far away object to other objects, angles are very narrow and similar to each other.</p>
            <p>If we have an object inside a group of objects, angles we obtain are very different.The intuition is to compute angles between object we are observing and others. If the object is not an outlier, is inside of the cluster, the variation is high between angles and we can obtain angles from 0 to 360.</p>
            <p><strong>Use the variance of angles for a point to determine outlier</strong>.</p>
            <p><strong>Combine angles and distance</strong> to model outliers.</p>
            <blockquote>
            <p>Use the distance-weighted angle variance as the outlier score .</p>
            </blockquote>
            <blockquote>
            <p>Outliers are at the border of the data distribution.</p>
            </blockquote>
            <blockquote>
            <p>Normal points are in the center of the data distribution.</p>
            </blockquote>
            <h4 class="unnumbered" data-number="" id="model"><strong>Model</strong></h4>
            <p>Consider for a given point p the angle between px and py for any two x y from the database.</p>
            <p>We connect p with x and p with y and consider the angles, consider the spectrum of all these angles.</p>
            <p>The broadness of this spectrum is a score for the outlierness of a point.</p>
            <p><img src="../media/image583.png" /></p>
            <p>Measure the variance of the angle spectrum weighted by the corresponding distances (for lower dimensional data sets where angles are less reliable).</p>
            <p>The <strong>Angle-based outlier factor</strong>(ABOF):</p>
            <p><span class="math display">\[
                \Large{ABOF(o) = VAR_{x,y \in D, x \ne o,y \ne o}{\dfrac{\langle \overrightarrow{ox}, \overrightarrow{oy} \rangle}{dist(o,x)^2dist(o,y)^2}}}
            \]</span></p>
            <p>The angle is normalized by the distance between <span class="math inline">\(o\)</span> and <span class="math inline">\(x\)</span> and <span class="math inline">\(o\)</span> and <span class="math inline">\(y\)</span>.</p>
            <p><strong>Properties</strong>:</p>
            <ul>
            <li><p>Small ABOF <span class="math inline">\(\Rightarrow\)</span> outlier, the variance is low and points are far from <span class="math inline">\(o\)</span>, obtained by the denominator</p></li>
            <li><p>High ABOF <span class="math inline">\(\Rightarrow\)</span> no outlier, we are in a center of a group so the variance of angles if high and distance is low.</p></li>
            </ul>
            <p><strong>Complexity</strong> <span class="math inline">\(O(n^3)\)</span> is very high, but we can have efficient approximation computation methods Another aspect is that this approach can be generalized to handle arbitrary types of data.</p>
            <h1 data-number="10" id="clustering-graphs-and-network-data"><span class="header-section-number">10</span> Clustering Graphs and Network Data</h1>
            <p>Network is a collection of entities that are interconnected with links.</p>
            <p>An example is social network where entities are people and links are friendships.</p>
            <p>We cannot truly understand a complex system unless we understand the underlying network.</p>
            <p>Everything is connected, studying individual entities gives only a partial view of a system.</p>
            <p>Two main themes:</p>
            <ul>
            <li><p>What are the structural properties of the network?</p></li>
            <li><p>How do processes happen in the network?</p></li>
            </ul>
            <p>In mathematics, networks are called graphs, the entities are nodes, and the links are edges.Networks of thousands, millions, or billions of nodes are impossible to process visually.</p>
            <p>Problems become harder and processes are more complex.</p>
            <p>Current problems are:</p>
            <ul>
            <li><p>Information/Virus Cascade. How do viruses spread between individuals? How does information propagate in social and information networks?</p></li>
            <li><p>Ranking of nodes on the web. We also need algorithms to compute the importance of nodes in a graph. For instance, the PageRank algorithm in Google.</p></li>
            <li><p>Link prediction, that consists in, given a snapshot of a social network at time t, seek to accurately predict the edges that will be added to the network during the interval from time t to a given future time t'. Applications: Accelerate the growth of a social network, Identify suspect relationships.</p></li>
            <li><p>Network content. Users on online social networks generate content. Mining the content in conjunction with the network can be useful.</p></li>
            </ul>
            <p>It can be useful for social raccomandations. Do friends post similar content on Facebook? Can we understand a user’s interests by looking at those of their friends?</p>
            <ul>
            <li><p>Social media. Twitter has become a global “sensor” detecting and reporting everything. Automatically detect events using Twitter like Earthquake news propagation or Sentiment mining.</p></li>
            <li><p>Clustering and Finding Communities, which are cohesive subgroups are subsets of actors among whom there are relatively strong, direct, intense, frequent, or positive ties.</p></li>
            <li><p>Community Evolution. People become similar to those they interact with.</p></li>
            <li><p>Bi-partite graphs, clustering customers buying similar products; introducing similarity metrics.</p></li>
            <li><p>Web search engines, where we analyze click through graphs and Web graphs. In click-through information, an edge links a query to a web page if a user clicks the web page when asking the query. Valuable information can be obtained by cluster analyses on the query–web page bipartite graph. In web graph. each web page is a vertex, and each hyperlink is an edge pointing from a source page to a destination page.</p></li>
            <li><p>Social networks, in which the vertices are individuals or organizations, and the links are interdependencies between the vertices, representing friendship, common interests, or collaborative activities. Customers within a cluster may influence one another regarding purchase decision making.</p></li>
            </ul>
            <h3 data-number="10.0.1" id="network-introduction"><span class="header-section-number">10.0.1</span> Network Introduction</h3>
            <p>A network is basically a graph where:</p>
            <ul>
            <li><p>Objects: nodes are called vertices N</p></li>
            <li><p>Interactions: links are called edges E</p></li>
            <li><p>System: network is referenced as graph G(N,E)</p></li>
            </ul>
            <p>Graph is a mathematical representation of a network</p>
            <p>How can we build the graph? We have to choose of the proper network representation of a given domain/problem determines our ability to use networks successfully. In some cases there is a unique, unambiguous representation, while in other cases, the representation is by no means unique.</p>
            <p>The way you assign links will determine the nature of the question you can study.</p>
            <h3 data-number="10.0.2" id="how-can-we-cluster-network-data"><span class="header-section-number">10.0.2</span> How Can We Cluster Network Data?</h3>
            <ul>
            <li><p>We can apply <strong>standard clustering algorithms by introducing a specific definition of similarity</strong> measures:</p>
            <ul>
            <li><p>Geodesic distances</p></li>
            <li><p>Distance based on random walk (SimRank)</p></li>
            </ul></li>
            <li><p><strong>Graph clustering methods</strong>:</p>
            <ul>
            <li><p>Minimum cuts</p></li>
            <li><p>Density-based clustering, for example SCAN</p></li>
            </ul></li>
            </ul>
            <h2 data-number="10.1" id="similarity-measures"><span class="header-section-number">10.1</span> Similarity Measures</h2>
            <p>If we are able to define similarity we can exploit clustering algorithms to cluster nodes.</p>
            <h3 data-number="10.1.1" id="geodesic-distance"><span class="header-section-number">10.1.1</span> Geodesic Distance</h3>
            <p><strong>The Geodesic Distance is the distance given by the shortest path between two vertices in a graph.</strong></p>
            <ul>
            <li>Geodesic distance (A, B): number of edges (length) of the shortest path between A and B (if not connected, defined as infinite)</li>
            </ul>
            <p><strong>Exploiting it we can define:</strong></p>
            <ul>
            <li><strong>Eccentricity of v</strong>, eccen(v): The largest geodesic distance between v and any other vertex u ∈ V − {v}.</li>
            </ul>
            <p>E.g., eccen(a) = eccen(b) = 2; eccen(c) = eccen(d) = eccen(e) =3</p>
            <p><img src="../media/image585.png" /></p>
            <ul>
            <li><strong>Radius of graph G</strong>: The minimum eccentricity of all vertices, i.e., the distance between the “most central point” and the “farthest border”</li>
            </ul>
            <p>r = min v∈V eccen(v)</p>
            <p>E.g., radius (g) = 2.</p>
            <ul>
            <li><strong>Diameter of graph G</strong>: The maximum eccentricity of all vertices, i.e., the largest distance between any pair of vertices in G</li>
            </ul>
            <p>d = max v∈V eccen(v)</p>
            <blockquote>
            <p>E.g., diameter (g) = 3</p>
            <p><strong>A peripheral vertex</strong> is a vertex that achieves the diameter.</p>
            <p>E.g., Vertices c, d, and e are peripheral vertices</p>
            </blockquote>
            <h3 data-number="10.1.2" id="similarity-in-social-networks-introduction"><span class="header-section-number">10.1.2</span> Similarity in Social Networks Introduction</h3>
            <p>Let us consider the similarity between two vertices in a customer social network.</p>
            <p>How well can geodesic distance measure similarity and closeness in a network?</p>
            <p>Suppose that Ada and Bob are two customers in the network</p>
            <p>The geodesic distance (i.e., the length of the shortest path between Ada and Bob) is the shortest path that a message can be passed from Ada to Bob and viceversa.</p>
            <p>Is this information useful? Typically, the company is not interested in how a message is passed from Ada to Bob but in understand if they’re reachable.</p>
            <p>It can be un-useful for our aim. We need to define what does similarity mean in a social network</p>
            <p>When we deal with similarity measure, we can consider two different meanings.</p>
            <p><strong>Structural context-based similarity:</strong></p>
            <p>Two customers are considered similar to one another if they have similar neighbors in the social network. Two people receiving recommendations from a good number of common friends often make similar decisions: intuitive!</p>
            <p><strong>Similarity based on random walk:</strong></p>
            <p>The company sends promotional information to both Ada and Bob in the social network. Ada and Bob may randomly forward such information to their friends (or neighbors) in the network. The closeness between Ada and Bob can then be measured by the likelihood that other customers simultaneously receive the promotional information that was originally sent to Ada and Bob.</p>
            <p>These meanings associated with the concept of similarity help us introduce a similarity measure useful for our work.</p>
            <p>The structural-context similarity can express what we want.</p>
            <h3 data-number="10.1.3" id="similarity-rank-based-on-structural-context"><span class="header-section-number">10.1.3</span> Similarity Rank Based on Structural Context</h3>
            <p><strong>SimRank</strong>: structural-context similarity, and it is based on the similarity of its neighbors.</p>
            <p>In a directed graph G = (V,E), we define:</p>
            <ul>
            <li><p>individual in-neighborhood of v: I(v) = {u | (u, v) ∈ E}, the edge is incoming</p></li>
            <li><p>individual out-neighborhood of v: O(v) = {w | (v, w) ∈ E}, the edge is outgoing</p></li>
            </ul>
            <p>Similarity measured in terms in SimRank between two vertices u and v is defined as:</p>
            <p><img src="../media/image586.png" /></p>
            <p>where C is a constant between 0 and 1.</p>
            <p>If a vertex does not have any neighbor, we define s(u,v) = 0.</p>
            <p>We define a SimRank between two nodes in terms of SimRank between other nodes.</p>
            <p>How can compute SimRank? Iteratively, compute the previous equation until a fixed point is reached.</p>
            <blockquote>
            <p>Let <span class="math inline">\(n\)</span> be the number of nodes in graph <span class="math inline">\(G\)</span>.</p>
            </blockquote>
            <blockquote>
            <p>For each iteration <span class="math inline">\(i\)</span> we can keep <span class="math inline">\(n^2\)</span> entries <span class="math inline">\(s_i(^*,^*)\)</span>, where <span class="math inline">\(s_i(u,v)\)</span> gives the score between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> on iteration <span class="math inline">\(i\)</span>.</p>
            </blockquote>
            <blockquote>
            <p>We start with <span class="math inline">\(s_0(^*,^*)\)</span> where each <span class="math inline">\(s_0(u,v)\)</span> is a lower bound on the actual SimRank score <span class="math inline">\(s(u,v)\)</span>:</p>
            </blockquote>
            <p><img src="../media/image587.png" /></p>
            <blockquote>
            <p>To compute <span class="math inline">\(s_{i+1}(u,v)\)</span> from <span class="math inline">\(s_i(^*,^*)\)</span> we use :</p>
            </blockquote>
            <p><img src="../media/image588.png" /></p>
            <p>We iteratively compute and update this, but we have the termination condition when <span class="math inline">\(u = v\)</span>.</p>
            <p>In a number of iterations, we have similarity in terms of SimRank for each pair of vertices.</p>
            <p>The values <span class="math inline">\(s_i(^*,^*)\)</span> are non-decreasing as <span class="math inline">\(i\)</span> increases.</p>
            <p><strong>Complexity:</strong></p>
            <p><img src="../media/image589.png" /> where <span class="math inline">\(d_2\)</span> is the average of <img src="../media/image590.png" /></p>
            <p><span class="math inline">\(K\)</span> is the number of iterations and typically is equal to 5.</p>
            <p>Given this similarity we can apply any clustering algorithms exploiting it.</p>
            <p>We consider similar neighbors as context-base similarity.</p>
            <p>The definition is recursive, and we identified that for computing the SimRank we can exploit a iterative computation.</p>
            <p>(*,*) represents each possible node in the graph.</p>
            <p>We start with <span class="math inline">\(i = 0\)</span>, we assume that it is 0 or 1 based on the previous formula, and then we calculate <span class="math inline">\(s_{i+1}\)</span> as iterative process.</p>
            <p>They are not decreasing because we start from 0 if <span class="math inline">\(u \neq v\)</span> and so we cannot decrease more.</p>
            <p>This approach converges after <span class="math inline">\(k\)</span> iterations.</p>
            <p>But we have to compute <span class="math inline">\(n^2\)</span> similarities so the complexity is high.</p>
            <p>At the end of the process we can generate the <span class="math inline">\(G^2\)</span> graph. <span class="math inline">\(G^2\)</span> represents an ordered pair of nodes of <span class="math inline">\(G\)</span>. A node <span class="math inline">\((a,b)\)</span> of <span class="math inline">\(G^2\)</span> points to a node <span class="math inline">\((c,d)\)</span> if, in <span class="math inline">\(G\)</span>, <span class="math inline">\(a\)</span> points to <span class="math inline">\(c\)</span> and <span class="math inline">\(b\)</span> points to <span class="math inline">\(d\)</span>.</p>
            <p>The example represents the Web pages of two professors ProfA and ProfB, their students StudentA and StudentB, and the home page of their university Univ</p>
            <p><img src="../media/image591.png" /></p>
            <p>University has a link to ProfA which has a link to StudentA and so on.</p>
            <p>Applying the simRank to measure the similarity we generate the <span class="math inline">\(G^2\)</span> and we will have a connection in front of that condition. We create paths between pairs.</p>
            <p>{Univ, Univ} has simRank equal to 1 because we have same nodes.</p>
            <p>{ProfA, ProfB} has a similarity equal to 0.414, because both linked from University, and so on.</p>
            <p>We exploited similarity based on the structure of the graph, we considered input edges in our nodes.</p>
            <p>Now, we see a similarity definition in which we consider the random walk.</p>
            <h3 data-number="10.1.4" id="similarity-rank-based-on-random-walk"><span class="header-section-number">10.1.4</span> Similarity Rank Based on Random Walk</h3>
            <p>Similarity based on random walk: in a strongly connected graph a path exists between every two nodes).</p>
            <p>We define the expected distance from u to v:</p>
            <p><img src="../media/image592.png" /></p>
            <p>The sum is computed over all tours t which start at u and end at v, and do not touch v except at the end.</p>
            <p>For a tour <img src="../media/image593.png" /> the length l(t) of t is</p>
            <p>k-1, the number of edges we travel along the tour.</p>
            <p>The probability P(t) of travelling t is:</p>
            <p><img src="../media/image594.png" /></p>
            <p>This is the probbaility to walk along the tour. We make this consideration for each node on the walk.</p>
            <p>Note that the case where u = v, for which d (u,v) = 0 is a special case of the formula of the distance: only one tour is in the summation and it has length 0, we remain in the same node.</p>
            <p>The expected distance from u to v is exactly the expected number of steps a random surfer, who at each step follows a random out-edge, would take before he first reaches v, starting from u.</p>
            <p><strong>Expected meeting distance (EMD):</strong> the expected meeting distance <span class="math inline">\(m(u,v)\)</span> between <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> is the expected number of steps required before two surfers, one starting at <span class="math inline">\(u\)</span> and the other at <span class="math inline">\(v\)</span>, would meet if they walked (randomly) in lock-step.</p>
            <p>The EMD is symmetric by definition.</p>
            <p>We have two surfers one starting from u and one from v and we measure just the number of steps requires for obtaining the surfers meet each other.</p>
            <p><img src="../media/image595.png" /></p>
            <p>In the first example, if the two surfers start they will move in parallel.</p>
            <p>To define EMD formally in <span class="math inline">\(G\)</span>, we use the derived graph <span class="math inline">\(G^2\)</span> of node-pairs.</p>
            <p>Each node <span class="math inline">\((u, v)\)</span> of <span class="math inline">\(V^2\)</span> can be thought of as the present state of a pair of surfers in <span class="math inline">\(V\)</span>, where an edge from <span class="math inline">\((u, v)\)</span> to <span class="math inline">\((c, d)\)</span> in <span class="math inline">\(G^2\)</span> says that in the original graph <span class="math inline">\(G\)</span>, one surfer can move from <span class="math inline">\(u\)</span> to <span class="math inline">\(c\)</span> while the other moves from <span class="math inline">\(v\)</span> to <span class="math inline">\(d\)</span>.</p>
            <p>A tour in <span class="math inline">\(G^2\)</span> of length <span class="math inline">\(n\)</span> represents a pair of tours in <span class="math inline">\(G\)</span> also having length <span class="math inline">\(n\)</span>. Formally, the EMD <span class="math inline">\(m(u, v)\)</span> is simply the expected distance in <span class="math inline">\(G^2\)</span> from <span class="math inline">\((u, v)\)</span> to any singleton node <span class="math inline">\((x, x)\)</span> ∈ <span class="math inline">\(V2\)</span> (where <span class="math inline">\(u = v\)</span>), since singleton nodes in <span class="math inline">\(G^2\)</span> represent states where both surfers are at the same node. More precisely,</p>
            <p><img src="../media/image596.png" /></p>
            <p>The sum is taken over all tours <span class="math inline">\(t\)</span> starting from <span class="math inline">\((u,v)\)</span> which touch a singleton node at the end and only at the end. Unfortunately, <span class="math inline">\(G^2\)</span> may not always be strongly connected (even if <span class="math inline">\(G\)</span> is), and in such cases there may be no tours <span class="math inline">\(t\)</span> for <span class="math inline">\((u,v)\)</span> in the summation. In this case, <span class="math inline">\(m(u,v) = ∞\)</span>. This definition would cause problems in defining distances for nodes from which some tours lead to singleton nodes while others lead to <span class="math inline">\((u, v)\)</span>.</p>
            <p>Solution: <strong>Expected-f Meeting distance</strong></p>
            <p>Map all distances to a finite interval: instead of computing expected length <span class="math inline">\(l(t)\)</span> of a tour, we can compute the expected <span class="math inline">\(f(l(t))\)</span>, for a nonnegative, monotonic function which is bounded on the domain <span class="math inline">\([0,∞)\)</span>.</p>
            <p><img src="../media/image597.png" /></p>
            <p>Instead of using directly <span class="math inline">\(l(t)\)</span> we use <span class="math inline">\(C^{l(t)}\)</span> and it solves the problem because close nodes have a lower score (meeting distances of 0 go to 1 and distances of <span class="math inline">\(∞\)</span> go to 0), matching our intuition of similarity.</p>
            <p><span class="math inline">\(s&#39;(a,b)=0\)</span> -&gt; when <span class="math inline">\(l(t)\)</span> becomes infinity, no tour from <span class="math inline">\((a,b)\)</span> to any singleton nodes</p>
            <p><span class="math inline">\(s&#39;(a,b)=1\)</span> -&gt; when <span class="math inline">\(a=b\)</span></p>
            <p><span class="math inline">\(s&#39;(a,b)∈[0,1]\)</span> -&gt; for all other values of <span class="math inline">\(a,b\)</span> This transformation avoid us to have infinite, when <span class="math inline">\(s(u,v)\)</span> tended to be infinite <span class="math inline">\(s&#39;(u,v)\)</span> tend to be 0.</p>
            <p>Some examples of expected-f meeting distance with C=0.8.</p>
            <p><img src="../media/image598.png" /></p>
            <p>It has been proved that the SimRank score, with parameter C, between two nodes is their expected-f meeting distance traveling back-edges, for <span class="math inline">\(f(z) = C^z\)</span>.</p>
            <p>Structure and expected f-meeting distance have similar results.</p>
            <p>In other words, <span class="math inline">\(s(u,v) = s&#39;(u,v)\)</span> for any two vertices <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>. That is, <strong>SimRank is based on both structural context and random walk, it implements both intuitions</strong>.</p>
            <h2 data-number="10.2" id="clustering-in-a-graph"><span class="header-section-number">10.2</span> Clustering in a Graph</h2>
            <p>How should we conduct <strong>clustering in a graph</strong>?</p>
            <p>Intuitively, we should cut the graph into pieces, each piece being a cluster, such that the vertices within a cluster are well connected and the vertices in different clusters are connected in a much weaker way.</p>
            <p>Inside a graph we are nodes related to each other because strongly interconnected.</p>
            <p>Let <span class="math inline">\(G = (V,E)\)</span> be a direct graph.</p>
            <p>A cut <span class="math inline">\(C(S,T)\)</span> is a partitioning of the set of vertices <span class="math inline">\(V\)</span> in <span class="math inline">\(G\)</span>, that is, <span class="math inline">\(V = S ∪ 𝑇\)</span> and <span class="math inline">\(S ∩ 𝑇 = ∅\)</span>. The cut set of a cut is the set of edges <span class="math inline">\(\{(u, v) ∈ E \| u ∈ S, v ∈ T\}\)</span></p>
            <p>When we cut the graph in subgraphs we cut edges connecting subgraphs, and cut are set of edges we cut for separating the graph.</p>
            <p>Size of the cut: number of edges in the cut set. If the edges are weighted, the value of the cut is the sum of weights. We want to cut the minimum number of edges, the minimum cut set, so that subgraphs are not related to each other.</p>
            <p>Minimum cut is good for deriving clusters in graphs, and with it the cut’s size is not greater than any other cut’s size.</p>
            <p>We have a polynomial time algorithms to compute minimum cuts of graphs (Edmonds-Karp algorithm)</p>
            <p><img src="../media/image600.png" /></p>
            <p>In this graph we can identify some clusters, but if we apply the minimum cut, that corresponds to find the minimum number of edges to cut.</p>
            <p>Doing it we separate <span class="math inline">\(l\)</span> from the rest of the graph.</p>
            <p>If we apply the minimum cut, it corresponds to find the minimum cut of edges to cut to separate the graph.</p>
            <p>We separate <span class="math inline">\(l\)</span> from the rest of the graph, but we can realize that we have two clusters.</p>
            <p>Cut <span class="math inline">\(C_2 = (\{a, b, c, d, e, f , l \}, \{g, h, i, j, k\})\)</span> leads to a much better clustering than <span class="math inline">\(C_1\)</span>. The edges in the cut set of <span class="math inline">\(C_2\)</span> are those connecting the two “natural clusters” in the graph.</p>
            <p>Specifically, for edges <span class="math inline">\((d,h)\)</span> and <span class="math inline">\((e,k)\)</span> that are in the cut set, most of the edges connecting <span class="math inline">\(d, h, e,\)</span> and <span class="math inline">\(k\)</span> belong to one cluster.</p>
            <p>We can exploit this information to introduce a better measure: <strong>Sparsity</strong>.</p>
            <h3 data-number="10.2.1" id="sparsest-cut"><span class="header-section-number">10.2.1</span> Sparsest Cut</h3>
            <p>Intuition: choose a cut where, for each vertex u that is involved in an edge in the cut set, most of the edges connecting to u belong to one cluster.</p>
            <p>The sparsity of a cut <span class="math inline">\(C = (S,T)\)</span> is defined as:</p>
            <p><img src="../media/image601.png" /></p>
            <p><strong>A cut is sparsest if its sparsity is not greater than that of any other cut.</strong></p>
            <p>Favors solutions that are both sparse (few edges crossing the cut) and balanced (close to a bisection). We cut the minimum number of edges but separating graph in subgraphs that approximately have the same size.</p>
            <p>The problem is known to be NP-Hard, and the best known algorithm is an <span class="math inline">\(O(\sqrt{log (𝑛)})\)</span> approximation.</p>
            <p>Ex. Cut <span class="math inline">\(C_2 =(\{a,b,c,d,e,f,l\},\{g,h,i,j,k\})\)</span> is the sparsest cut</p>
            <p>After execution we can measure the quality of the result.</p>
            <p>For <span class="math inline">\(k\)</span> clusters, the modularity of a clustering assesses the quality of the clustering:</p>
            <p><img src="../media/image602.png" /></p>
            <p>We have the sum of differences of those two. (module = cluster)</p>
            <p>li: number of edges between vertices in the i-th clusterdi: the sum of the degrees of the vertices in the i-th cluster where degree of a vertex u: number of edges connecting to u</p>
            <p>The modularity of a clustering of a graph is the difference between the fraction of all edges that fall into individual clusters and the fraction that would do so if the graph vertices were randomly connected.</p>
            <p>The optimal clustering of graphs maximizes the modularity.</p>
            <p>In this way we can measure the quality we obtain with graph-clustering approach.</p>
            <p>The sparsest cut problem has:</p>
            <ul>
            <li>High computational cost</li>
            </ul>
            <p>Many graph cut problems are computationally expensive</p>
            <ul>
            <li><p>The sparsest cut problem is NP-hard</p></li>
            <li><p>Need to tradeoff between efficiency/scalability and quality</p></li>
            </ul>
            <p>Sophisticated graphs</p>
            <ul>
            <li>May involve weights and/or cycles.</li>
            </ul>
            <p>High dimensionality</p>
            <ul>
            <li>A graph can have many vertices. In a similarity matrix, a vertex is represented as a vector (a row in the matrix) whose dimensionality is the number of vertices in the graph. With an high number of vertices we have an high number of features.</li>
            </ul>
            <p>Sparsity</p>
            <ul>
            <li><p>A large graph is often sparse, meaning each vertex on average connects to only a small number of other vertices</p></li>
            <li><p>A similarity matrix from a large sparse graph can also be sparse</p></li>
            </ul>
            <h3 data-number="10.2.2" id="how-to-perform-the-graph-clustering"><span class="header-section-number">10.2.2</span> How to Perform the Graph Clustering</h3>
            <p>There exist two kinds of methods:</p>
            <ul>
            <li><p>Clustering methods for high-dimensional data</p></li>
            <li><p>Clustering methods designed specifically for clustering graphs</p></li>
            </ul>
            <p><strong>Clustering methods for high-dimensional data:</strong></p>
            <ul>
            <li><p>Extract a similarity matrix from a graph using a similarity measure</p></li>
            <li><p>A clustering algorithm for high-dimensional data is therefore applied</p></li>
            </ul>
            <p><strong>Clustering methods are designed specifically for clustering graphs:</strong></p>
            <ul>
            <li>Exploit the peculiarities of the graph for performing the clustering process</li>
            </ul>
            <h4 class="unnumbered" data-number="" id="scan-density-based-clustering-of-networks">SCAN: Density-based clustering of Networks</h4>
            <p>We don’t have to fix <span class="math inline">\(k\)</span>, it follows the density but we have to define the concept of density with vertices and edges.</p>
            <p>Individuals in a tight social group, or <strong>clique</strong>, know many of the same people, regardless of the size of the group. A clique is a group with strongly interconnected components, it doesn’t depend on the size but just on the interconnection.</p>
            <p>Individuals who are <strong>hubs</strong> know many people in different groups but belong to no single group. They act as bridge multiple groups.</p>
            <p>Individuals who are <strong>outliers</strong> reside at the margins of society. Hermits, for example, know few people and belong to no group To identify them we exploit the SCAN approach.</p>
            <p><strong>We define the Neighborhood of a Vertex:</strong></p>
            <p>Define <span class="math inline">\(Γ(v)\)</span> as the immediate neighborhood of a vertex (i.e. the set of people that an individual knows ), vertices connected by the edge.</p>
            <p><img src="../media/image603.png" /></p>
            <p>The desired features tend to be captured by a measure, called <strong>Structural Similarity</strong>:</p>
            <p><img src="../media/image604.png" /></p>
            <p>Ratio between the cardinality of the intersection of the neighborhood of <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> over the square root of the product of cardinalities.</p>
            <p>The neighborhood of <span class="math inline">\(u\)</span>:</p>
            <p><img src="../media/image605.png" /></p>
            <p>We also have u itself.</p>
            <p>If we consider <span class="math inline">\(v\)</span>, we have yellow vertices and <span class="math inline">\(v\)</span> itself.</p>
            <p>Structural similarity is large for members of a clique and small for hubs and outliers.</p>
            <p>This, because in case of cliques the numerator is high because we expect the neighborhood of it and neighbors has a lot of common edges, the cardinality of the two are approximately similar and the value of denominator is like the value of the numerator.</p>
            <p>In the case of an outlier, we expect that the numerator is low because its neighborhood is very limited. The connection between an outlier and another graph is limited.</p>
            <p>In the denominator, the neighborhood of the outlier is low, but the neighborhood of connected vertices may not be low.</p>
            <p>In the case of a hub, for example 6 in the figure, we have that the numerator is not very high and the denominator the neighborhood of 6 is quite high.</p>
            <p><img src="../media/image606.png" /></p>
            <p>SCAN uses a similarity threshold <span class="math inline">\(\varepsilon\)</span> to define the cluster membership.</p>
            <p>For a vertex <span class="math inline">\(v ∈ V\)</span>, the <strong>ɛ-Neighborhood</strong> of <span class="math inline">\(v\)</span> is defined as:</p>
            <p><img src="../media/image607.png" /></p>
            <p>set of neighbors such that the structure of similarity is higher or equal to epsilon.</p>
            <p>A core vertex is a vertex inside of a cluster.</p>
            <p><span class="math inline">\(v\)</span> is a core vertex if and only if:</p>
            <p><img src="../media/image608.png" /></p>
            <p>where 𝜇 is a popularity threshold.</p>
            <p>SCAN grows cluster from core vertices (similar to DBSCAN).</p>
            <p>If a vertex <span class="math inline">\(v\)</span> is in the ɛ-Neighborhood of a core <span class="math inline">\(u\)</span>, then <span class="math inline">\(v\)</span> is assigned to the same cluster as <span class="math inline">\(u\)</span>.</p>
            <p>We start from a core vertex and we grow the cluster.</p>
            <p>The growing process continues until no cluster can be further grown.</p>
            <p>Formally, a vertex <span class="math inline">\(w\)</span> can be directly reached from a core <span class="math inline">\(v\)</span> if</p>
            <p><img src="../media/image609.png" /></p>
            <p><span class="math inline">\(v\)</span> is core and <span class="math inline">\(w\)</span> belong to the eps-neighborhood of <span class="math inline">\(v\)</span>.</p>
            <p>Structure reachable: transitive closure of direct structure reachability. A vertex v can be reached from a core vertex u if there exist vertices w1, ..., wn such that w1 can be reached from u, wi can be reached from wi-1, for 1&lt;i&lt;=n, and v can be reached from wn.</p>
            <p>Structure connected: two vertices v and w, which may or may not be cores, are said connected there exists a core u such that v and w can be reached from u.</p>
            <p><img src="../media/image610.png" /></p>
            <p>We must identify u such that v and w can be reached from u with the structure reachability we identified before.</p>
            <ul>
            <li><p>Structure-connected cluster C is characterized by:</p>
            <p><strong>Connectivity:</strong></p>
            <p><img src="../media/image611.png" /></p>
            <p><strong>Maximality:</strong></p>
            <p><img src="../media/image612.png" /></p></li>
            </ul>
            <p>Hubs do not belong to any cluster and bridge to many clusters.</p>
            <p>Outliers do not belong to any cluster and connect to less clusters.</p>
            <p><img src="../media/image613.png" /></p>
            <p><img src="../media/image614.png" /></p>
            <p>The result is strongly dependent on the parameters we use.</p>
            <p>We identify possible outliers and hub, and we distinguish between them if we have connections between them and clusters.</p>
            <p>An example is the following. We want to define clusters, hubs and outliers. We set:</p>
            <p><img src="../media/image615.png" /></p>
            <p>We can start from any vertex; we start from 13.</p>
            <p><img src="../media/image616.png" /></p>
            <p>We compute the structure of similarity to identify if it’s a core.</p>
            <p>We have to identify to do that the neighborhood of v and w, the neighbors of 13 are 9 and 13.</p>
            <p>The neighborhood of 9 has 9, 8, 10, 12, 13.</p>
            <p>The cardinality of the neighborhood of 9 is 5, of 13 is 2 and the intersection is composed by 9 and 13.</p>
            <p>= 2/sqrt(2*5) = 0.63.</p>
            <p>We have to compare it with epsilon to decide if 13 can be a core and we discover that it is lower, we are sure that 13 is not a core vertex.</p>
            <p>The eps-neighborhood of v is empty.</p>
            <p>It can be an hub or an outlier.</p>
            <p>We extract another vertex to decide if it’s a core, repeating the consideration we’ve seen.</p>
            <p>Let’s see 8.</p>
            <p><img src="../media/image617.png" /></p>
            <p>We will repeat the computation with 7,9 and 12.</p>
            <p>Considering 9, 8 has 4 vertices in the neighborhood, 9 has 5 and the intersection is composed by 8,9 and 12.</p>
            <p>= 4/sqrt(20) = 0.67.</p>
            <p>In case of 12, we have 6 elements in its neighborhood, 4 vertices in the neighborhood of 8 and the intersection is composed by 4 elements.</p>
            <p><img src="../media/image618.png" /></p>
            <p>= 4/sqrt(24) = 0.82.</p>
            <p>The same for 7.</p>
            <p><img src="../media/image619.png" /></p>
            <p>In our case we don’t have enough vertices inside the eps-neighborhood.</p>
            <p><img src="../media/image620.png" /><img src="../media/image621.png" /></p>
            <p>For 12, the structure similarity is quite high.</p>
            <p>We can conclude 12 is a core vertex, we start the cluster and we add the neighbors of 12 in the cluster.</p>
            <p>We try to understand if we can include also other points.</p>
            <p>We have to compute if 6 is really a core point to include in our cluster.</p>
            <p><img src="../media/image622.png" /></p>
            <p>6 is not a core point and cannot be included in the clusters because it acts as a bridge between the two clusters.</p>
            <p><img src="../media/image623.png" /></p>
            <p>We can continue with 5, and so on.</p>
            <p>At the end we will have two clusters, and 13 and 6 non included in the clusters.</p>
            <p>But we can elect 6 as hub and 13 as an outlier, because:</p>
            <p><img src="../media/image624.png" /></p>
            <p><strong>Running time</strong></p>
            <p>The running time is <span class="math inline">\(O(|E|)\)</span>. For sparse networks it is <span class="math inline">\(O(|V|)\)</span></p>
            <p>With the increase of the number of vertices the running time is stable.</p>
            <p>The results we obtain are still strongly related to the parameters we choose.</p>
            <h1 data-number="11" id="advanced-frequent-pattern-analysis"><span class="header-section-number">11</span> Advanced Frequent Pattern Analysis</h1>
            <p>Before applying some ML algorithms it’s useful to apply some analysis.</p>
            <h2 data-number="11.1" id="pattern-mining-in-multi-level-multi-dimensional-space"><span class="header-section-number">11.1</span> Pattern Mining in Multi-Level, Multi-Dimensional Space</h2>
            <h3 data-number="11.1.1" id="mining-multiple-level-association-rules"><span class="header-section-number">11.1.1</span> Mining Multiple-Level Association Rules</h3>
            <p>Items often form hierarchies in our application domain, for example we can have milk specialized in different kinds of milk. Considering the hierarchy, items at the lower level are expected to have lower support; they are derived items and has of course a lower support than parents. It should be used different minsup’s depending on the level we consider. In higher levels we could use an higher min_sup.</p>
            <p><img src="../media/image625.png" /></p>
            <p><strong>Flexible min-support thresholds</strong> are also useful for other thigs. For example, considering that some items are more valuable but less frequent.</p>
            <ul>
            <li>Use non-uniform, group-based min-support. E.g., {diamond, watch, camera}: 0.05%; {bread, milk}: 5%;</li>
            </ul>
            <p>If we want to analyze this valuable pattern we should use a different minsup.</p>
            <p><strong>Redundancy Filtering</strong>: Some rules may be redundant due to “ancestor” relationships between items.</p>
            <ul>
            <li>milk -&gt; wheat bread [support = 8%, confidence = 70%]</li>
            <li>2% milk -&gt; wheat bread [support = 2%, confidence = 72%]</li>
            </ul>
            <p>The first rule is an ancestor of the second rule.</p>
            <p>A rule <strong>is <em>redundant</em> if its support and confidence are close to the “expected” value</strong>, based on the rule’s ancestor.</p>
            <p>The support of the derived rule is 2%, while for milk -&gt; wheat bread is 8%, but this is the relation in terms of percentage of these items in these transactions that we have in terms of milk and 2% milk.This support is what we expect just considering the ratio between the transaction in which we have milk and in which we have 2% milk.</p>
            <p>We can avoid to consider the most specialized rule because support and confidence can be derived directly from that association.</p>
            <h3 data-number="11.1.2" id="mining-multi-dimensional-association"><span class="header-section-number">11.1.2</span> Mining Multi-Dimensional Association</h3>
            <p>There’s another interesting problem. If we assume that the predicate is the same, we have single-dimensional rules:</p>
            <p><strong>buys(X, “milk”) -&gt; buys(X, “bread”)</strong></p>
            <p>We can have also multi-dimensional rules, that consider &gt;= 2 dimensions or predicates.</p>
            <ul>
            <li>Inter-dimensional association rules (<em>no repeated predicates</em>): <strong>age(X,“19-25”) &amp; occupation(X,“student”) -&gt; buys(X, “coke”)</strong></li>
            <li>‎hybrid-dimensional association rules (<em>repeated predicates</em>): <strong>age(X,“19-25”) &amp; buys(X, “popcorn”) -&gt; buys(X, “coke”)</strong></li>
            </ul>
            <h3 data-number="11.1.3" id="mining-quantitative-association"><span class="header-section-number">11.1.3</span> Mining Quantitative Association</h3>
            <p>There’s another interesting problem, which regards quantitative attributes: numeric, implicit ordering among values. We have to apply some discretization, that can be <em>static discretization</em> (predefined concept hierarchies) or <em>dynamic discretization</em> (binning and clustering). Techniques to discretize can be categorized by how numerical attributes, such as age or salary are treated:</p>
            <p>1.<strong>Static discretization</strong> based on predefined concept hierarchies(data cube methods)</p>
            <p>2.<strong>Dynamic discretization</strong> based on data distribution</p>
            <p>3.<strong>Clustering</strong>: Distance-based association</p>
            <p>4.<strong>Deviation</strong>: we can express rules working with specific deviation from the standard value.</p>
            <p><em>Sex = female =&gt; Wage: mean=$7/hr (overall mean = $9)</em></p>
            <h4 class="unnumbered" data-number="" id="static-discretization-of-quantitative-attributes"><strong>Static Discretization of Quantitative Attributes</strong></h4>
            <p>We try to summarize a number of values using hierarchies, we can use higher values of the hierarchy as items in frequent pattern analysis. Some things to know:</p>
            <ul>
            <li>Discretized prior to mining using concept hierarchy.</li>
            <li>Numeric values are replaced by ranges.</li>
            <li>In relational database, finding all frequent k-predicate sets will require k table scans.</li>
            <li>Data cube is well suited for mining.</li>
            <li>The cells of an n-dimensional cuboid correspond to the predicate sets.</li>
            <li>Mining from data cubes can be much faster.</li>
            </ul>
            <p><img src="../media/image626.png" /></p>
            <h4 class="unnumbered" data-number="" id="quantitative-association-rules-based-on-statistical-inference-theory"><strong>Quantitative Association Rules Based on Statistical Inference Theory</strong></h4>
            <p>Finding extraordinary and therefore interesting phenomena, <em>E.g., (Sex = female) =&gt; Wage: mean=$7/hr (overall mean = $9)</em>.</p>
            <ul>
            <li>LHS: a subset of the population (left part of the rule).</li>
            <li>RHS: an extraordinary behavior of this subset (right part)The rule is accepted only if a statistical test confirms the inference with high confidence.</li>
            </ul>
            <p>Some subrule can highlights the extraordinary behavior of a subset of the pop. of the super rule <em>E.g., (Sex = female) ^ (South = yes) =&gt; mean wage = $6.3/hr</em></p>
            <p>Two <strong>forms of rules</strong>:</p>
            <ul>
            <li><em>Categorical</em>=&gt;quantitative rules</li>
            </ul>
            <p>Or</p>
            <ul>
            <li><em>Quantitative</em>=&gt; quantitative rules</li>
            </ul>
            <p><em>E.g., Education in [14-18] (yrs) =&gt; mean wage = $11.64/hr</em></p>
            <p>Open problem is to find efficient methods for LHS containing two or more quantitative attributes.</p>
            <h3 data-number="11.1.4" id="mining-rare-patterns-and-negative-patterns"><span class="header-section-number">11.1.4</span> Mining Rare Patterns and Negative Patterns</h3>
            <p><strong>Rare patterns</strong> have very low support but interesting.</p>
            <ul>
            <li>E.g., buying Rolex watches. We know we have to use a very low minsup.</li>
            <li><em>Mining:</em> Setting individual-based or special group-based support threshold for valuable items.</li>
            </ul>
            <p>In <strong>negative (correlated) patterns</strong>, since it is unlikely that one buys Ford Expedition (an SUV car) and Toyota Prius (a hybrid car) together, Ford Expedition and Toyota Prius are likely negatively correlated patterns.</p>
            <p><strong>Negatively correlated patterns</strong> that are infrequent tend to be more interesting than those that are frequent.</p>
            <p>We try to define negative correlated patterns.</p>
            <ul>
            <li><strong><em>Definition 1 (support-based)</em></strong></li>
            </ul>
            <blockquote>
            <p>If itemsets X and Y are both frequent but rarely occur together, i.e., sup(X U Y) &lt; sup (X) * sup(Y) then X and Y are negatively correlated.</p>
            <p>If the frequency of purchasing together X and Y is lower than the expected frequency (the product, it’s like we consider them as independent) we suppose the two items are negatively correlated.</p>
            <p><strong>Problem:</strong> A store sold two needle 100 packages A and B, only one transaction containing both A and B.</p>
            <ul>
            <li><p>When there are in total 200 transactions, we haves(A U B) = 1/200 = 0.005, s(A) * s(B) = 100/200 * 100/200 = 0.25, s(A U B) &lt; s(A) * s(B)</p></li>
            <li><p>When there are 10^5 transactions, we haves(A U B) = 1/10^5, s(A) * s(B) = 100/10^5 * 100/10^5 = 1/10^3 * 1/10^3, s(A U B) &gt; s(A) * s(B)</p></li>
            </ul>
            <p>Where is the problem? —This formula is affected by null transactions, i.e., the support-based definition is not null-invariant! If this number is very high we lose their relation.</p>
            </blockquote>
            <ul>
            <li><strong><em>Definition 2 (negative itemset-based)</em></strong></li>
            </ul>
            <blockquote>
            <p>If X and Y are strongly negatively correlated, then</p>
            <p><img src="../media/image627.png" /></p>
            <p>Also this definition suffers from the null-variant problem.</p>
            <p>Let’s consider 200 transactions:</p>
            <p><img src="../media/image628.png" /></p>
            <p>Considering 10^6 transactions:</p>
            <p><img src="../media/image629.png" /></p>
            </blockquote>
            <ul>
            <li><strong><em>Definition 3 (Kulczynski measure-based)</em></strong></li>
            </ul>
            <blockquote>
            <p>If itemsets X and Y are frequent, but (P(X|Y) + P(Y|X))/2 &lt; є, where є is a negative pattern threshold, then X and Y are negatively correlated.</p>
            <p>Ex. For the same needle package problem, when no matter there are 200 or 105 transactions, if min_sup = 0.01% and є = 0.02, we have</p>
            <p><img src="../media/image630.png" /></p>
            <p><img src="../media/image631.png" /></p>
            </blockquote>
            <h2 data-number="11.2" id="constraint-based-frequent-pattern-mining"><span class="header-section-number">11.2</span> Constraint-Based Frequent Pattern Mining</h2>
            <p>Finding <strong>all</strong> the patterns in a database <strong>autonomously</strong> is unrealistic! The patterns could be too many but not focused! Data mining should be an <strong>interactive process</strong>. User directs what to be mined using a data mining query language (or a graphical user interface).</p>
            <p>In <strong>constraint-based mining</strong> we provide constraints on what to be mined (<em>user flexibility</em>) and explores such constraints for efficient mining (<em>optimization</em>).</p>
            <p>Note: still find all the answers satisfying constraints, not finding some answers in “heuristic search”.</p>
            <h3 data-number="11.2.1" id="constraints-in-data-mining"><span class="header-section-number">11.2.1</span> Constraints in data Mining</h3>
            <ul>
            <li><p><strong>Knowledge type constraint</strong>: classification, association, etc.</p></li>
            <li><p><strong>Data constraint</strong> — using SQL-like queries: find product pairs sold together in stores in Chicago this year.</p></li>
            <li><p><strong>Dimension/level constraint</strong> - in relevance to region, price, brand, customer category.</p></li>
            <li><p><strong>Rule (or pattern) constraint</strong>: small sales (price &lt; $10) triggers big sales (sum &gt; $200).</p></li>
            <li><p><strong>Interestingness constraint</strong>: strong rules: min_support &gt;= 3%, min_confidence &gt;= 60%, what we used with association rules.</p></li>
            </ul>
            <h3 data-number="11.2.2" id="meta-rule-guided-mining"><span class="header-section-number">11.2.2</span> Meta-Rule Guided Mining</h3>
            <p>In applying constraints we can exploit <strong>metarules</strong>. A metarule forms a hypothesis regarding the relationships that the user is interested in probing or confirming. Meta-rule can be in the rule form with partially instantiated predicates and constants</p>
            <p><em>P1(X, Y) ^ P2(X, W) =&gt; buys(X, “iPad”)</em></p>
            <p>We are interested in determining which type of customer buys iPad. The resulting rule derived can be <em>age(X, “15-25”) ^ profession(X, “student”) =&gt; buys(X, “iPad”)</em></p>
            <p>Fixing the metarule we are looking for association rules with two predicates in the left side and one in the right side with predicate buys and what to buy. In general, it can be in the form of <em>P1 ^ P2 ^ ... ^ Pl =&gt; Q1 ^ Q2 ^ ... ^ Qr</em></p>
            <h3 data-number="11.2.3" id="constraint-based-frequent-pattern-mining-1"><span class="header-section-number">11.2.3</span> Constraint-Based Frequent Pattern Mining</h3>
            <p><img src="../media/image632.png" /></p>
            <p>Dimension/level constraints and interestingness constraints can be applied after mining to filter out discovered rules, hough it is generally more efficient and less expensive to use them during mining to help prune the search space. We can use them to improve the performance of our analysis.</p>
            <p><strong>How can we use rule constraints to prune the search space?</strong> More specifically, what kind of rule constraints can be ‘pushed’ deep into the mining process and still ensure the completeness of the answer returned for a mining query?</p>
            <p>We can work in two directions:</p>
            <ul>
            <li><p><strong>Pruning pattern search space:</strong> The former checks candidate patterns and decides whether a pattern can be pruned using the constraints.</p></li>
            <li><p><strong>Pruning data search space:</strong> checks the data set to determine whether the particular data piece will be able to contribute to the subsequent generation of satisfiable patterns (for a particular pattern) in the remaining mining process. We just elaborate data to understand if a data piece contributes to the satisfiable generation.</p></li>
            </ul>
            <p><strong>Pattern space pruning constraints</strong> has 4 types:</p>
            <ul>
            <li><p><strong>Anti-monotonic</strong>: If constraint c is violated, its further mining can be terminated</p></li>
            <li><p><strong>Monotonic</strong>: If c is satisfied, no need to check c again</p></li>
            <li><p><strong>Succinct</strong>: c must be satisfied, so one can start with the data sets satisfying c</p></li>
            <li><p><strong>Convertible</strong>: c is not monotonic nor anti-monotonic, but it can be converted into it if items in the transaction can be properly ordered</p></li>
            </ul>
            <p>Allow us to prune directly eliminate candidate itemset because we are sure that these cannot be satisfied using constraints.</p>
            <p><strong>Dataspace pruning constraints:</strong></p>
            <ul>
            <li><p><strong>Data succinct:</strong> Data space can be pruned at the initial pattern mining process</p></li>
            <li><p><strong>Data anti-monotonic:</strong> If a transaction t does not satisfy c, t can be pruned from its further mining</p></li>
            </ul>
            <p>These constraints allow us to reduce the number of transactions. The idea is to exploit constraints to reduce computational time. We have constraints on the pattern space and on the data space.</p>
            <h4 class="unnumbered" data-number="" id="pattern-space-pruning-with-anti-monotonicity-constraints"><strong>Pattern Space Pruning with Anti-Monotonicity Constraints</strong></h4>
            <p>A constraint C is <strong>anti-monotone</strong> if the super pattern satisfies C, all of its sub-patterns do so too. In other words, <strong>anti-monotonicity</strong>: If an itemset S <em>violates</em> the constraint, so does any of its superset.</p>
            <p><em>Ex. 1. sum(I.price) &lt;= v is anti-monotone</em>, because adding other elements we cannot reduce the sum</p>
            <p><img src="../media/image633.png" /></p>
            <p><em>Ex. 2. range(I.profit) &lt;= 15 is anti-monotone</em>, considering in the itemset items in which the range is higher we cannot reduce the range adding items in the itermset. Itemset ab violates C, the range is 40 and violates the constraint on the range. So does every superset of ab.</p>
            <p><em>Ex. 3. sum(I.price) &gt;= v is not anti-monotone</em>, because we can have other items and the sum increases but the constraint is satisfied in any case</p>
            <p><em>Ex. 4. support count is anti-monotone:</em> this is the core property used in Apriori.</p>
            <p>Another approach we can adopt, is to use the apriori algorithm to generate all possible frequent patterns and them impose the satisfaction of constraints. This imply that we need time to generate a very high number of patterns and then reduce this number imposing satisfaction of constraints. With the reasoning we’ve seen we avoid to generate patterns not able to satisfy constraints, we reduce the computational time.</p>
            <h4 class="unnumbered" data-number="" id="pattern-space-pruning-with-monotonicity-constraints"><strong>Pattern Space Pruning with Monotonicity Constraints</strong></h4>
            <p>A constraint C is <strong>monotone</strong> if the pattern satisfies C, we do not need to check C in subsequent mining. ernatively, monotonicity: If an itemset S <strong>satisfies</strong> the constraint, so does any of its superset.</p>
            <p><img src="../media/image634.png" /></p>
            <p>If we are sure a constraint is satisfied we cannot avoid to verify the verification for super-itemsets.</p>
            <p><em>Ex. 1. sum(I.Price) &lt;= v is monotone</em> If we add other items I can only increase the sum and not decrease, increasing the number of items we enforce the satisfaction.</p>
            <p><em>Ex. 2. min(I.Price) &lt;= v is monotone</em> If the min is lower a threshold, if we add another element the min can decrease but not increase.</p>
            <p><em>Ex. 3. C: range(I.profit) &gt;= 15.</em> Itemset ab satisfies C. So does every superset of ab.</p>
            <h4 class="unnumbered" data-number="" id="pattern-space-pruning-with-succinctness"><strong>Pattern Space Pruning with Succinctness</strong></h4>
            <p><strong>We can enumerate all and only those sets that are guaranteed to satisfy the constraint</strong>. That is, if a rule constraint is succinct, we can directly generate precisely the sets that satisfy it, even before support counting begins. The idea is to have possible constraints that allow us to enumerate only sets that are able to satisfy the constraints. This avoids the substantial overhead of the generate-and- test paradigm (constraints are precounting prunable).</p>
            <p><em>min(I.Price) &gt;= v</em> is succinct <em>sum(I.Price) &gt;= v</em> is not succinct</p>
            <p>because we can explicitly and precisely generate all the itemsets that satisfy the constraint (there exists a precise formula, we do not need check the rule constraint). We can directly enumerate items with property price that can be used in the itemset. All items lower than v cannot be used to satisfy that constraint.</p>
            <p><strong>Succinctness:</strong> Given A1, the set of items satisfying a succinctness constraint C, then any set I satisfying C is based on A1 , i.e., I contains a subset belonging to A1.</p>
            <p><em>Idea:</em> Without looking at the transaction database, whether an itemset I satisfies constraint C can be determined based on the selection of items.</p>
            <p><em>Optimization:</em> If C is succinct, C is pre-counting pushable</p>
            <h4 class="unnumbered" data-number="" id="example-of-the-apriori-algorithm">Example of the Apriori Algorithm</h4>
            <p><img src="../media/image635.png" /></p>
            <p>In front of constraints, we use the <strong>Naive Algorithm: Apriori + Constraint:</strong></p>
            <p><img src="../media/image636.png" /></p>
            <p>We have the possibility to verify that for an itemset this is not verified. We reduce the computational time terminating the execution of Apriori algorithm than usual. The same <strong>using succinct</strong>.</p>
            <p><img src="../media/image637.png" /></p>
            <p>We can use convertible constraint.</p>
            <h3 data-number="11.2.4" id="convertible-constraints-ordering-data-in-transactions"><span class="header-section-number">11.2.4</span> <strong>Convertible Constraints: Ordering Data in Transactions</strong></h3>
            <p>The idea is to <strong>convert tough constraints into anti- monotone or monotone by properly ordering items.</strong></p>
            <p><img src="../media/image638.png" /></p>
            <ul>
            <li>Examine C: <em>avg(S.profit) &gt;= 25</em></li>
            </ul>
            <p>Order items in value-descending order: &lt;a, f, g, d, b, h, c, e&gt;</p>
            <p>If an itemset afb violates C, so does also <em>afbh, afb</em>*. In this case we can transform that constraint into an anti-monotone or anti-monotone constraint. In our case, it becomes <strong>anti-monotone</strong>! If afb violates the constraint we can add other items in the lower position and what we can obtain is that this average decreases more, so this constraint is not anti-monotone in origin but if we order items in respect to the property we are testing we can transform it in an anti-monotone constraint.</p>
            <h4 class="unnumbered" data-number="" id="strongly-convertible-constraints">Strongly Convertible Constraints</h4>
            <ul>
            <li><p><em>avg(X) &gt;= 25</em> is convertible anti-monotone w.r.t. item <strong>value descending</strong> order R: &lt;a, f, g, d, b, h, c, e&gt;. If an itemset af violates a constraint C, so does every itemset with af as prefix, such as afd. When we add items, the average decreases because we sorted items in decreasing order with respect to the problem we are testing.</p></li>
            <li><p><em>avg(X) &gt;= 25</em> is convertible monotone w.r.t. item <strong>value ascending</strong> order R^-1: &lt;e, c, h, b, d, g, f, a&gt;. If an itemset d satisfies a constraint C, so does itemsets df and dfa, which having d as a prefix. We are using values ascending so values increase. Higher values allow us to satisfy more the constraint we have.</p></li>
            <li><p>Thus, avg(X) &gt;= 25 <strong>is strongly convertible</strong>.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="can-apriori-handle-convertible-constraints">Can Apriori Handle Convertible Constraints?</h4>
            <p>A convertible, not monotone nor anti-monotone nor succinct constraint cannot be pushed deep into the an Apriori mining algorithm. Within the level wise framework, no direct pruning based on the constraint can be made.</p>
            <p>Itemset df violates constraint C: <em>avg(X) &gt;= 25</em>.</p>
            <p>Since adf satisfies C, Apriori needs df to assemble adf, df cannot be pruned. But it can be pushed into frequent-pattern growth framework!</p>
            <p><img src="../media/image639.png" /></p>
            <p>C: <em>avg(X) &gt;= 25, min_sup=2</em> are the constraints we want to satisfy. We list items in every transaction in value descending order R: &lt;a, f, g, d, b, h, c, e&gt;. C is convertible anti-monotone w.r.t. R.</p>
            <p>We scan TDB once, remove infrequent items. Item h is dropped. Itemsets a and f are good, because they are frequent.</p>
            <p><strong>Projection-based mining:</strong> We project our dataset and imposing an appropriate order on item projection. Many tough constraints can be converted into (anti)-monotone.</p>
            <p>### Handling Multiple Constraints</p>
            <p>Different constraints may require different or even conflicting item-ordering. If there exists an order R s.t. both C1 and C2 are convertible w.r.t. R, then there is no conflict between the two convertible constraints.</p>
            <p>If <strong>there exists conflict</strong> on order of items, we try to satisfy one constraint first and then using the order for the other constraint try to mine frequent itemsets in the corresponding projected database.</p>
            <h4 class="unnumbered" data-number="" id="what-constraints-are-convertible">What Constraints Are Convertible?</h4>
            <p><img src="../media/image640.png" /></p>
            <p>If one constraint is convertible anti-monotone and monotone, it is defined as strongly convertible.</p>
            <h3 data-number="11.2.5" id="data-space-pruning-with-data-anti-monotonicity"><span class="header-section-number">11.2.5</span> <strong>Data Space Pruning with Data Anti-monotonicity</strong></h3>
            <p>A constraint c is data anti-monotone if for a pattern p which cannot be satisfied by a transaction t under c, p’s superset cannot be satisfied by t under c either. This means that we can avoid to consider t for verifying the pattern, the superset of p.</p>
            <p>The key for data anti-motone is recursive data reduction.</p>
            <p><img src="../media/image641.png" /></p>
            <ul>
            <li><p>Ex. 1 sum(I.Price) &gt;= v is data anti-monotone</p></li>
            <li><p>Ex. 2. min(I.Price) &lt;= v is data anti-monotone</p></li>
            <li><p>Ex. 3. C: range(I.profit) &gt;= 25 is data anti-monotone. If we have Itemset {b, c}‘s projected DB -&gt; If we have transaction T10’: {d, f, h}, when we project along along {b,c}, the other part of the transaction we have available {d, f, h}. If we consider the constraint C it cannot be satisgfied by T10’. The profit for d is -15, for f is -10 and for h is -5. But the range of b and c is already 20. If I include d,f and h, this range cannot be increases. Since C cannot be satisfied by T10’, T10’ can be pruned. T20’: {d, f, g, h}, T30’: {d, f, g} … This means that I can just prune T10’ because cannot let us to satisfy constraints.</p></li>
            </ul>
            <p>We reduce the transactions I have just to analyze for verifying if the items can be frequent. We reduce the number of transactions we observe for verifying the frequency in the itemset. We determine it considering if the transaction can help us to satisfy constrains.</p>
            <p><img src="../media/image642.png" /></p>
            <p>I cannot perform data space pruning at the beginning, but we have to perform during the execution of apriori algorithm. We can eliminate items because not frequent.</p>
            <p>This is a list of constraint-based mining:</p>
            <p><img src="../media/image643.png" /></p>
            <p>A classification of constraints:</p>
            <p><img src="../media/image644.png" /></p>
            <h2 data-number="11.3" id="mining-high-dimensional-data-and-colossal-patterns"><span class="header-section-number">11.3</span> <strong>Mining High-Dimensional Data and Colossal Patterns</strong></h2>
            <p>We have many algorithms, but can we mine large (i.e., colossal) patterns? ― such as just size around 50 to 100? <strong>Unfortunately, not!</strong> Why not? ― <strong>the curse of “downward closure”</strong> of frequent patterns is the problem.</p>
            <p>The “downward closure” property is the following:</p>
            <pre><code>Any sub-pattern of a frequent pattern is frequent.

            We have to generate all possible subpatterns to generate the colossal pattern.</code></pre>
            <p><strong>Example</strong>. If (a1, a2, ..., a100) is frequent, then a1, a2, ..., a100, (a1, a2), (a1, a3), ..., (a1, a100), (a1, a2, a3), ... must all be frequent! There are about 2^100 such frequent itemsets! No matter using breadth-first search (e.g., Apriori) or depth-first search (FPgrowth), we have to examine so many patterns.</p>
            <p><strong>Thus, the downward closure property leads to explosion!</strong></p>
            <p><img src="../media/image645.png" /></p>
            <p>Closed/maximal patterns may partially alleviate the problem but not really solve it: We often need to mine scattered large patterns!</p>
            <p>Let the minimum support threshold σ= 20</p>
            <p><img src="../media/image646.png" /></p>
            <p>If we want only to represent the output in any case, we have an explotion of possible output, all possible combination are frequent pattern.</p>
            <p>Also, if we have a limited number of transactions, if we have an high number of items and they can generate a lot of frequent itemsets if we provide only closed patterns we have in any case an explotion of closed patterns.</p>
            <p><img src="../media/image647.png" /></p>
            <h3 data-number="11.3.1" id="mining-colossal-patterns-motivation-and-philosophy"><span class="header-section-number">11.3.1</span> Mining Colossal Patterns: Motivation and Philosophy</h3>
            <p>It is often the case that only a small number of patterns are colossal, i.e., of large size. Colossal patterns are usually attached with greater importance than those of small pattern sizes.</p>
            <p>We have <strong>no hope for completeness</strong>. If the mining of mid-sized patterns is explosive in size, there is no hope to find colossal patterns efficiently by insisting “complete set” mining philosophy, It’s almost impossible to generate all possible pattern to arrive to generate a colossal pattern.</p>
            <p><strong>Jumping out of the swamp of the mid-sized results:</strong> What we may develop is a philosophy that may jump out of the swamp of mid-sized results that are explosive in size and jump to reach colossal patterns directly. We can try to avoid the overall generation of min-size patterns and combine them to jump forward the colossal pattern.</p>
            <p><strong>Striving for mining almost complete colossal patterns:</strong> The key is to develop a mechanism that may quickly reach colossal patterns and discover most of them. The risk is that we cannot find them all, we must find a trade-off between computation and optimal results.</p>
            <h3 data-number="11.3.2" id="alas-a-show-of-colossal-pattern-mining"><span class="header-section-number">11.3.2</span> Alas, A Show of Colossal Pattern Mining!</h3>
            <p><img src="../media/image648.png" /></p>
            <p>Let the min-support threshold σ= 20. Then there are <img src="../media/image649.png" /> closed/maximal frequent patterns of size 20.</p>
            <p>However, there is only one with size greater than 20, (it is the colossal): It is α= {41,42,...,79} of size 39.</p>
            <p>The existing fastest mining algorithms (e.g., FPClose, LCM) fail to complete running, they have to generate all possible pattern to generate the colossal one. The algorithm we analyze now outputs this colossal pattern in seconds, but this does not guarantee to generate all possible colossal patterns, but we will have a good subset.</p>
            <h3 data-number="11.3.3" id="pattern-fusion-strategy"><span class="header-section-number">11.3.3</span> <strong>Pattern-Fusion Strategy</strong></h3>
            <p>It cannot guarantee to mine them all but can guarantee an approximation with reasonable time. <strong>Pattern-Fusion traverses the tree in a bounded-breadth way.</strong> Always pushes down a frontier of a bounded-size candidate pool. Only a fixed number of patterns in the current candidate pool will be used as the starting nodes to go down in the pattern tree ― thus avoids the exponential search space. We traverse the tree in such a way to generate only pattern with high elements that allow us to go towards colossal patterns.</p>
            <p><strong>Pattern-Fusion identifies “shortcuts” whenever possible.</strong> Pattern growth is not performed by single-item addition but by leaps and bounded: agglomeration of multiple patterns in the pool.</p>
            <p>We don’t add one item each item, but we combine itemsets using shortcuts to produce the colossal itemsets.</p>
            <p>These shortcuts will direct the search down the tree much more rapidly towards the colossal patterns.</p>
            <p><img src="../media/image650.png" /></p>
            <h4 class="unnumbered" data-number="" id="observation-colossal-patterns-and-core-patterns">Observation: Colossal Patterns and Core Patterns</h4>
            <p>We have to understand how to combine them correctly to go towards colossal patterns.</p>
            <p><img src="../media/image651.png" /></p>
            <p>When we eliminate items in the colossal pattern we can assume the support of the subitemset we created is very close to the support of the colossal pattern.</p>
            <p>Subpatterns α1 to αk cluster tightly around the colossal pattern α by sharing a similar support. We call such subpatterns core patterns of α.</p>
            <p>We go towards the subpatterns of the colossal pattern with support similar to the colossal pattern.</p>
            <h4 class="unnumbered" data-number="" id="robustness-of-colossal-patterns">Robustness of Colossal Patterns</h4>
            <p><strong>Core Patterns</strong> Intuitively, for a frequent pattern α, a subpattern β is a τ-core pattern of α if β shares a similar support set with α, i.e.,</p>
            <p><img src="../media/image652.png" /></p>
            <p>where |Dα| is the number of patterns containing α and τ is called the core ratio. The number of patterns containing beta is higher than the one containing alpha, because alpha is a superpattern of beta. We have to fix tau to define them.</p>
            <p><strong>Robustness of Colossal Patterns</strong> A colossal pattern is robust in the sense that it tends to have much more core patterns than small patterns.</p>
            <p><strong>(d,τ)-robustness</strong> A pattern α is (d, τ)-robust if d is the maximum number of items that can be removed from α for the resulting pattern to remain a τ-core pattern of α.</p>
            <p>For a (d,τ)-robust pattern α, it has<img src="../media/image653.png" />(2^d) core patterns. Colossal patterns tend to have a large number of core patterns.</p>
            <p><strong>Pattern distance</strong>: For patterns α and β, the pattern distance of α and β is defined to be</p>
            <p><img src="../media/image654.png" /></p>
            <p>If two patterns α and β are both core patterns of a same pattern, they would be bounded by a “ball” of a radius specified by their core ratio τ.</p>
            <p><img src="../media/image655.png" /></p>
            <p>The distance between alpha and beta will be lower than that. Once we identify one core pattern, we will be able to find all the other core patterns by a bounding ball of radius r(τ).</p>
            <h4 class="unnumbered" data-number="" id="example-core-patterns">Example: Core Patterns</h4>
            <p><img src="../media/image656.png" /></p>
            <p><img src="../media/image657.png" /></p>
            <p>The last thing is quite natural because to achieve these patterns we have to achieve an high number of core patterns. A colossal pattern has far more core patterns than a small-sized pattern. A colossal pattern has far more core descendants of a smaller size c. A random draw from a complete set of pattern of size c would more likely to pick a core descendant of a colossal pattern. A colossal pattern can be generated by merging a set of core patterns.</p>
            <h4 class="unnumbered" data-number="" id="colossal-patterns-correspond-to-dense-balls">Colossal Patterns Correspond to Dense Balls</h4>
            <p>Due to their robustness, colossal patterns correspond to dense balls. A random draw in the pattern space will hit somewhere in the ball with high probability.</p>
            <p><img src="../media/image658.png" /></p>
            <p>In the previous example, the probability of drawing a descendant of abcef is 0.9.</p>
            <h3 data-number="11.3.4" id="idea-of-pattern-fusion-algorithm"><span class="header-section-number">11.3.4</span> Idea of Pattern-Fusion Algorithm</h3>
            <p>We have an high number of possible sub-items of the larger itemset. The idea of Pattern-Fusion Algorithm is the following.</p>
            <ul>
            <li><p>Generate a complete set of frequent patterns up to a small size, and from this apply an algorithm like apriori.</p></li>
            <li><p>Randomly pick a pattern β, and β has a high probability to be a core-descendant of some colossal pattern α</p></li>
            <li><p>Identify all α’s descendants in this complete set, and merge all of them ― This would generate a much larger core-descendant of α</p></li>
            <li><p>In the same fashion, we select K patterns. This set of larger core-descendants will be the candidate pool for the next iteration</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="pattern-fusion-the-algorithm">Pattern-Fusion: The Algorithm</h4>
            <p><strong>Initialization (Initial pool):</strong> Use an existing algorithm to mine all frequent patterns up to a small size, e.g., 3 (for example with the apriori algorithm).</p>
            <p>Then, perform a certain number of <strong>iterations (Iterative Pattern Fusion)</strong>: - At each iteration, K seed patterns are randomly picked from the current pattern pool</p>
            <ul>
            <li><p>For each seed pattern thus picked, we find all the patterns within a bounding ball centered at the seed pattern</p></li>
            <li><p>All these patterns found are fused together to generate a set of super-patterns. All the super-patterns thus generated form a new pool for the next iteration</p></li>
            </ul>
            <p><strong>Termination:</strong> when the current pool contains no more than K patterns at the beginning of an iteration</p>
            <p><img src="../media/image659.png" /></p>
            <p>We start with the set of patterns empty.</p>
            <p><img src="../media/image660.png" /></p>
            <p>We aggregate in (9) what we found in the core list.</p>
            <p>When the number of such beta exceeds a threshold, which is determined by the system, we resort to a sampling technique to decide the set of beta to retain, reducing computational effort.</p>
            <h4 class="unnumbered" data-number="" id="why-is-pattern-fusion-efficient">Why is Pattern-Fusion Efficient?</h4>
            <p>This is a bounded-breadth pattern tree traversal.</p>
            <p>It avoids explosion in mining mid-sized ones, because randomness comes to help to stay on the right path.</p>
            <p>We provide the ability to identify “short-cuts” and take “leaps”.</p>
            <p>We fuse small patterns together in one step to generate new patterns of significant sizes. We reach efficient solution but no one guarantee we will find all possible colossal patterns.</p>
            <h4 class="unnumbered" data-number="" id="pattern-fusion-leads-to-good-approximation">Pattern-Fusion Leads to Good Approximation</h4>
            <p>The larger the pattern, the greater the chance it will be generated, because very large pattern have a very large number of subitemsets, so we have an high probability to pickup them. The more distinct the pattern, the greater the chance it will be generated.</p>
            <p>We have to identify how much we are close to the real solution and if we can save time. With the increase of the size of the metric, the Pattern Fusion approach maintain approximately the same computational time.</p>
            <p>The approximation error of Pattern-Fusion is rather close to uniform sampling (which randomly picks K patterns from the complete answer set).</p>
            <p>It can discover almost all real colossal patterns and the computational time is almost constant with the increasing of the minimum support threshold.Of course, it produces an approximation but it’s a really good one.</p>
            <h2 data-number="11.4" id="mining-compressed-or-approximate-patterns"><span class="header-section-number">11.4</span> <strong>Mining Compressed or Approximate Patterns</strong></h2>
            <p>When we return patterns we can reduce closed patterns but sometimes they are very high. We can compress patterns.</p>
            <p>We introduce the pattern distance measure:</p>
            <p><img src="../media/image661.png" /></p>
            <p>δ-clustering is used to do that. We have that for each pattern P, find all patterns S which can be expressed by(that is, O(S) ⊂ O(P)) and their distance to P are within δ (δ-cover).</p>
            <p>All patterns in the cluster can be represented by P.</p>
            <p><img src="../media/image662.png" /></p>
            <p>Returning closed frequent pattern, we have to report P1, P2, P3, P4, P5 because support is different. This definition has no kind of compression.</p>
            <p>Max-pattern, returns P3 but we have info loss. We lose information about the support of subpatterns.</p>
            <p>A desirable output is P2, P3, P4 because if we consider P2 and P1, P2 is a super-itemset and the support is comparable. The distance we saw is very low.</p>
            <p>We decided to compress the output by giving in output just itemsets that contains other itemsets and approximately have similar supports.</p>
            <h3 data-number="11.4.1" id="redundancy-award-top-k-patterns"><span class="header-section-number">11.4.1</span> Redundancy-Award Top-k Patterns</h3>
            <p>Mining the top-k most frequent patterns is a strategy for reducing the number of patterns returned during mining.</p>
            <p>However, in many cases, frequent patterns are not mutually independent but often clustered in small regions.</p>
            <p>This is somewhat like finding 20 population centers in the world, which may result in cities clustered in a small number of countries rather than evenly distributed across the globe.</p>
            <p>Why this observation? Why redundancy-aware top-k patterns?</p>
            <p>Desired patterns must be characterized by high significance &amp; low redundancy. We propose to exploit the MMS (Maximal Marginal Significance) for measuring the combined significance of a pattern set.</p>
            <p><img src="../media/image663.png" /></p>
            <p>The significance of (a) is represented by white-black colors. Not considering redundancy and just significancy we just pick-up itemsets with highest significancy. A significance measure S is a function mapping a pattern p ∈ P to a real value such that S(p) is the degree of interestingness (or usefulness) of the pattern p.</p>
            <ul>
            <li><p><strong>Objective measures</strong> depend only on the structure of the given pattern and the underlying data used in the discovery process.</p></li>
            <li><p><strong>Subjective measures</strong> are based on user beliefs in the data. They therefore depend on the users who examine the patterns.</p></li>
            </ul>
            <p>Redundancy R between two patterns p and q is defined as: <em>R(p,q) = S(p)+S(q)-S(p,q)</em></p>
            <p>The ideal redundancy measure R(p,q) is usually hard to obtain. However, we can approximate redundancy using distance between patterns. The <strong>problem of finding redundancy-aware top-k patterns can thus be transformed into finding a k-pattern set that maximizes the marginal significance</strong>, which is a well studied problem in information retrieval. A document has high marginal relevance if it is both relevant to the query and contains minimal marginal similarity to previously selected documents, where the marginal similarity is computed by choosing the most relevant selected document. We want to select significant itemsets but not redundant with other itemsets we are giving in output.</p>
            <h1 data-number="12" id="sequential-pattern-mining"><span class="header-section-number">12</span> Sequential Pattern Mining</h1>
            <p>Frequent pattern mining use itemsets and we don’t have any order in items. We would like to investigate now sequences.</p>
            <p><strong>Sequential pattern mining</strong> discovers frequent subsequences as patterns in a sequence database.</p>
            <p>A sequence database stores a number of records, where <strong>all records are sequences of ordered events</strong>, with or without concrete notions of time.</p>
            <p>An example sequence database is retail customer transactions or purchase sequences in a grocery store showing, for each customer, the collection of store items they purchased every week for one month but not in the single time, where the order of items bought at once is not useful.</p>
            <p>Records are stored as follows:</p>
            <p>[Transaction/Customer ID, &lt;Ordered Sequence Events&gt;]</p>
            <p>Examples:</p>
            <p>[T1, &lt;(bread, milk), (bread, milk, sugar), (milk), (tea, sugar)&gt;];</p>
            <p>[T2, &lt;(bread), (sugar, tea)&gt;] Each element in a sequence is an itemset.</p>
            <p><strong>Web Usage Mining</strong> is an application of sequential pattern mining.</p>
            <p>Finding user navigational patterns on the world wide web by extracting knowledge from web logs.</p>
            <p>Ordered sequences of events are composed of single items and not sets of items, with the assumption that a web user can physically access only one web page at any given point in time.</p>
            <p>Our sequence is a sequence of visits in the single page.</p>
            <p>Given a set of events E = {a, b, c, d, e, f}, a web access sequence database for four users may have four records:</p>
            <p>[T1, &lt;abdac&gt;];</p>
            <p>[T2, &lt;eaebcac&gt;];</p>
            <p>[T3, &lt;babfaec&gt;];</p>
            <p>[T4, &lt;abfac&gt;].</p>
            <p>A web log pattern mining can find a frequent sequence, abac, indicating that over 90% of users who visit product a’s web page also immediately visit product b’s web page.</p>
            <h2 data-number="12.1" id="web-usage-mining"><span class="header-section-number">12.1</span> Web Usage Mining</h2>
            <p>How can we collect this web log? On the <strong>server-side</strong>, <strong>client-side</strong> or on a <strong>proxy server</strong>?</p>
            <ul>
            <li><p><strong>Server-side</strong>: reflects the access of a web site by multiple users, is good for mining multiple users’ behavior and web recommender systems. But as cons, server logs may not be entirely reliable due to caching, as cached page views are not recorded in a server log. In fact, we have the local cache and the cache on proxy before reaching the server.</p></li>
            <li><p><strong>Client-side</strong>: requires that a remote agent be implemented or a modified browser be used to collect single-user data in the client, thus eliminating caching and session identification problems, and is useful for web content personalization applications. This cannot be easy and some users can avoid to have this agent.</p></li>
            <li><p><strong>Proxy server</strong>: reveal the actual HTTP requests from multiple clients to multiple web servers, thus characterizing the browsing behavior of a group of anonymous users sharing a common server.</p></li>
            </ul>
            <p>Data Format of the Log:</p>
            <p>137.207.76.120 - [30/Aug/2009:12:03:24 -0500] “GET /jdk1.3/docs/relnotes/deprecatedlist.html HTTP/1.0” 200 2781.</p>
            <p>Other techniques such as <strong>cookie and sniffer</strong> may be needed to have a complete analysis.</p>
            <p>In most cases, researchers assume that user web visit information is completely recorded in the web server log, which is preprocessed to obtain the transaction database to be mined for sequences.</p>
            <p>Let’s see it now from a formal point of view.</p>
            <h2 data-number="12.2" id="sequential-pattern-mining-1"><span class="header-section-number">12.2</span> Sequential Pattern Mining</h2>
            <p>An <strong>itemset</strong> is a set drawn from items in I, and denoted (<span class="math inline">\(i_1\)</span>, <span class="math inline">\(i_2\)</span>, … , <span class="math inline">\(i_k\)</span>), where <span class="math inline">\(i_j\)</span> is an item or event.</p>
            <h3 data-number="12.2.1" id="problem-definition"><span class="header-section-number">12.2.1</span> Problem definition:</h3>
            <p><strong>Given</strong></p>
            <ul>
            <li><p>A set of sequential records (called <strong>sequences</strong>) representing a sequential database D</p></li>
            <li><p>A minimum support threshold called <strong>min_sup</strong></p></li>
            <li><p>A set of k unique items or events <strong>I={<span class="math inline">\(i_1\)</span>,<span class="math inline">\(i_2\)</span>, … , <span class="math inline">\(i_k\)</span>}</strong></p></li>
            </ul>
            <p>We aim to <strong>find</strong> the set of all frequent sequences S in the given sequence database D of items I at the given min_sup.</p>
            <h3 data-number="12.2.2" id="lexicographic-order"><span class="header-section-number">12.2.2</span> <strong>Lexicographic Order</strong></h3>
            <p>A <strong>sequence S</strong> is denoted as a sequence of elements &lt;<span class="math inline">\(e_1\)</span>,<span class="math inline">\(e_2\)</span>,<span class="math inline">\(e_3\)</span>, … , <span class="math inline">\(e_q\)</span>&gt;, where the sequence element <span class="math inline">\(e_j\)</span> is an itemset (e.g., (be) in &lt;a(be)c(ad)&gt;, but also (aec) can be, just because it’s a part of unique transaction) that <strong>might contain only one item</strong> (which is also referred to as 1-itemset).</p>
            <p>A sequence element is a lexicographically ordered list of items.</p>
            <p>Assume an itemset t of distinct items <span class="math inline">\(t = \{i_1, i_2, ... , i_k\}\)</span>, and another itemset of distinct items also <span class="math inline">\(t&#39; = \{ j_1, j_2, ... , j_l\}\)</span>, where <span class="math inline">\(i_1 \le i_2 \le ··· \le i_k\)</span> and <span class="math inline">\(j_1 \le j_2 \le ··· \le j_l\)</span>, such that <span class="math inline">\(\le\)</span> indicates “occurs before” relationship.</p>
            <p>Then, for itemsets, <span class="math inline">\(t &lt; t&#39;\)</span> (t is lexicographically less than <span class="math inline">\(t\)</span>) if and only if either of the following is true:</p>
            <ol type="1">
            <li><p> for some integer <span class="math inline">\(h\)</span>, <span class="math inline">\(0 \le h \le \min\{k, l\}\)</span>, we have <span class="math inline">\(i_r = j_r\)</span> for <span class="math inline">\(r &lt; h\)</span>, and <span class="math inline">\(i_h &lt; j_h\)</span>, or</p></li>
            <li><p><span class="math inline">\(k &lt; l\)</span>, and <span class="math inline">\(i_1 = j_1\)</span>, <span class="math inline">\(i_2 = j_2, ... , i_k = j_k\)</span>.</p></li>
            </ol>
            <p>Example (1): (abc) &lt; (abec), because c preceds e, and (af) &lt; (bf), because a preceds b</p>
            <p>Example (2): (ab) &lt; (abc), because the itemset is longer</p>
            <h4 class="unnumbered" data-number="" id="sequences"><strong>Sequences</strong></h4>
            <p>A sequence with k elements is called a k-sequence. An item can occur only once in an itemset, but it can occur several times in different itemsets of a sequence.</p>
            <p>A sequence <span class="math inline">\(\alpha = &lt; e_{i_1},e_{i_2},e_{i_3}, ... , e_{i_m} &gt;\)</span> is a <strong>subsequence of another sequence</strong> <span class="math inline">\(\beta = &lt; e_{i_1},e_{i_2},e_{i_3}, ... , e_{i_n} &gt;\)</span>, denoted <span class="math inline">\(\alpha \preceq \beta\)</span>, if there exist integers <span class="math inline">\(i_1 &lt; i_2 &lt; ... &lt; i_m\)</span> and all events <span class="math inline">\(e_{i_j} \in \alpha\)</span> and <span class="math inline">\(e_i \in \beta\)</span> and <span class="math inline">\(i_1 \ge 1\)</span> and <span class="math inline">\(i_m \le n\)</span>, such that <span class="math inline">\(e_{i_j} \subseteq e_i\)</span>.</p>
            <p>This definition of inclusion is the one we saw for itemsets.</p>
            <p>A sequential pattern is maximal if it is not a subsequence of any other sequential pattern.</p>
            <p>Example of sequence:</p>
            <p><img src="../media/image664.png" /></p>
            <p>In the sequence database we have 5 sequences, we can interpret them in a way that items in pharentesis are items in itemsets.</p>
            <p>If we search for (ae) in the sequence, we need to find an element with a and e inside, together.</p>
            <p>The support threshold is the number of sequence in which the subsequence is present.</p>
            <p>The problem is complex, we need to use specific algorithms to reduce the computational effort.</p>
            <h4 class="unnumbered" data-number="" id="sequence-lexicographical-ordering"><strong>Sequence lexicographical ordering</strong></h4>
            <p>Assume a lexicographical order <span class="math inline">\(\le\)</span> of items I in the sequential access database, denoted <span class="math inline">\(\le I\)</span>.</p>
            <p>If an item <span class="math inline">\(i\)</span> occurs before an item <span class="math inline">\(j\)</span>, it is denoted <span class="math inline">\(i \le I j\)</span>; this order is also extended to sequences and subsequences by defining <span class="math inline">\(S_a \le S_b\)</span> if <span class="math inline">\(S_a\)</span> is a subsequence of <span class="math inline">\(S_b\)</span>.</p>
            <p>Consider all sequences arranged in a sequence tree T (referred to as a Lexicographical Tree).</p>
            <ul>
            <li><p><strong>The root of the tree is labeled as an empty set {}</strong>.</p></li>
            <li><p>Recursively, if <span class="math inline">\(n\)</span> is a node in the tree T, then <span class="math inline">\(n\)</span>’s children are all nodes <span class="math inline">\(n&#39;\)</span> such that <span class="math inline">\(n \le n&#39;\)</span> and <span class="math inline">\(\forall m \in T: n&#39; \le m\)</span> if n ≤ m. Each sequence in the tree can be extended by adding a 1-sequence to its (sequence-extended sequence) end or adding an itemset to its end (itemset-extended sequence), which is not applicable to the case of web log mining.</p></li>
            </ul>
            <p><img src="../media/image665.png" /></p>
            <p>We start with the root, and starting from the lexicographic order of items we can generate all possible lexicographic ordered sequences.</p>
            <p>We should generate all possible sequences, and this is hard to manage.</p>
            <p>We have just to impose some threshold for frequency or support of a sequence.</p>
            <h4 class="unnumbered" data-number="" id="support-of-a-sequence"><strong>Support of a Sequence</strong></h4>
            <p>The <strong>frequency</strong> or <strong>support</strong> of a sequence (or subsequence) S, denoted <span class="math inline">\(\sigma(S)\)</span> is the total number of sequences of which S is a subsequence divided by the total number of sequences in the database D, whereas the absolute support (or support count) of a sequence (or subsequence) S is the total number of sequences in D of which S is a subsequence.</p>
            <p>A sequence is called <strong>frequent</strong> if its frequency is not less than a <strong>user-specified threshold</strong>, called <strong>minimum support</strong> denoted min sup or the greek letter <span class="math inline">\(\xi\)</span>.</p>
            <p>A frequent sequence <span class="math inline">\(S_\alpha\)</span> is called a <strong>frequent closed sequence</strong> if there exists no proper supersequence of <span class="math inline">\(S_\alpha\)</span> with the same support, that is, <span class="math inline">\(S_\beta\)</span> such that <span class="math inline">\(S_\alpha \preceq S_\beta\)</span> and <span class="math inline">\(\sigma(S_\alpha) = \sigma(S_\beta)\)</span> (same support); otherwise it is said that sequence <span class="math inline">\(S_\alpha\)</span> is absorbed by <span class="math inline">\(S_\beta\)</span>.</p>
            <p>In math language this is the definition: <span class="math inline">\(S_\alpha\)</span> is a frequent closed sequence if <span class="math display">\[
                \nexists S_\beta: S_\alpha \preceq S_\beta \land \sigma(S_\alpha) = \sigma(S_\beta)
            \]</span></p>
            <p>Assume the frequent sequence <span class="math inline">\(S_\beta = &lt; beadc &gt;\)</span> is the only superset of the frequent sequence <span class="math inline">\(S_\alpha = &lt; bea &gt;\)</span>, if <span class="math inline">\(\sigma(S_\alpha) = \sigma(S_\beta)\)</span>, then <span class="math inline">\(S_\alpha\)</span> is not a frequent closed sequence; on the other hand, if <span class="math inline">\(\sigma(S_\alpha) &gt; \sigma(S_\beta)\)</span>, then <span class="math inline">\(S_\alpha\)</span> is a frequent closed sequence. Notice that <span class="math inline">\(\sigma(S_\beta)\)</span> cannot be greater than <span class="math inline">\(\sigma(S_\alpha)\)</span>, because <span class="math inline">\(S_\alpha \preceq S_\beta\)</span> (is a subsequence of).</p>
            <p>Let’s see an example.</p>
            <p><img src="../media/image666.png" /></p>
            <p>Let’s assume that we have this supermarket, and we want to investigate if we have frequent subsequences.</p>
            <p>Note: Use Minsup of 25%</p>
            <p>{{30}, {90}} is a frequent sequence because present in 1 and 4. It’s also maximal because we don’t have supersequences of these sequences. We need to verify if exists supersequences also with lower support.</p>
            <p>{(10 20) (30)} does not have minsup (because is only supported by Customer 2)</p>
            <p>{(30)}, {(70)}, {(30) (40)} … are frequent sequences but not maximal.</p>
            <p>For {{30}} we have both {{30}{90}} and {{30}{40 70}} as supersequences.</p>
            <h2 data-number="12.3" id="the-algorithms"><span class="header-section-number">12.3</span> <strong>The Algorithms</strong></h2>
            <p>The problem of frequent sequences mining is approached using algorithms that typically exploits these phases:</p>
            <ul>
            <li><p><strong>Sort Phase</strong></p></li>
            <li><p><strong>Litemset Phase</strong></p></li>
            <li><p><strong>Transformation Phase</strong></p></li>
            <li><p><strong>Sequence Phase</strong></p></li>
            <li><p><strong>Maximal Phase</strong></p></li>
            </ul>
            <h3 data-number="12.3.1" id="sort-phase"><span class="header-section-number">12.3.1</span> <strong>Sort Phase</strong></h3>
            <p>We <strong>sort the database</strong> using the customer ID as the major key. We do it in respect of the time, so using transaction-Time as the minor key.</p>
            <p><strong>Converts the original transaction database into a database of customer sequences</strong></p>
            <p><img src="../media/image667.png" /></p>
            <p>We need this form to produce the sequence corresponding to the customer.</p>
            <h3 data-number="12.3.2" id="litemset-large-itemset-phase"><span class="header-section-number">12.3.2</span> <strong>Litemset (Large Itemset) Phase</strong></h3>
            <p>We count how many itemsets are large, supported by fraction of customers larger than minsup.</p>
            <p>If we don’t have frequent itemsets we can’t have frequent sequences, so we start from frequent itemsets.</p>
            <p>We determine the large itemsets where large means <strong>support <span class="math inline">\(\ge\)</span> minsup</strong>.</p>
            <p><strong>Recall</strong>: each itemset in a large sequence has to be a large itemset</p>
            <p><strong>Support counting</strong> measures the fraction of customers purchasing items in the itemset.</p>
            <p><img src="../media/image668.png" /></p>
            <p>Each large itemset is then mapped to a set of <strong>contiguous integers</strong></p>
            <p>Transformation used to compare large itemsets in constant time and reduce the time required to check if a sequence is contained in a customer sequence.</p>
            <p>This is a mapping to speed-up the computation.</p>
            <p><img src="../media/image669.png" /></p>
            <p>We can realize that some itemsets are not frequent in our example database.</p>
            <p>{30} is frequent because appears in different sequences, same for {90}. {10} is not because appears only in that sequence.</p>
            <p>In any case it depends on the threshold we choose.</p>
            <p>Only considering the large itemsets we reduce the complexity of our problem.</p>
            <p>We replace itemsets and we can have one-itemsets but also sets with more items.</p>
            <p>We replace them with an integer number, to reduce the time to check if a sequence is contained in a customer sequence.</p>
            <p>We have to <strong>eliminate non-large itemsets</strong> and <strong>replace them with integer numbers</strong>.</p>
            <h3 data-number="12.3.3" id="transformation-phase"><span class="header-section-number">12.3.3</span> <strong>Transformation Phase</strong></h3>
            <p>Need to repeatedly determine <strong>which of a given set of large sequences are contained in a customer sequence</strong>. To make this fast:</p>
            <ul>
            <li><p>Replace each transaction with all litemsets contained in the transaction.</p></li>
            <li><p>Transactions with no litemsets are dropped. (still considered for support counts).</p></li>
            </ul>
            <p><img src="../media/image670.png" /></p>
            <p>The original customer sequence is transformed in the costumer sequence in the second column (eliminating not frequent items and transforming), and after mapping on the sequence on the third column.</p>
            <p>Note: (10 20) dropped because of lack of support.</p>
            <p>While (40 60 70) is replaced with set of litemsets {(40),(70),(40 70)} (60 does not have min-sup). We can just pickup one of these items and each can be part of the subsequence.</p>
            <p>We extracted the database of sequences and we preprocessed it to eliminate not frequent itemsets and cannot generate frequent sequences. We performed those mapping to speed-up the computation.</p>
            <h3 data-number="12.3.4" id="sequence-phase"><span class="header-section-number">12.3.4</span> <strong>Sequence Phase</strong></h3>
            <p>Now, we use the set of large itemsets to <strong>find the desired sequences</strong>.The algorithms we exploit has a similar structure to <strong>Apriori algorithms</strong> used to find large itemsets.</p>
            <ul>
            <li><p>Use seed set to generate candidate sequences.</p></li>
            <li><p>Count support for each candidate.</p></li>
            <li><p>Eliminate candidate sequences which are not large.</p></li>
            </ul>
            <h4 class="unnumbered" data-number="" id="two-families-of-algorithms"><strong>Two families</strong> of algorithms:</h4>
            <ul>
            <li><p><strong>Count-all</strong>: count all large sequences including non-maximal sequences (it is careful with respect to the minimum support). This algorithm is <em>AprioriAll</em>.</p></li>
            <li><p><strong>Count-some</strong>: try to avoid counting non-maximal sequences by counting longer sequences first (it is careful with respect to maximality). These algorithms are <em>AprioriSome</em> and <em>DynamicSome</em>.</p></li>
            </ul>
            <h3 data-number="12.3.5" id="maximal-phase"><span class="header-section-number">12.3.5</span> <strong>Maximal Phase</strong></h3>
            <p>Independently of the algorithm we use, to <strong>find maximal sequences</strong> among large sequences.</p>
            <p>Given:</p>
            <ul>
            <li><p><strong>k-sequence</strong>: sequence of length</p></li>
            <li><p><strong>S</strong> set of all large sequences</p></li>
            </ul>
            <blockquote>
            <p>for (<span class="math inline">\(k=n\)</span>; <span class="math inline">\(k&gt;1\)</span>; <span class="math inline">\(k--\)</span>) do</p>
            <blockquote>
            <p>foreach k-sequence <span class="math inline">\(s_k\)</span> do</p>
            <blockquote>
            <p>delete from S all subsequences of <span class="math inline">\(s_k\)</span></p>
            </blockquote>
            </blockquote>
            </blockquote>
            <p><strong>We remove all subsequences of a maximal sequence</strong>.</p>
            <p>This phase is performed to find maximal sequences.</p>
            <p>Data-structures and an algorithm exist to do this efficiently. (<em>hash trees</em>)</p>
            <h2 data-number="12.4" id="aprioriall-count-all"><span class="header-section-number">12.4</span> <strong>AprioriAll</strong> (Count-All)</h2>
            <p>This is based on the same idea of the one saw in frequent pattern analysis.</p>
            <p>We start from the large itemsets determined in previous phases.</p>
            <p><img src="../media/image671.png" /></p>
            <p>Similiar to apriori algorithm but here we talk about sequences.</p>
            <p>The generation of candidates <span class="math inline">\(C_k\)</span> is performed as follows:</p>
            <ul>
            <li><p><strong>Step 1</strong>: Join two sequences in <span class="math inline">\(L_{k-1}\)</span> to generate <span class="math inline">\(C_k\)</span></p>
            <p>For each two sequences in <span class="math inline">\(L_{k-1}\)</span> that have the same from the 1<sup>st</sup> to k-2th itemsets, select the 1 to k-1 itemset from the first sequence, and join with the last itemset from another sequence.</p>
            <p>Example</p>
            <p><span class="math inline">\(L_3\)</span> = {123}{234}{124}{134}{135}</p>
            <p><span class="math inline">\(C_4\)</span> = {1 2 3 4} {1 3 4 5}{1 3 5 4} {1 2 4 3} (we also have to add the viceversa because the order here is important)</p></li>
            <li><p><strong>Step 2</strong>: Delete all sequences in <span class="math inline">\(C_k\)</span> if some of their sub-sequences are not in $L_{k-1}</p>
            <p>Example: <span class="math inline">\(C_4\)</span> = {1 2 3 4} <span class="math inline">\(\rightarrow\)</span> 1 2 3 is present, 2 3 4 is present, 1 3 4 is present so we don’t eliminate this sequence.</p>
            <p>{1 3 4 5} <span class="math inline">\(\rightarrow\)</span> 1 3 5 is present but 1 4 5 is not present so we are sure that this sequence is not frequent for the apriori property. <strong>We can remove this sequence</strong>.</p>
            <p>{1 3 5 4} <span class="math inline">\(\rightarrow\)</span> can be remove because 3 5 4 is not present</p>
            <p><img src="../media/image672.png" /></p></li>
            </ul>
            <p>We have all possible sequences minable from the customer sequences using the three phases seen before.</p>
            <p>We start from transactions, build the customer sequences using the customer index and then verify how many large itemsets we have.</p>
            <p>If we set minsup = 2, they are all frequent.</p>
            <p>Then we test <span class="math inline">\(L_2\)</span>. The candidates are &lt;1 2&gt; but also &lt;2 1&gt;, and so on.</p>
            <p>Then we apply the join between all posssible pairs to generate all possible <span class="math inline">\(L_3\)</span>.</p>
            <p>Then we use them to generate <span class="math inline">\(L_4\)</span>, and we check if they’re frequent.</p>
            <p>We obtain that the ony <span class="math inline">\(L_4\)</span> frequent sequence is &lt;1 2 3 4&gt;.</p>
            <p>But we are interested in all maximum sequences, we start from &lt;1 2 3 4&gt; and we delete all <span class="math inline">\(L_3\)</span> sequences contained in &lt;1 2 3 4&gt;.</p>
            <p>We don’t delete &lt;1 3 5&gt; because not contained in it.</p>
            <p>Now we verify the <span class="math inline">\(L_2\)</span> and so on.</p>
            <p>Applying the 3rd phase of the 5 phases, this is our answer.</p>
            <p>Answer: <strong>&lt;1 2 3 4&gt;, &lt;1 3 5&gt;, &lt;4 5&gt;</strong></p>
            <p>The problem of this algorithm is similar to the one of the Apriori algorithm, the generation of a very high number of candidates, and then we remove them using the Apriori property or counting the support.</p>
            <h2 data-number="12.5" id="apriorisome"><span class="header-section-number">12.5</span> <strong>AprioriSome</strong></h2>
            <p>We have an high number of non maximum sequences generated, that prevents us to count the support scanning the dataset.</p>
            <p>We have <strong>AprioriSome</strong> that tries to avoid counting non-maximal sequences by counting longer sequences first.</p>
            <p><strong>2 phases</strong>:</p>
            <ul>
            <li><p><strong>Forward Phase</strong> <span class="math inline">\(\rightarrow\)</span> find all large sequences of certain lengths.</p></li>
            <li><p><strong>Backward Phase</strong> <span class="math inline">\(\rightarrow\)</span> find all remaining large sequences. For example, we might count sequences of length 1, 2, 4 and 6 in the forward phase and count sequences of length 3 and 5 in the backward phase, we jump sequences 3 and 45</p></li>
            </ul>
            <p>We jump from a <strong>k</strong>-sequence to a <strong>k+n</strong>-sequence.</p>
            <p>During the backward phase in fact we just find all remaining large sequences.</p>
            <p>We determine which lengths to count using <strong>next()</strong> function. next() takes in as a parameter the length of the sequence counted in the last pass.</p>
            <p><em>next(k) = k + 1</em> (Same as AprioriAll)</p>
            <p>We can improve increasing the value to add, not 1 but an higher one.</p>
            <p><strong>Balances tradeoff</strong> between:</p>
            <ul>
            <li><p>Counting non-maximal sequences</p></li>
            <li><p>Counting extensions of small candidate sequences</p></li>
            </ul>
            <p><img src="../media/image674.png" /></p>
            <p><span class="math display">\[
                hit_k = \dfrac{|Lk|}{|Ck|}
            \]</span></p>
            <p>Intuition: As <span class="math inline">\(hit_k\)</span> increases, the time wasted by counting extensions of small candidates decreases.</p>
            <p>If <span class="math inline">\(hit_k\)</span> is high, most of the candidates we generated are frequent, so we jump in the increase of the number of itemsets in the sequence.</p>
            <p>We have an high probability that this candidate sequences are frequents, we jump and just apply the backward.</p>
            <h3 data-number="12.5.1" id="forward-phase"><span class="header-section-number">12.5.1</span> <strong>Forward Phase</strong></h3>
            <p><img src="../media/image675.png" /></p>
            <p><span class="math inline">\(L_{K-1}\)</span> known means that we generated and tested the frequent sequences at the level k-1. We use the apriori algorithm, we use directly the frequent sequences.</p>
            <p>In the candidate generation, if the large sequence set <span class="math inline">\(L_{k-1}\)</span> is not available, we use the candidate set <span class="math inline">\(C_{k-1}\)</span> to generate <span class="math inline">\(C_k\)</span>.</p>
            <p>If k is equal to the output of the function we test.</p>
            <p>In the forward phase we generate all k candidates, but some using k-1 frequent sequences, others by k-1 candidate sequences.</p>
            <p>Correctness is maintained because <span class="math inline">\(L_{k-1} \subseteq C_{k-1}\)</span> .</p>
            <h3 data-number="12.5.2" id="backward-phase"><span class="header-section-number">12.5.2</span> <strong>Backward Phase</strong></h3>
            <p>We want to avoid to verify in the sequence database some candidate sequences and since we are interested in the maximum sequences, we generate them, we verify if they are frequent and delete all subsequences of these frequent sequences.</p>
            <p>For all lengths which we skipped:</p>
            <ul>
            <li><p><strong>Delete sequences</strong> in candidate set which are <strong>contained in some large sequence</strong>.</p></li>
            <li><p><strong>Count remaining candidates</strong> and find all sequences with min. support.</p></li>
            <li><p>Also <strong>delete large sequences</strong> found in forward phase <strong>which are non-maximal</strong>.</p></li>
            </ul>
            <p><img src="../media/image676.png" /></p>
            <p>We perform the increment for the candidates remained after the deletion phase.</p>
            <p>We are interested in maximum sequences, so we delete all subsequences.</p>
            <p><img src="../media/image677.png" /></p>
            <p>In the forward phase we generated all candidate sequences, sometimes using frequent sequences and sometimes using candidate ones.</p>
            <p>In any case we verify if the candidate sequences are frequent.</p>
            <p>We start from <span class="math inline">\(L_1\)</span>, generate <span class="math inline">\(L_2\)</span> and test if they are frequent.</p>
            <p>We generate <span class="math inline">\(C_3\)</span> using <span class="math inline">\(L_2\)</span> and then we generate candidate 4 sequences using <span class="math inline">\(C_3\)</span> and checking if they’re frequent.</p>
            <p>We start the backward phase.</p>
            <p><img src="../media/image678.png" /></p>
            <p>We start from the maximum-sequence &lt;1 2 3 4&gt; and delete all subsequences contained in it. We are sure they are not maximal.</p>
            <p>Other sequences are tested checking if they are frequent.</p>
            <p>We remove &lt;1 2 5&gt; and &lt;3 4 5&gt; because they are not frequent scanning the dataset, and we remove all 2-subsequences.</p>
            <p>We preserve &lt;4 5&gt; because maximal and frequent.</p>
            <p>We eliminate sequences to test in the database, but it doesn’t help a lot because it doesn’t affect very much the computational effort, even if we have to increase the counter scanning the dataset.</p>
            <p>But the cons is that we generate more candidate sequences in any case because sometimes we exploit <span class="math inline">\(C_{k-1}\)</span> instead of <span class="math inline">\(L_{k-1}\)</span>.</p>
            <h2 data-number="12.6" id="aprioridynamicsome"><span class="header-section-number">12.6</span> <strong>AprioriDynamicSome</strong></h2>
            <p> Like AprioriSome, skip counting candidate sequences of certain lengths in the forward phase.</p>
            <p>The candidate sequences that are counted is determined by the variable step.</p>
            <h3 data-number="12.6.1" id="initialization-phase"><span class="header-section-number">12.6.1</span> <strong>Initialization phase</strong></h3>
            <p>All the candidate sequences of length up to and including step are counted.</p>
            <h3 data-number="12.6.2" id="forward-phase-1"><span class="header-section-number">12.6.2</span> <strong>Forward phase</strong></h3>
            <p>All sequences whose lengths are multiples of step are counted.</p>
            <p>For example, if step = 3, we will count sequences of lengths 1, 2, and 3 in the initialization phase, and 6, 9, 12, … in the forward phase.</p>
            <p><strong>We can generate sequences of length 6 by joining sequences of length 3</strong>. We can generate sequences of length 9 by joining sequences of length 6 with sequences of length 3, etc. However, to generate the sequences of length 3, we need sequences of lengths 1 and 2, and hence the initialization phase.</p>
            <p>We just adopt a step = 3.</p>
            <h3 data-number="12.6.3" id="backward-phase-1"><span class="header-section-number">12.6.3</span> <strong>Backward phase</strong></h3>
            <p>Count sequences for the lengths we skipped over during the forward phase. However, unlike in AprioriSome, <strong>these candidate sequences were not generated in the forward phase</strong>.</p>
            <p>We use the <strong>intermediate phase</strong> to generate them.</p>
            <p>For example, assume that we count <span class="math inline">\(L_3\)</span> and <span class="math inline">\(L_6\)</span>, and <span class="math inline">\(L_9\)</span> turns out to be empty in the forward phase, so we have to stop there. We generate <span class="math inline">\(C_7\)</span> and <span class="math inline">\(C_8\)</span> (intermediate phase), and then count <span class="math inline">\(C_8\)</span> followed by <span class="math inline">\(C_7\)</span> after deleting non-maximal sequences (backward phase). This process is then repeated for <span class="math inline">\(C_4\)</span> and <span class="math inline">\(C_5\)</span>.</p>
            <p><img src="../media/image679.png" /></p>
            <p>We don’t generate all candidates but just jump.</p>
            <p>For each customer sequence we count the support.</p>
            <p><img src="../media/image680.png" /></p>
            <p>We generate sequence sand verify if they are frequent.</p>
            <p><img src="../media/image681.png" /></p>
            <p>We start from the last <span class="math inline">\(k\)</span> and generate <span class="math inline">\(C_k\)</span> after decreasing <span class="math inline">\(k\)</span> from <span class="math inline">\(L_{k-1}\)</span> if exists, otherwise from $C_{k-1} and then we apply the backward phase.</p>
            <p>The generation from <span class="math inline">\(L_k\)</span> and Lstep for the <span class="math inline">\(C_{k+step}\)</span>, we have:</p>
            <p><img src="../media/image682.png" /></p>
            <p>If we have these sequences:</p>
            <p><img src="../media/image683.png" /></p>
            <p>We compute the end and the start for each sequence.</p>
            <p>We join <span class="math inline">\(X_k\)</span> with <span class="math inline">\(X_j\)</span> if <span class="math inline">\(X_k.end &lt; X_j.start\)</span>.</p>
            <p>The result of the join with the join condition <span class="math inline">\(X_2.end &lt; X_2.start\)</span> (where <span class="math inline">\(X_2\)</span> denotes the set of sequences of length 2) is the single sequence &lt;1 2 3 4&gt; .</p>
            <p>We generate the k+step candidate sequences in this way.</p>
            <h3 data-number="12.6.4" id="why-do-we-need-otf-generate"><span class="header-section-number">12.6.4</span> Why do we need otf-generate?</h3>
            <ul>
            <li><p>The <em>apriori-generate</em> procedure used for AprioriSome <strong>could generate more candidates</strong> (it however needs to be generalized to generate <span class="math inline">\(C_{k+j}\)</span> from <span class="math inline">\(L_k\)</span>. Essentially, the join condition has to be changed to require equality of the first <span class="math inline">\(k-j\)</span> terms, and the concatenation of the remaining terms).</p></li>
            <li><p>In addition, if the size of <span class="math inline">\(|L_k| + |L_{step}|\)</span> is less than the size of <span class="math inline">\(C_{k+step}\)</span> generated by AprioriSome, <strong>it may be faster to find all members of <span class="math inline">\(L_k\)</span> and <span class="math inline">\(L_{step}\)</span> contained in c</strong> than to find all members of <span class="math inline">\(C_{k+step}\)</span> contained in c.</p></li>
            <li><p>The intuition behind this generation procedure is that if <span class="math inline">\(s_k \in L_k\)</span> and <span class="math inline">\(s_j \in L_j\)</span> are both contained in c, and they don’t overlap in c, then <span class="math inline">\(s_k.s_j\)</span> is a candidate <span class="math inline">\((k + j)\)</span>-sequence.</p></li>
            </ul>
            <h3 data-number="12.6.5" id="execution-of-aprioridynamicsome"><span class="header-section-number">12.6.5</span> Execution of AprioriDynamicSome</h3>
            <p><img src="../media/image684.png" /></p>
            <p>In the <strong>forward phase</strong> we directly generate <span class="math inline">\(C_4\)</span> starting from <span class="math inline">\(L_2\)</span> with step = 2. We check if they’re frequent then. We start from <span class="math inline">\(L_4\)</span> to generate <span class="math inline">\(C_6\)</span> which is empty.</p>
            <p>In the <strong>intermediate phase</strong> we generate <span class="math inline">\(C_3\)</span> using <span class="math inline">\(L_2\)</span>. We don’t have <span class="math inline">\(C_5\)</span>.</p>
            <p>In the <strong>backward phase</strong> we verify if <span class="math inline">\(C_3\)</span> is frequent, but we need only to verify the candidate sequences in <span class="math inline">\(C_3\)</span> that we don’t delete considering <span class="math inline">\(L_4\)</span>. We generate a lower number of candidates and reduce the check of candidate sequences.</p>
            <h2 data-number="12.7" id="performance-comparison"><span class="header-section-number">12.7</span> <strong>Performance Comparison</strong></h2>
            <p>We use some datasets to evaluate performance.</p>
            <p>We consider performances in terms of computational time, not in terms of result because they give the same maximum-sequences.</p>
            <p><img src="../media/image685.png" /></p>
            <p>We plot what happens in terms of time using different minsup.</p>
            <p><em>DynamicSome</em> generates too many candidates, therefore we cannot manage a big production of candidates.</p>
            <p><em>AprioriSome</em> does a little better than <em>AprioriAll</em>, it avoids counting many non-maximal sequences, but the difference is not relevant.</p>
            <p><img src="../media/image686.png" /></p>
            <p>Advantage of <em>AprioriSome</em> is reduced for 2 reasons:</p>
            <ol type="1">
            <li><p>AprioriSome generates more candidates.</p></li>
            <li><p>Candidates remain memory resident even if skipped over.</p></li>
            </ol>
            <p>This is what happens with the relative time:</p>
            <p><img src="../media/image687.png" /></p>
            <p>We have stability using the number of transaction per customers.</p>
            <p>The solution proposed to reduce the number of candidates do not perform very well, we have some advantages but not that relevant.</p>
            <h3 data-number="12.7.1" id="bottlenecks-of-apriori-like-methods"><span class="header-section-number">12.7.1</span> <strong>Bottlenecks of Apriori-like methods</strong></h3>
            <ul>
            <li><p>A huge set of candidates could be generated</p></li>
            <li><p>Many scans of database in mining</p></li>
            <li><p>Encounter difficulty when mining long sequential patterns. We have an exponential number of short candidates. A length-100 sequential pattern needs <span class="math inline">\(\dfrac{|D_\alpha|}{|D_\beta|} \ge \tau\)</span> candidates.</p></li>
            </ul>
            <h2 data-number="12.8" id="freespan-fp-growth-for-sequential-pattern-mining"><span class="header-section-number">12.8</span> <strong>FreeSpan: FP-growth for sequential pattern mining</strong></h2>
            <p>The solution is to use a solution similar to <strong>FPGrowth</strong>.</p>
            <p>We had a limitation of scan to the database to 2 with FPGrowth and similar here but we have to adapt it to sequences.</p>
            <p><strong>Can we extend FP-growth to sequential pattern mining?</strong></p>
            <ul>
            <li><p>A straightforward construction of sequential-pattern tree does not work well.</p></li>
            <li><p>A level-by-level project does not achieve high performance either</p></li>
            <li><p>An interesting method is to explore <strong>alternative-level projection</strong></p></li>
            </ul>
            <h3 data-number="12.8.1" id="alternative-level-projection"><span class="header-section-number">12.8.1</span> Alternative-level Projection</h3>
            <p>The idea is to map the sequence database into projected sequence database.</p>
            <p>We find frequent items from database and produce the list of frequent items in support descending order, and is called <strong>f_list</strong>.</p>
            <p>All sequential patterns can be divided into several subsets <strong>without overlap</strong>.</p>
            <p><img src="../media/image689.png" /></p>
            <p>We produce the f_list and sort the list in descending order and project the sequence database.</p>
            <p>In particular, all sequential patterns will be divided into the 6 subsets indicated.</p>
            <p>The complete set of sequential patterns containing item <span class="math inline">\(i\)</span> but no items following <span class="math inline">\(i\)</span> in f_list can be found in the <span class="math inline">\(i\)</span>-projected database.</p>
            <p>We project the database considering items in f-list and all sequential pattern containing this specific item <span class="math inline">\(i\)</span> can be mined from the projection of the database along the item we consider.</p>
            <p>A sequence <span class="math inline">\(s\)</span> is projected as <span class="math inline">\(s_i\)</span> to the <span class="math inline">\(i\)</span>-projected database if there is at least an item <span class="math inline">\(i\)</span> in <span class="math inline">\(s\)</span></p>
            <p><span class="math inline">\(s_i\)</span> is a copy of <span class="math inline">\(s\)</span> by removing from <span class="math inline">\(s\)</span> all the infrequent items and any frequent item <span class="math inline">\(j\)</span> following <span class="math inline">\(i\)</span> in f_list</p>
            <p>Example: &lt;(ah)(bf)abf&gt; is projected to <span class="math inline">\(f\)</span>-projected database as &lt;a(bf)abf&gt; because h is not frequent so is removed and the projection contains only frequent items.</p>
            <p>And then to <span class="math inline">\(a\)</span>-projected database as &lt;abab&gt;, because in the f-list we remove items with the support lowest of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. And then to <span class="math inline">\(b\)</span>-projected database as &lt;bb&gt;, because in the f-list we remove items with the support lowest of <span class="math inline">\(b\)</span>.</p>
            <p>When we project, we can work with parallel or partition projection.</p>
            <h4 class="unnumbered" data-number="" id="parallel-projection"><strong>Parallel Projection</strong></h4>
            <p>In <strong>parallel projection</strong> we scan the database once, form all projected dbs at a time. May derive many and rather large projected dbs if sequence on average contains many frequent items.</p>
            <p>I can produce an high numer of projected ddatabases that can be crowdebd</p>
            <p>Let each transaction contain on average <span class="math inline">\(l\)</span> frequent items. A transaction is then projected to <span class="math inline">\((l−1)\)</span>-projected database. The total size of the projected data from this transaction is <span class="math inline">\(1+2+···+(l−1) = l\dfrac{(l−1)}{2}\)</span>. This implies that the total size of the single item-projected databases is about <span class="math inline">\(\dfrac{(l−1)}{2}\)</span> times of that of the original database. We have an <strong>high memory occupation</strong>.</p>
            <p>We must store them because we have to work with them.</p>
            <p>To avoid such an overhead, partition projection method is proposed.</p>
            <h4 class="unnumbered" data-number="" id="partition-projection"><strong>Partition Projection</strong></h4>
            <p>In <strong>partition projection</strong> we project a sequence to the projected database of the last frequent item in it.</p>
            <p>When scanning the database to be projected, a transaction T is projected to the <span class="math inline">\(a_i\)</span>-projected database only if <span class="math inline">\(a_i\)</span> is a frequent item in T and there is no any other item after <span class="math inline">\(a_i\)</span> in the list of frequent items appearing in the transaction.</p>
            <p>Since a transaction is projected to only one projected database at the database scan, after the scan, the database is partitioned by projection into a set of projected databases, and hence it is called partition projection.</p>
            <h3 data-number="12.8.2" id="example-of-database-projection"><span class="header-section-number">12.8.2</span> <strong>Example of Database Projection</strong></h3>
            <p><img src="../media/image690.png" /></p>
            <p>In parallel projection we scan the sequence database and produce the projection along <span class="math inline">\(f,e,d,a\)</span> just scanning the database. When we project along <span class="math inline">\(e\)</span> we consider all sequences where <span class="math inline">\(e\)</span> is present and not <span class="math inline">\(f\)</span>, in <span class="math inline">\(d\)</span> where <span class="math inline">\(d\)</span> is present and <span class="math inline">\(e\)</span> and <span class="math inline">\(f\)</span> are not.</p>
            <p>But we replicate sequences in several databases.</p>
            <p>In partition projection approaches, we scan the database and create this projection, storing the sequence only in the projected database in which we have the frequent item <span class="math inline">\(a_i\)</span> with any item after <span class="math inline">\(a_i\)</span>.</p>
            <p>When we discover <span class="math inline">\(f\)</span> is present, we insert the sequence into the <span class="math inline">\(f\)</span>-projection database but not in others.</p>
            <p>We add this sequence also in another database, if scanning the <span class="math inline">\(f\)</span>-projected database we find for example <span class="math inline">\(e\)</span> but we store if without <span class="math inline">\(f\)</span>.</p>
            <p>In this way we can save memory because we don’t save all the dataset but we build the projection dynamically.</p>
            <p>We have all same sequences of the one we have using the parallel projection.</p>
            <p>We save memory in this way.</p>
            <p>Each time when a projected database is being processed, to ensure the remaining projected databases obtain the complete information, each transaction in it is projected to the <span class="math inline">\(a_j\)</span>-projected database, where <span class="math inline">\(a_j\)</span> is the item in the transaction such that there is no any other item after <span class="math inline">\(a_j\)</span> in the list of frequent items appearing in the transaction.</p>
            <p>It’s like to “propagate” sequences on-the-fly.</p>
            <p>Then we can work with the projected database, and we don’t need to scan all the database.</p>
            <p>When projecting the <span class="math inline">\(f\)</span>-projected DB, we find local freq. items other than <span class="math inline">\(f\)</span> and have sequential pattern of multiple <span class="math inline">\(f\)</span>’s.</p>
            <p>We scan <span class="math inline">\(f\)</span>-projected db once and find sequential pattern containing {<span class="math inline">\(f, b\)</span>}, because these are the frequent items that we have in the projection of the database, including multiple <span class="math inline">\(f\)</span>’s.</p>
            <p>Then we count multiple <span class="math inline">\(b\)</span>’s, we perform one more scan finds sequential patterns containing two <span class="math inline">\(b\)</span>’s and one <span class="math inline">\(f\)</span>.</p>
            <p><img src="../media/image691.png" /></p>
            <p>The only two sequences containing <span class="math inline">\(f\)</span> will be in the <span class="math inline">\(f\)</span>-projected database.</p>
            <p>The only frequent items we have are <span class="math inline">\(b\)</span> and <span class="math inline">\(f\)</span> and possible sequence we can generate are the ones with <span class="math inline">\(b\)</span> and <span class="math inline">\(f\)</span>. We consider <span class="math inline">\(b\)</span>, <span class="math inline">\(f\)</span> and their duplication.</p>
            <p>We generate frequent sequences containing frequent items for the <span class="math inline">\(f\)</span>-projected database.</p>
            <p>The advantage is that we perform scans only on the projected databases.</p>
            <h3 data-number="12.8.3" id="mining-by-level-by-level-projected-databases"><span class="header-section-number">12.8.3</span> Mining by <strong>Level by Level</strong> Projected Databases</h3>
            <h4 class="unnumbered" data-number="" id="algorithm-5"><strong>Algorithm</strong>:</h4>
            <ul>
            <li><p>Scan database once, find frequent items and get f_list</p></li>
            <li><p>Recursively do database projection level by level</p></li>
            </ul>
            <p><strong>Benefits</strong>: <strong>only need to find frequent items</strong> in each projected database, instead of exploring candidate sequence generation. We have that the number of combinations is much less than their possible combinations. <strong>Works well in sparse databases</strong>.</p>
            <p><strong>Cost</strong>: partition and projection of databases.</p>
            <p>The idea is similar to FPGrowth and we can exploit two kinds of projections.</p>
            <p>We use two <span class="math inline">\(b\)</span> because it is the frequency on the <span class="math inline">\(f\)</span>-projection.</p>
            <p>We repeat this process and at the end we have the maximum frequent patterns.</p>
            <p>In parallel projection we use just one scan of the database, but we increase the size of memory occupation.</p>
            <p>In partition projection we generate only the projection with the last item<!-- , we have the complete $v$-projection but not the $e$-projection, $e$-projection and so on but we have them when we exploit the sequences in the $f$-projection-->.</p>
            <p>We do not increase the memory occupation in respect to the original database, but we generate these projections when we analyze the previous projections.</p>
            <p>At the end we have all the projections with all the sequences inside.</p>
            <h3 data-number="12.8.4" id="mining-by-alternative-level-projected-databases"><span class="header-section-number">12.8.4</span> Mining by <strong>Alternative Level</strong> Projected Databases</h3>
            <p>We can improve what we are doing with the projection database because we can work suggesting, for each projected database, the possible sequences.</p>
            <p>We can speed-up the process of the identification of candidate sequence, exploiting some heuristic.</p>
            <h4 class="unnumbered" data-number="" id="algorithm-6"><strong>Algorithm</strong></h4>
            <p>We generate <strong>frequent item matrix</strong> <span class="math inline">\(\rightarrow\)</span> a triangular matrix <span class="math inline">\(F[j, k]\)</span>, where <span class="math inline">\(1 \le j \le m\)</span> and <span class="math inline">\(1 \le k \le j\)</span>, <span class="math inline">\(m\)</span> is the number of frequent items.</p>
            <p><span class="math inline">\(F[j, j]\)</span> has only one counter, recording the appearance of sequence &lt; jj &gt;</p>
            <p>he generic element <span class="math inline">\(F[ j,k ]\)</span> has 3 counters (A, B, C), where:</p>
            <p><strong>A</strong>: number of occurrences that <span class="math inline">\(k\)</span> occurs after <span class="math inline">\(j\)</span> which is &lt; jk &gt;</p>
            <p><strong>B</strong>: number of occurrences that <span class="math inline">\(k\)</span> occurs before <span class="math inline">\(j\)</span> which is &lt; kj &gt;</p>
            <p><strong>C</strong>: number of occurrences that <span class="math inline">\(j\)</span> occurs concurrently with <span class="math inline">\(k\)</span> &lt;( jk)&gt;</p>
            <p>This matrix is produced scanning the projection we have, so the first sequence &lt; bd ) c b (ac) &gt; increases the first two counters of matrix <span class="math inline">\(F[ b,c ]\)</span> by 1 (because we have just that bc) since two cases , &lt;b c&gt; and &lt;c b&gt;, but not &lt;( bc )&gt; occur here. How can we produce this matrix?</p>
            <p>If we have this SDB:</p>
            <p><img src="../media/image692.png" /></p>
            <p>bcdaef in columns and rows that are our items and they’re sorted in decreasing order with respect to the frequence. It is a symmetric matrix.</p>
            <p>Along the diagonal we have the number of sequences in which we have repetition of the same item, so in this case when we consider the value in diagonal we have to consider if we have two times that sequence. In the sequence database we can see that in the first three and last sequence we have the repetition of b.</p>
            <p>We have 4 in the diagonal because 4 sequences out of 5 have this repetition.</p>
            <p>For c we have 1 because we just have one sequence with the repetition of c.</p>
            <p>For each other item we have a tuple where, for example for (b,c), the items consider the sequences in which we just have b and c, the sequences in which we have c and b and the sequences containing &lt;bc&gt;.</p>
            <p>This matrix is useful to make considerations for the generation of possible sequences.</p>
            <p>Then <strong>the frequent item matrix is used to generate the length 2 sequential patterns</strong> and a set of projected databases, which are then used to generate length 3 and longer sequential patterns.</p>
            <p>We produce a <strong>set of annotations</strong> to make this work, that indicates which set of items or sequences should be examined in the projection and later mining of level 3 databases.</p>
            <p>We have two types of annotations:</p>
            <ul>
            <li><p>Annotations of <strong>item repeating patterns</strong></p></li>
            <li><p>Annotations of <strong>projected databases</strong></p></li>
            </ul>
            <p>They avoid considering candidate sequences we are sure they are not frequent, and they tell us which to consider with a high probability they are frequent.</p>
            <p>To <strong>generate Length-2 Sequential Patterns</strong>, we have the indication of the two-sequential pattern directly but also the indication of the sequence.</p>
            <p>We can directly analyze if they are frequent.</p>
            <p>For each counter, if the value in the counter is no less than min_sup, output the corresponding sequential pattern.</p>
            <p><img src="../media/image693.png" /></p>
            <p>We can generate all frequent 2-sequences based on the minsup we fixed.</p>
            <p><strong>Generating Annotations on Item repeating Patterns</strong></p>
            <p>These are annotation in which each item in the pairs can be really repeated.</p>
            <p>We want to understand if the item in the 2-sequence can be repeated.</p>
            <p>If we can have repetition of items in the pattern, this means we should have <span class="math inline">\(F[j,j]\)</span>, we are considering the repetition of <span class="math inline">\(j\)</span> in a sequence.</p>
            <p>If <span class="math inline">\(F[j,j] \ge minsup\)</span>, then we have the possibility to generate sequences with the repetition of <span class="math inline">\(j\)</span>. We have repetition of <span class="math inline">\(j\)</span> that are frequent.</p>
            <p>We can consider the repetition of <span class="math inline">\(j\)</span> and we have to analyze the possibility to have repetition of <span class="math inline">\(j\)</span> with other items as possible frequent sequence in our db.</p>
            <p>We have to remember that the information on the diagonal is the number of sequences in which we have repetition of the item we are considering.</p>
            <p><img src="../media/image694.png" /></p>
            <p><strong>i+</strong> means that we could have sequence &lt;ij&gt; but we can also have the repetition of i, we can have &lt;iij&gt; or &lt;iji&gt;.</p>
            <p>If <span class="math inline">\(F[j,j] \ge min\_sup\)</span> succeds we have to generate <strong>j+</strong> for the same reason.</p>
            <p><img src="../media/image695.png" /></p>
            <p>We have sequence &lt;bf&gt; in two sequences in the sequence database, we have &lt;fb&gt; in two sequences in the sequence database and &lt;bf&gt; as itemset in two sequences in the sequence database.</p>
            <p>If I consider b, I discover that b is present in 4 sequences, but it is in 4 sequences as &lt;bb&gt;. This means that the sequence &lt;bb&gt; is frequent.</p>
            <p>In conclusion, I have the possibility that for &lt;bf&gt; I can have the repetition of b, for example &lt;bbf&gt;.</p>
            <p>About &lt;fb&gt; it appears two times but ff is present in form of repetition two times in the sequences, so I have that &lt;fb&gt; is present and also f can be repeated.</p>
            <p>For this consideration &lt;bf&gt; and &lt;fb&gt; are present and frequent, and both b and f can be repeated.</p>
            <p>I can generate &lt;bbf&gt; or &lt;bff&gt;, both are possible because I have this consideration thanks to the matrix.</p>
            <p>Also, bf can be repeated because both b and f are frequent in terms of repetition, for example bfbf.</p>
            <p>This is why we have <strong>{b+f+}</strong>.</p>
            <p>&lt;be&gt; separately is frequent because it is present in three sequences, eb is not and e is not frequent.</p>
            <p>We can exploit sequences with the repetition of b but not with the repetition of e, we can explore bbe or bbbe but not beee, because e is not frequent, we have &lt;b*e&gt;.</p>
            <p>The difference between the two annotations <strong>&lt;b*e&gt;</strong> and <strong>{b+f+}</strong>, is that in &lt;b*e&gt; we consider <strong>only the sequence be</strong> while in the {b+f+}, we can consider bf, fb but <strong>also the possible repetition of bf</strong>.</p>
            <p>With these annotations we can generate possible candidate sequences that can be frequent.</p>
            <p><strong>Generating Annotations on Projected Databases</strong></p>
            <p>For row <span class="math inline">\(j\)</span></p>
            <ul>
            <li><p>For each <span class="math inline">\(i &lt; j\)</span>, if <span class="math inline">\(F[ i , j], F[k, j]\)</span> and <span class="math inline">\(F[ i , k]( k &lt; i )\)</span> may form a pattern generating triple (i.e., all the corresponding pairs are frequent), <span class="math inline">\(k\)</span> should be added to <span class="math inline">\(i\)</span>’s projected column set</p></li>
            <li><p>If there is a choice between sequence or set, <strong>sequence is preferred</strong></p></li>
            </ul>
            <p><img src="../media/image696.png" /></p>
            <p>If we consider &lt;(ce)&gt;we have two, so it is frequent as itemset.</p>
            <p>be and bc are frequent, so we can have that ce can be combined with b because be and bc are both frequent.</p>
            <p>We can generate just this &lt;(ce)&gt;:{b} that indicates we can generate a projection along ce and b can form a frequent sequence with ce.</p>
            <p>We generate possible projections of the sequence database along specific items.</p>
            <p>At the end we have this table:</p>
            <table>
            <colgroup>
            <col style="width: 7%" />
            <col style="width: 29%" />
            <col style="width: 33%" />
            <col style="width: 29%" />
            </colgroup>
            <thead>
            <tr class="header">
            <th>Item</th>
            <th>Length-2 seq pat.</th>
            <th>Ann. on rep. Items</th>
            <th>Ann. on proj. DBs</th>
            </tr>
            </thead>
            <tbody>
            <tr class="odd">
            <td>f</td>
            <td>&lt;bf&gt;:2, &lt;fb&gt;:2, &lt;(bf)&gt;:2</td>
            <td>&lt;b<sup>+</sup>f<sup>+</sup>&gt;</td>
            <td>None</td>
            </tr>
            <tr class="even">
            <td>e</td>
            <td>&lt;be&gt;:3, &lt;(ce)&gt;:2</td>
            <td>&lt;b<sup>+</sup>e&gt;</td>
            <td>&lt;(ce)&gt;:{b}</td>
            </tr>
            <tr class="odd">
            <td>d</td>
            <td>&lt;bd&gt;:2, &lt;db&gt;:2, &lt;(bd)&gt;:2, &lt;cd&gt;:2, &lt;dc&gt;:2, &lt;da&gt;:2</td>
            <td>{b<sup>+</sup>d}, &lt;da<sup>+</sup>&gt;</td>
            <td>&lt;da&gt;:{b,c}, {cd}:{b}</td>
            </tr>
            <tr class="even">
            <td>a</td>
            <td>&lt;ba&gt;:3, &lt;ab&gt;:2, &lt;ca&gt;:2, &lt;aa&gt;:2</td>
            <td>&lt;aa<sup>+</sup>&gt;, {a<sup>+</sup>b<sup>+</sup>}, &lt;ca<sup>+</sup>&gt;</td>
            <td>&lt;ca&gt;:{b}</td>
            </tr>
            <tr class="odd">
            <td>c</td>
            <td>&lt;bc&gt;:4, &lt;cb&gt;:3</td>
            <td>{b<sup>+</sup>c}</td>
            <td>None</td>
            </tr>
            <tr class="even">
            <td>b</td>
            <td>&lt;bb&gt;:4</td>
            <td>&lt;bb<sup>+</sup>&gt;</td>
            <td>None</td>
            </tr>
            </tbody>
            </table>
            <p><img src="../media/image698.png" /></p>
            <p>We have the length-2 sequential patterns, annotations on repeated items and annotations on projected databases.</p>
            <p>Starting from this we can generate candidate sequences we can explore.</p>
            <p>Based on the annotations for item repeating patterns and projected databases, S is scanned one more time</p>
            <p>The set item repeating patterns generated is {&lt; bbf &gt;:2,&lt; fbf &gt;: 2, &lt;(bf)b&gt;:2 , &lt;(bf)f&gt;:2, &lt;(bf)bf&gt;:2,&lt;( bd )b&gt;:2, &lt;bba &gt;:2, &lt;aba&gt;:2, &lt;abb &gt;:2, &lt;bcb &gt;:3, &lt;bbc &gt;:</p>
            <p>There are four projected databases: &lt;( ce )&gt;:{b}, &lt; b,c&gt; }, {cd}:{b} and &lt;ca &gt;:{b}.</p>
            <p>For a projected database whose annotation contains exactly three items, its associated sequential patterns can be obtained by a simple scan of the projected database.</p>
            <p>For a projected database whose annotation contains more than three items, one can construct frequent item matrix for this projected database and recursively mine its sequential patterns by the alternative level projection technique.</p>
            <p>This allows us to reduce the complexity of the problem just guiding the search for frequent sequences.</p>
            <p>In fact, we reduce the candidate sequences to test.</p>
            <p>We use some datasets and some comparison algorithms.</p>
            <p><img src="../media/image699.png" /></p>
            <p>The <em>FreeSpan</em> with annotations is really faster and decreasing minsup the run-time is almost constant.</p>
            <p>The advantages on <em>FreeSpan</em> in respect to the Apriori-like methods is that:</p>
            <ul>
            <li><p>we project a large sequence database recursively into a set of small projected sequence databases based on the currently mined frequent sets</p></li>
            <li><p>the alternatively-level projection in FreeSpan <strong>reduces the cost of scanning multiple projected databases</strong> and <strong>takes advantages of Apriori-like 3-way candidate filtering</strong></p></li>
            </ul>
            <p>We can reduce the number of candidate sequences and using annotations we can help to investigate the space.</p>
            <h1 data-number="13" id="data-stream-analysis"><span class="header-section-number">13</span> Data Stream Analysis</h1>
            <p>We have to analyze data in streaming.</p>
            <p>Data stream a potentially unbounded , ordered sequence of instances . A data stream <span class="math inline">\(S\)</span> may be shown as <span class="math inline">\(S = \{ x_1 , x_2 , x_3 , ... , x_N \}\)</span>, where <span class="math inline">\(x_i\)</span> is <span class="math inline">\(i\)</span>-th data instance, which is a <span class="math inline">\(d\)</span>-dimensional feature vector and <span class="math inline">\(N\)</span> goes to infinity.</p>
            <p><img src="../media/image700.png" /></p>
            <p>We theoretically have an infinite quantity of data. We have a time order.</p>
            <p>The input is a data stream, we should have a stream processing engine to elaborate then and we want to produce knowledge in output.</p>
            <p>In a classification problem we assumed to have a bounded training set. We train the classifier using it and it must be available, here we assume it arrives in streaming.</p>
            <p>We have to produce a classifier or a cluster when we start to receive data, we don’t have to wait them all.</p>
            <p>With new instances we will improve the classifier.</p>
            <p>We have to approach this problem providing a new algorithm.</p>
            <p>We have to tune the clusters or the classification model,</p>
            <p>Traditional Data Mining techniques usually require</p>
            <ul>
            <li><p><strong>Entire dataset to be present</strong></p></li>
            <li><p><strong>Multiple scans of the overall dataset</strong></p></li>
            <li><p><strong>Random access to instances</strong></p></li>
            <li><p><strong>Computationally heavy learning phases</strong>, except in lazy learners</p></li>
            </ul>
            <h2 data-number="13.1" id="challenges-of-stream-mining"><span class="header-section-number">13.1</span> Challenges of stream mining</h2>
            <ul>
            <li><p><strong>Impractical</strong> (and impossible) to <strong>store the whole dataset</strong></p></li>
            <li><p><strong>Impractical</strong> (and impossible) to <strong>perform multiple scans of the overall dataset</strong>, we have an high number of instances and we receive them in streaming</p></li>
            <li><p><strong>Random access is expensive</strong></p></li>
            <li><p><strong>Simple calculation per data</strong> due to time and space constraints</p></li>
            </ul>
            <h2 data-number="13.2" id="motivation"><span class="header-section-number">13.2</span> Motivation</h2>
            <p>A growing number of applications generate streams of data</p>
            <ul>
            <li><p>Performance measurements in network monitoring and traffic management</p></li>
            <li><p>Log records generated by Web Servers</p></li>
            <li><p>Tweets on Twitter</p></li>
            <li><p>Transactions in retail chains, ATM operations in banks</p></li>
            <li><p>Sensor network data</p></li>
            </ul>
            <p>Application characteristics</p>
            <ul>
            <li><p>Massive volumes of data</p></li>
            <li><p>Records arrive at a rapid rate</p></li>
            </ul>
            <h2 data-number="13.3" id="computational-model"><span class="header-section-number">13.3</span> Computational Model</h2>
            <p><img src="../media/image701.png" /></p>
            <p>The <strong>SPE</strong> elaborate the data stream and produce an approximated answer.</p>
            <h3 data-number="13.3.1" id="stream-processing-requirements"><span class="header-section-number">13.3.1</span> Stream processing requirements</h3>
            <ul>
            <li><p><strong>Single pass</strong>: Each record is examined at most once, we don’t have time to perform iterations on these instanced</p></li>
            <li><p><strong>Bounded storage</strong>: Limited Memory (M) for storing synopsis and these are representation of the data streams</p></li>
            <li><p><strong>Real time</strong>: Per record processing time (to maintain synopsis) must be low</p></li>
            </ul>
            <h2 data-number="13.4" id="algorithms-1"><span class="header-section-number">13.4</span> <strong>Algorithms</strong></h2>
            <p>Generally, algorithms <strong>compute approximate answers</strong>, we have to balance time and precision, it’s difficult to compute answers accurately with limited memory.</p>
            <ul>
            <li><p>Approximate answers - <strong>Deterministic bound</strong></p>
            <p>Algorithms only compute an approximate answer but bounds on error. These algorithms can provide approximate answers but also guarantee that this approximation provides bounds on error, promising that the error is bounded.</p></li>
            <li><p>Approximate answers - <strong>Probabilistic bound</strong></p>
            <p>Algorithms compute an approximate answer with high probability</p>
            <p>With probability at least <span class="math inline">\(1-\delta\)</span>, the computed answer is within a factor <span class="math inline">\(\varepsilon\)</span> of the actual answer</p>
            <p>We have a limited error with respect to the ideal answer.</p></li>
            </ul>
            <p>These are single pass algorithms for processing streams <strong>also applicable to (massive) terabyte databases</strong>!</p>
            <h3 data-number="13.4.1" id="concept-drift"><span class="header-section-number">13.4.1</span> <strong>Concept Drift</strong></h3>
            <p>For efficiency problems we just need to perform the single pass.</p>
            <p>One of the problems we have is the problem of the <strong>concept drift</strong> which is the unforeseen change in statistical properties of data stream instances over time.</p>
            <p>If new instances have the same statistical property the classifier works properly, nothing is changing, otherwise the model we are implementing do not work well.</p>
            <p>It affects the model we generated, the classifier/cluster we generate can’t be valide anymore.</p>
            <p>We have to adapt the model to the condition we have.</p>
            <p>There are <strong>four types</strong> of concept drift: sudden, gradual, incremental and recurring.</p>
            <ul>
            <li><p><strong>Sudden concept drift</strong> : Between two consecutive instances, the change occurs at once , and after this time only instances of the new class are received</p>
            <p><img src="../media/image702.png" /></p></li>
            <li><p><strong>Gradual concept drift</strong> : The number of instances belonging to the previous class decreases gradually while the number of instances belonging to the new class increases over time. During a gradual concept drift, instances of both previous and new classes are visible. We have a period in which we have <strong>instances of both classes</strong>.</p>
            <p><img src="../media/image703.png" /></p></li>
            <li><p><strong>Incremental concept drift</strong> : Data instances belonging to the previous class evolves to a new class step by step. After the concept drift is completed, the previous class disappears. The instances that arrive during the concept drift are of transitional forms and they do not have to belong to either of the classes.</p>
            <p><img src="../media/image704.png" /></p></li>
            <li><p><strong>Recurring concept drift</strong> : The data instances change between two or more statistical characteristics several times. Neither of the classes disappears permanently but both of them arrive in turns.</p>
            <p><img src="../media/image705.png" /></p>
            <p>We have an oscillation between the two classes.</p></li>
            </ul>
            <p>We have to model the concept drift.</p>
            <h3 data-number="13.4.2" id="data-structures"><span class="header-section-number">13.4.2</span> <strong>Data Structures</strong></h3>
            <p>It is not possible to store and manage the whole input data, only a synopsis of the input stream is stored: special data structures enable to incrementally summarize the input stream.</p>
            <p>Four commonly used data structures:</p>
            <ol type="1">
            <li><p><strong>Feature vectors</strong>: summary of the data instances, we had these for example in <em>BIRCH</em></p></li>
            <li><p><strong>Prototype arrays</strong>: keep only a number of representative instances that exemplify the data</p></li>
            <li><p><strong>Core set trees</strong>: keep the summary in a tree structure</p></li>
            <li><p><strong>Grids</strong>: keep the data density in the feature space, we use a grid and we update the density of cells in the grid</p></li>
            </ol>
            <p>They are used to summarize the data.</p>
            <p>The ML algorithm will work on this data structure. We receive the data, update the data structure and apply ML algorithms using the data structure, the summarization of instances.</p>
            <p>The update should be fast, because when we receive an instance and update the data structure, we have to analyze the instances. With a high rate of receiving instances we need a fast way to update data structure.</p>
            <h3 data-number="13.4.3" id="the-window-model"><span class="header-section-number">13.4.3</span> <strong>The Window Model</strong></h3>
            <p>Sometimes it is more efficient to process recent data instead of the whole data and sometimes also more robust, we can have the concept drift and we must update our model but forcing more on the recent instances.</p>
            <p>We need to give <strong>more importance to the recent instances</strong>.</p>
            <p>To exploit this concept, we can use window models.</p>
            <h4 class="unnumbered" data-number="" id="damped-window-model"><strong>Damped Window Model</strong></h4>
            <p>In <strong>damped window model</strong> recent data have more weight than the older data: the importance of the instances decreases by time; they will have an higher weight.</p>
            <p>We can decide the slope of the red line just to assign more importance to recent instances.</p>
            <p>Usually implemented using <strong>decay functions</strong> which scale down the weight of the instances, depending on the time passed since the instance is received</p>
            <p><span class="math display">\[
                \large{f(t) = 2^{-\lambda t}}
            \]</span></p>
            <p>For example, we can use this exponential function where lambda is the decay rate. Higher decay rate in the function means a more rapid decrease in the value</p>
            <p><img src="../media/image707.png" /></p>
            <p>We have to use data structure that allow us to store information we need, to generate for example clustering without losing information or losing few of them.</p>
            <p>We typically focus on specific windows at the time to generate updates of the structures.</p>
            <p>If we create the model at the beginning, if the distribution of data changes we need to change the model, and here is the problem of concept drift.</p>
            <p>One possible approach is to retrain periodically the model to tune it on changes on the distribution of data.</p>
            <p>In the damped window model, <strong>old data weights almost zero</strong>, they disappear with the increase of the time.</p>
            <h4 class="unnumbered" data-number="" id="landmark-window-model"><strong>Landmark window model</strong></h4>
            <p>The whole data between two landmarks are included in the processing and all of the instances have equal weight.</p>
            <p>We work focusing on a window between two landmarks and work on it.</p>
            <p>Consecutive windows do not intersect and the new window just begins from the point the previous window ends.</p>
            <p>Let <span class="math inline">\(w\)</span> be the window length. Then, data instances belonging to the <span class="math inline">\(m\)</span>-th window are calculated using:</p>
            <p><span class="math display">\[
                \Large{W_m = [x_{m * w}, ... ,x_{(m+1)*w-1}]} \hspace{1cm}
                m = \left\lfloor \dfrac{i}{w} \right\rfloor
            \]</span></p>
            <p><img src="../media/image710.png" /></p>
            <h4 class="unnumbered" data-number="" id="sliding-window-model"><strong>Sliding window model</strong></h4>
            <p><strong>The window swaps one instance at each step</strong>: the older instance moves out of the window, and the most recent instance moves in to the window by FIFO style.</p>
            <p>All instances in the window have equal weight and consecutive windows mostly overlap</p>
            <p>Let <span class="math inline">\(w\)</span> be the window length.</p>
            <p>Then, data instances belonging to the <span class="math inline">\(m\)</span>-th window are calculated using</p>
            <p><span class="math display">\[
                \Large{W_m = [x_m, ..., x_{(m+w-1)}]}
            \]</span></p>
            <p><img src="../media/image712.png" /></p>
            <p>These are data structure model to preprocess the data.</p>
            <h2 data-number="13.5" id="data-stream-clustering"><span class="header-section-number">13.5</span> <strong>Data Stream Clustering</strong></h2>
            <p>Several algorithms were proposed to manage data space.</p>
            <p><em>BIRCH</em> is an example of algorithm that can be used to manage data streams.</p>
            <p>It has a data structure to summarize information relative to the data.</p>
            <p><img src="../media/image713.png" /></p>
            <p>We have these 5 kinds of approaches.</p>
            <p>In classification we need the label with data and it’s not typically common.</p>
            <p>In prediction it’s different because after some time we can have the ground-truth.</p>
            <p><strong>In most cases, true class labels are not available for stream instances</strong> and there is no prior knowledge about the number of classes.</p>
            <p>Therefore, clustering, being unsupervised is <strong>one of the most suitable data mining and data analysis methods for data streams</strong>.</p>
            <h3 data-number="13.5.1" id="adaptive-streaming-k-means"><span class="header-section-number">13.5.1</span> <strong>Adaptive Streaming k-Means</strong></h3>
            <p>We need to update clusters when we receive data in streaming.</p>
            <p><img src="../media/image714.png" /></p>
            <p>We just use a number of instances at the beginning for initialization, and this is the parameter <span class="math inline">\(l\)</span>.</p>
            <p>We select the cluster with highest silhouette.</p>
            <p>We obtain candidate centroids from that function.</p>
            <p>Until we don’t have any change we exploit the first centroids computed, while if we have some change we re-inizialize again centroids.</p>
            <h4 class="unnumbered" data-number="" id="initialization-phase-1"><strong>Initialization phase</strong></h4>
            <p>The function <em>determineCentroids()</em> finds <span class="math inline">\(k\)</span> and determines candidate centroids.</p>
            <p>It estimates the <strong>probability density function</strong> (PDF) of the data for each feature and determine the directional changes of the PDF curves: each change identifies a new region. The region can be defined as the area between two consecutive directional changes of the PDF curve.</p>
            <p><img src="../media/image715.png" /></p>
            <p>We use this function and we just determine the variation between two different regions in the PDF function.</p>
            <p>Number of regions is considered as a candidate <span class="math inline">\(k\)</span> and centers of these regions are considered as candidate initial centroids</p>
            <p>We perform this work feature by feature, so we will have a different <span class="math inline">\(k\)</span> for each feature.</p>
            <p>Different features generally show different distributions and different centroids, so we have that <span class="math inline">\(k\)</span> can vary:</p>
            <p><span class="math display">\[
                \large{k \in [k_{min}, k_{min}+k_{max}]}
            \]</span></p>
            <p>The loop 2-5 is executed for these values of <span class="math inline">\(k\)</span> and for candidate centroids, the computations of using the kmeans of clusters.</p>
            <p>Clustering results of different <span class="math inline">\(k\)</span> values are <strong>compared according to silhouette coefficient</strong> and <strong>best <span class="math inline">\(k\)</span> is selected</strong> with its corresponding centroids.</p>
            <p>During the initialization phase we try to do this optimization.</p>
            <h4 class="unnumbered" data-number="" id="continuous-clustering-phase"><strong>Continuous clustering phase</strong></h4>
            <p>We start to receive new instances and can locate them considering the centroids.</p>
            <p>function <em>changeDetected()</em>:</p>
            <p>We compute the standard deviation(<span class="math inline">\(\sigma\)</span>) and mean(<span class="math inline">\(\mu\)</span>) of the input data and are stored during the execution</p>
            <p>The algorithm tracks how these two values change over time and predicts a concept drift according to the change. We monitor if <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(\mu\)</span> changes along the time, in that case we have concept drift. When a concept drift is predicted, current cluster centroids are no longer valid (we have a change in distribution of data). In such a case the concept drift is realized at line 9 and a re-initialization is triggered at line 10, by using <span class="math inline">\(l\)</span> instances to update the clusters we should have.</p>
            <p>We have to perform it feature by feature, and we have to decide the threshold to use.</p>
            <h4 class="unnumbered" data-number="" id="complexity-analysis"><strong>Complexity Analysis</strong></h4>
            <p>Let <span class="math inline">\(l\)</span> be the length of the initial data sequence, and <span class="math inline">\(d\)</span> be the data dimension.</p>
            <p>The complexity of estimating <span class="math inline">\(k\)</span> for a single dimension is <span class="math inline">\(O(l)\)</span></p>
            <p>Since this estimation is <strong>performed for all dimensions</strong>, total <span class="math inline">\(k\)</span> estimation complexity becomes <span class="math inline">\(O(d \cdot l)\)</span>.</p>
            <p>After determining initial centroids running k-means takes <span class="math inline">\(O(d \cdot k \cdot cs )\)</span>, since no iterations of the algorithm are needed, where <span class="math inline">\(cs\)</span> is the number of different centroid sets.</p>
            <p>Assigning a newly received data instance to the nearest cluster during the online phase is <span class="math inline">\(O(k)\)</span>, we have to compute the distance of the new instance and <span class="math inline">\(k\)</span> centroids. As a result, <strong>total worst case complexity of the algorithm</strong> is <span class="math inline">\(O(k) + O(d \cdot l) + O(d \cdot k \cdot cs ) = O(d \cdot l) + O(d \cdot k \cdot cs )\)</span>.</p>
            <p>We have to spend time in inizialization phase, but <strong>computing the cluster for the new instance is easy</strong>, we just to compute the distance, but it works until the distribution remains the same.</p>
            <h3 data-number="13.5.2" id="mudi-stream"><span class="header-section-number">13.5.2</span> <strong>MuDi Stream</strong></h3>
            <p>MuDi Stream is a <strong>hybrid algorithm</strong> based on both <strong>density based</strong> and <strong>grid based</strong> approaches</p>
            <p>Input data instances are clustered in a density based approach and outliers are detected using grids (to reduce the computational time to work with the cell instead of the single instance).</p>
            <p>For <strong>Data synopsis, core mini clusters are used</strong>: they are <strong>specialized feature vectors</strong> which keep <strong>weight, center, radius and the maximum distance</strong> from an instance to the mean. In the <strong>online phase</strong> core mini clusters are created and kept up to date for each new data instance, we update the data structure and periodically we have an <strong>offline phase</strong> to compute clusters. In the offline phase final clustering is executed over the core mini clusters. The online phase should perform very fast because we work at the arrival of the data.</p>
            <h4 class="unnumbered" data-number="" id="online-phase"><strong>Online Phase</strong>:</h4>
            <p><img src="../media/image717.png" /></p>
            <p>Parameters affect the final result.</p>
            <p><span class="math inline">\(cmc\)</span> = core mini cluster</p>
            <p>Instances are located in specific cells while arriving, at the beginning not dense and while receiving instances, subcells become dense.</p>
            <h4 class="unnumbered" data-number="" id="offline-phase"><strong>Offline Phase</strong>:</h4>
            <p><img src="../media/image718.png" /></p>
            <p>It is similar to <em>DBSCAN</em> working on dense core mini-clusters.</p>
            <p>Some <span class="math inline">\(cmc_p\)</span> can be detected as outlier because isolated and not used to create the cluster.</p>
            <p>It’s hybrid because <strong>we use grids but we exploit density</strong>.</p>
            <p>In the offline phase we use DBSCAN but <strong>with core mini-clusters</strong> and not instances.</p>
            <p>Inside a loop, an unvisited core mini-cluster is randomly chosen at line 3 and marked as visited at line 4. If this core mini-cluster has no neighbors, it is marked as noise at line 16.</p>
            <p>If it has neighbors, a new final cluster is created with this core mini-cluster and its neighbors, at lines 6-8. After that, each unvisited core mini-cluster in the new created final cluster is marked as visited and its neighbors are added to the same final cluster, at lines 9-14.</p>
            <p>This loop continues until all core mini-clusters are marked as visited.</p>
            <p>MUDI-stream is <strong>not suitable for high dimensional data</strong>, which makes the processing time longer, because of the grid structure (we would have an high number of cells).</p>
            <p>Clustering quality of MuDi-Stream <strong>strongly depends on input parameters density threshold</strong>, decay rate for damped window model and grid granularity.</p>
            <p>These parameters require an expert knowledge about the data.</p>
            <p>The windows is present because we have a decrease of the weight of the instances in the cells and that’s why we can periodically remove low weighted grids and <span class="math inline">\(cmc\text{s}\)</span>.</p>
            <h4 class="unnumbered" data-number="" id="complexity-analysis-1"><strong>Complexity Analysis</strong></h4>
            <p>Complexity of this linear search on core mini-clusters for each new data instance is <span class="math inline">\(O(c)\)</span> where <span class="math inline">\(c\)</span> is the number of core mini clusters</p>
            <p>Let <span class="math inline">\(G\)</span> be total density grids for all dimensions, which is exponential to the number of dimensions. Space complexity of the grid is <span class="math inline">\(O(log G)\)</span> because the scattered grid are pruned during the execution.</p>
            <p>Moreover, time complexity of mapping a data instance to the grid is <span class="math inline">\(O(log log G)\)</span> because the list of the grids is maintained as a tree During the pruning, all core mini clusters and grids are examined.</p>
            <p>This makes time complexity of pruning <span class="math inline">\(O(c)\)</span> for core mini clusters and <span class="math inline">\(O(log G)\)</span> for grids.</p>
            <p>As a result, the overall time complexity of MuDi Stream is <span class="math inline">\(O(c) + O(log log G) + O(c) + O(log G) = O(c) + O(log G )\)</span>.</p>
            <h3 data-number="13.5.3" id="cedas"><span class="header-section-number">13.5.3</span> <strong>CEDAS</strong></h3>
            <p>Clustering of evolving data streams into <strong>arbitrarily shaped clusters</strong></p>
            <p><em>CEDAS</em> is a <strong>fully online</strong> data stream clustering algorithm.</p>
            <p>Density based algorithm designed for clustering data streams with concept drifts</p>
            <p><strong>Damped window model</strong> is employed with a <strong>linear decay function</strong> instead of an exponential one</p>
            <p><img src="../media/image719.png" /></p>
            <p>Reducing by lambda is the decay which in fact is linear.</p>
            <p>Energy decreases from 1 with the time.</p>
            <h4 class="unnumbered" data-number="" id="complexity-analysis-2"><strong>Complexity Analysis</strong></h4>
            <p>For each new data instance, <em>CEDAS</em> performs a <strong>linear search</strong> on the micro-clusters.</p>
            <p>Complexity of this linear search is <span class="math inline">\(O(c)\)</span> where <span class="math inline">\(c\)</span> is the number of micro-clusters.</p>
            <p>After that, energy of each micro-cluster is reduced, which also requires an <span class="math inline">\(O(c)\)</span> complexity.</p>
            <p>The last step, which updates the graph structure, is executed only when a new micro-cluster is created or removed. In the worst case, all micro-clusters are visited, so worst case time complexity of this step is again <span class="math inline">\(O(c)\)</span>.</p>
            <p>Therefore, the overall time complexity of CEDAS is <span class="math inline">\(O(c)\)</span>, the number of micro-clusters.</p>
            <h3 data-number="13.5.4" id="improved-data-stream-clustering-algorithm"><span class="header-section-number">13.5.4</span> <strong>Improved Data Stream Clustering Algorithm</strong></h3>
            <p>Improved clustering algorithm based on high speed network data stream.</p>
            <p>Online phase composed of two main sub phases: <strong>Initialization phase</strong> and <strong>continuous clustering phase</strong>.</p>
            <p>Major micro clusters and critical micro clusters are used</p>
            <p>Major microclusters have <strong>high densities</strong> and will be included in the final clustering process</p>
            <p>Critical micro clusters have <strong>low densities</strong> and treated as <strong>potential outliers</strong></p>
            <p><strong>Damped window model</strong> is used and low weighted major and critical micro clusters are removed periodically.</p>
            <p>We exploit the decay rate again.</p>
            <p>Threshold values of major and critical micro clusters are global parameters in the algorithm, instead of being specific to each micro cluster. However, they are dynamic parameters and continuously updated during the execution.</p>
            <h4 class="unnumbered" data-number="" id="online-phase-1"><strong>Online phase</strong></h4>
            <p><img src="../media/image720.png" /></p>
            <p>In initialization we can work with any kind of algorithm, because we work with static data instances.</p>
            <p>The OR depends on if <span class="math inline">\(x\)</span> is closer to a major or critical micro-cluster, or far away from both.</p>
            <p>The pruning period depending on the decay rate</p>
            <h4 class="unnumbered" data-number="" id="offline-phase-1"><strong>Offline phase</strong></h4>
            <p><img src="../media/image721.png" /></p>
            <p>At the end we create clusters starting from the major micro-cluster and adding other micro-clusters.</p>
            <p>If the distance between a micro-cluster and another major micro-cluster is less than or equal to the sum of their radii, then they are directly density reachable.</p>
            <p>If any adjacent two clusters in a set of micro-clusters are directly density reachable, then the set of micro-clusters is density reachable, so we can add them.</p>
            <h4 class="unnumbered" data-number="" id="complexity-analysis-3"><strong>Complexity Analysis</strong></h4>
            <p>Let <span class="math inline">\(l\)</span> be the length of the initial data sequence. Complexity of the initialization equals to complexity of <em>DBSCAN</em>, which is <span class="math inline">\(O(l \cdot log l)\)</span> in average and <span class="math inline">\(O(l^2)\)</span> in worst case.</p>
            <p>In the continuous clustering phase, a linear search is performed on micro-clusters for each new data instance. Complexity of this linear search is <span class="math inline">\(O(c)\)</span> where <span class="math inline">\(c\)</span> is the number of micro-clusters.</p>
            <p>When it is pruning period, pruning task is executed for each microcluster one by one and this also requires a complexity of <span class="math inline">\(O(c)\)</span>.</p>
            <p>Therefore, the total worst-case complexity is <span class="math inline">\(O(c) + O(c) = O(c)\)</span>.</p>
            <h3 data-number="13.5.5" id="dbiecm"><span class="header-section-number">13.5.5</span> <strong>DBIECM</strong></h3>
            <p>DBIECM is an <strong>online, distance-based, evolving</strong> data stream clustering algorithm</p>
            <p>Exploits the <strong>Davies Bouldin Index</strong> (DBI) which is used as the evaluation criterion, that uses dispersion and separation.</p>
            <p><span class="math display">\[
                \large{V_{DB} = \dfrac{1}{k}\sum_{i = 1}^{k}{R_i}}
            \]</span> where <span class="math inline">\(R_i = \max_{i \ne j}{R_{ij}} \hspace{1cm}R_{ij} = \dfrac{S_i + S_j}{D_{ij}}\)</span></p>
            <p><span class="math display">\[
                \large{S_i = \left(\dfrac{1}{|C_i|} \sum_{x \in C_i}{D^p}(x,v_i) \right)^{\dfrac{1}{p}}, p &gt; 0 \hspace{1cm}D_{ij} = \left(\sum_{l = 1}^{d}{|v_{il}-v_{jl}|^t} \right)^{\dfrac{1}{d}}, t &gt; 1}
            \]</span> Where:</p>
            <ul>
            <li><p><span class="math inline">\(S_i \rightarrow\)</span> Dispersion</p></li>
            <li><p><span class="math inline">\(C_i \rightarrow\)</span> Number of objects in cluster <span class="math inline">\(C_i\)</span></p></li>
            <li><p><span class="math inline">\(v_i \rightarrow\)</span> Centroid of cluster <span class="math inline">\(C_i\)</span></p></li>
            <li><p><span class="math inline">\(D_{ij} \rightarrow\)</span> Separation</p></li>
            </ul>
            <p>We take again in consideration <strong>compactness and separation</strong>.</p>
            <p><img src="../media/image723.png" /></p>
            <p>We exploit DBI to determine the best cluster to add these instances.</p>
            <p>DBIECM requires the <strong>maximum cluster radius as a parameter</strong>, this parameter directly affects the final cluster count and consequently the clustering quality.</p>
            <p>Maximum cluster radius strongly depends on the input data and <strong>requires an expert knowledge about the data</strong>. Being distance based, DBIECM can detect <strong>only hyperspherical clusters</strong>.</p>
            <p>DBIECM <strong>does not employ any time window model</strong>, thus no input data instance out dates, all input data exist in the final clustering Moreover, no outlier detection mechanism is implemented. However, it is possible to specify an outlier threshold value and mark the clusters with low cardinality as outliers.</p>
            <p>We don’t use a decay approach.</p>
            <h4 class="unnumbered" data-number="" id="complexity-analysis-4"><strong>Complexity Analysis</strong></h4>
            <p>When a new data instance is received, a linear search is performed on clusters. Complexity of this linear search is <span class="math inline">\(O(k)\)</span>.</p>
            <p>Pairwise distances between all clusters are used for DBI calculation, thus DBI calculation requires a complexity proportional to <span class="math inline">\(O(k^2)\)</span>.</p>
            <p>When there exist more than one candidate cluster for the new data instance, the instance is added to all of them one by one and DBI is calculated accordingly. This requires a complexity proportional to <span class="math inline">\(O(k^3)\)</span>.</p>
            <p>Therefore, although the average complexity of DBIECM depends on the input data, the <strong>total worst-case complexity</strong> is <span class="math inline">\(O(k) + O(k^3) = O(k^3)\)</span></p>
            <p><strong>If <span class="math inline">\(k\)</span> is big this complexity can be relevant</strong>.</p>
            <h3 data-number="13.5.6" id="i-hastream-in-my-notes-marcelloni-said-that-we-overlooked-it"><span class="header-section-number">13.5.6</span> <strong>I-HASTREAM</strong> (IN MY NOTES MARCELLONI SAID THAT WE OVERLOOKED IT)</h3>
            <p><strong>Two phases, adaptive, density based, hierarchical</strong>, data stream clustering algorithm.</p>
            <p>In the <strong>online phase</strong>, synopsis of the data is created as microclusters.</p>
            <p>In the <strong>offline phase</strong>, micro-clusters are maintained in a graph structure as a minimum spanning tree and hierarchical clustering is employed for the final clustering</p>
            <p>Main contributions of I-HASTREAM are to perform the final clustering on a minimum spanning tree and to incrementally update the minimum spanning tree according to the changes in the microclusters, instead of generating it from scratch. Both contributions are related to the offline phase.</p>
            <p><img src="../media/image724.png" /></p>
            <p>Because no algorithmic details are specified about the online phase, the <strong>analysis of the complexity of <em>I-HASTREAM</em> was not possible</strong>.</p>
            <h3 data-number="13.5.7" id="comparison-1"><span class="header-section-number">13.5.7</span> <strong>Comparison</strong></h3>
            <p><img src="../media/image725.png" /></p>
            <p>Most of them use a damped window method, so with a decay.</p>
            <p>Some exploits only online phase, others exploit both.</p>
            <p><img src="../media/image726.png" /></p>
            <p>They can adapt to concept drift.</p>
            <p>In online phase we update data structures and in offline phase we generate periodically clusters, exploiting k-means, DBSCAN or some other algorithm.</p>
            <p>It’s interesting to understand how clusters modify over time, considering the evolution of clusters, easier with k-means because we have centroids even if simpler information but not with DBSCAN where we consider shape, location and other things.</p>
            <h2 data-number="13.6" id="classification-1"><span class="header-section-number">13.6</span> <strong>Classification</strong></h2>
            <h3 data-number="13.6.1" id="vfdt-very-fast-decision-tree"><span class="header-section-number">13.6.1</span> <strong>VFDT</strong> (Very Fast Decision Tree)</h3>
            <p>With classification we have to assume to receive streams of labeled data.</p>
            <p>The problem here is to learn a classifier with data arriving in streaming, our training set is provided in streaming.</p>
            <p><strong>Classic decision tree learners</strong> assume all training data can be simultaneously stored in main memory</p>
            <p><strong>Disk based decision tree learners</strong> repeatedly read training data from disk sequentially</p>
            <p>Prohibitively expensive when learning complex trees</p>
            <p>We use a modification of decision trees, we learn decision tree while instances arrive assuming they can be infinite.</p>
            <p><strong>Goal</strong>: design decision tree learners that <strong>read each example at most once</strong>, and <strong>use a small constant time to process it</strong>.</p>
            <p>In order to find the best attribute at a node, it may be sufficient to consider only a small subset of the training examples that pass through that node.</p>
            <p>We want to be sure that the splitting is reliable.</p>
            <p>We want to understand if we can approximate the decision with a limited number of instances, being sure that we don’t have a high error in taking this decision.</p>
            <p>Given a stream of examples, we use the <strong>first ones to choose the root attribute</strong>.</p>
            <p>Once the root attribute is chosen, the <strong>successive examples are passed down to the corresponding leaves</strong>, and used to choose the attribute there, and so on recursively.</p>
            <p>We have to decide <strong>when to split a leaf to create a new subtree</strong>.</p>
            <p>We use the <strong>Hoeffding bound</strong> to decide how many examples are enough at each node to take a decision.</p>
            <p>Warning: the approach analyzed in the subsequent phases has some theoretical problem, but you can find it implemented in several tools which manage data streams.</p>
            <p>It works but have a wrong assumption(kek).</p>
            <h4 class="unnumbered" data-number="" id="hoeffding-bound"><strong>Hoeffding Bound</strong></h4>
            <p>Let <span class="math inline">\(X\)</span> a random variable varying in a range <span class="math inline">\(R\)</span></p>
            <p>Let us assume that we have <span class="math inline">\(n\)</span> observations of <span class="math inline">\(X\)</span></p>
            <p>Let <span class="math inline">\(x&#39;\)</span> be the average value of the <span class="math inline">\(n\)</span> observation</p>
            <p>The Hoeffding bound states that with probability <span class="math inline">\(1-\sigma\)</span> , the mean <span class="math inline">\(X&#39;\)</span> of <span class="math inline">\(X\)</span> is at least <span class="math inline">\(x&#39; - \varepsilon\)</span> where:</p>
            <p><span class="math display">\[
                \large{\varepsilon = \sqrt{\dfrac{R^2ln1/\delta}{2n}}}
            \]</span></p>
            <p>It guarantees the error is limited after a number of instances we are able to collect.</p>
            <p>We can set <span class="math inline">\(\varepsilon\)</span>, determine <span class="math inline">\(n\)</span>, the number of observations to be collected to be sure that the error we have is <span class="math inline">\(\varepsilon\)</span> when we approximate the average <span class="math inline">\(x\)</span> computed with the <span class="math inline">\(n\)</span> observation the mean of <span class="math inline">\(X\)</span> with that probability.</p>
            <p>If we set the probability and <span class="math inline">\(\varepsilon\)</span>, we have the number of instances to collect.</p>
            <p><strong>How do we use the Hoeffding Bound?</strong></p>
            <p>Let <span class="math inline">\(G(X_i)\)</span> be the heuristic measure used to choose test attributes (e.g.</p>
            <p>Information Gain, Gini Index)</p>
            <p><span class="math inline">\(X_A\)</span>: the attribute with the highest attribute evaluation value after seeing <span class="math inline">\(n\)</span> examples.</p>
            <p><span class="math inline">\(X_B\)</span>: the attribute with the second highest split evaluation function value after seeing n examples.</p>
            <p>Given a desired <span class="math inline">\(\delta\)</span>: if <span class="math inline">\(\Delta\bar{G} = \bar{G}(X_A) - \bar{G}(X_B) &gt; \varepsilon\)</span>, with <span class="math inline">\(\varepsilon = \sqrt{\dfrac{R^2ln1/\delta}{2n}}\)</span></p>
            <p>and R=lnc, where c is the number of classes, after seeing n examples at a node,</p>
            <p>the Hoeffding bound guarantees the true <span class="math inline">\(\Delta G \ge \Delta \bar{G} - \varepsilon &gt; 0\)</span> with probability <span class="math inline">\(1-\delta\)</span></p>
            <p>This node can be split using <span class="math inline">\(X_A\)</span> and the succeeding examples will be passed to the new leaves</p>
            <p>We can express the number of instances we need to have the possibility to split the node because we can decide that the attribute <span class="math inline">\(X_A\)</span> is the one we can choose for splitting the node.</p>
            <p>We can start with the root, collect instances, decide to split the root.</p>
            <p>When <span class="math inline">\(\Delta G\)</span>, the <span class="math inline">\(G\)</span> computed without splitting and with an attribute, is higher than <span class="math inline">\(\varepsilon\)</span> we are sure, for the Hoeffding Bound of the original paper, that <strong>the approximation is very close to the decision tree</strong> that we will have collecting all instances.</p>
            <h4 class="unnumbered" data-number="" id="problem"><strong>Problem</strong>:</h4>
            <p><strong>Split measures</strong>, like information gain and Gini index, <strong>cannot be expressed as a sum <span class="math inline">\(S\)</span> of elements <span class="math inline">\(Y_i\)</span>.</strong></p>
            <p>We have this as a problem because the Hoeffding Bound is defined for this kind of variable so we just expressed in terms of sum of elements, but <strong>the information of gain is not a sum of elements</strong>.</p>
            <p>When we just add instances, we don’t have that <span class="math inline">\(G\)</span> is a sum of these instances.</p>
            <p>This is why the formula we saw is <strong>not correct theoretically</strong>.</p>
            <p>Actually, the correct formula is:</p>
            <p><span class="math display">\[
                \large{\epsilon = C_{Gain}(K,N)\sqrt{\dfrac{ln(1/\delta)}{2N}}}
            \]</span></p>
            <p>where</p>
            <p><span class="math display">\[
                C_{Gain}(K,N) = 6(Klog_2eN + log_22N) + 2log_2K
            \]</span></p>
            <p>that <strong>changes the number of instances to collect that approximate the decision tree we would have having all instances</strong>.</p>
            <p><strong>Two considerations</strong>:</p>
            <ul>
            <li><p>Pre-pruning is carried out by considering at each node a “null” attribute <span class="math inline">\(X_A\)</span> that consists of not splitting the node. Thus <strong>a split will be performed if</strong>, with confidence <span class="math inline">\(1- \delta\)</span>, <strong>the best split found is better (according to <span class="math inline">\(G\)</span>) than not splitting</strong></p></li>
            <li><p>The most significant part of the time cost per example is <strong>recomputing <span class="math inline">\(G\)</span>.</strong></p></li>
            </ul>
            <p>It is <strong>inefficient to recompute <span class="math inline">\(G\)</span> for every new example</strong>, because it is unlikely that the decision to split will be made at that specific point, we have to accumulate some points before appreciating the modification. Thus, VFDT (Very Fast Decision Tree) learner <strong>allows the user to specify a minimum number of new examples</strong> <span class="math inline">\(n_{min}\)</span> that must be accumulated at a leaf before <span class="math inline">\(G\)</span> is recomputed. We recompute <span class="math inline">\(G\)</span> after accumulating that minimum number of instances.</p>
            <h4 class="unnumbered" data-number="" id="algorithm-7"><strong>Algorithm</strong></h4>
            <p>Calculate the information gain for the attributes and determines the best two attributes</p>
            <p><strong>Pre pruning</strong>: consider a “null” attribute that consists of not splitting the node</p>
            <p>At each node, check for the condition:</p>
            <p><span class="math display">\[
                \large{\Delta \bar{G} = \bar{G}(X_A) - \bar{G}(X_B) &gt; \varepsilon}
            \]</span></p>
            <p>If condition is satisfied, <strong>create child nodes based on the test at the node</strong></p>
            <p>If not, stream in more examples and perform calculations till condition is satisfied</p>
            <p>We accumulate instances in the leaf and we have to decide if they can classify correctly instances or we need to split them, to decide if we have to split we have to decide when we accumulate a sufficient number of instances to consider that a reliable decision.</p>
            <p>The Hoeffding Bound tells us when we accumulate a sufficient number of instances to take that decision.</p>
            <p><img src="../media/image733.png" /></p>
            <p><img src="../media/image734.png" /></p>
            <p>We update the decision tree using each instance and just update the label of the leaves.</p>
            <p><img src="../media/image735.png" /></p>
            <p>We select <span class="math inline">\(X_a\)</span> as the attribute to split.</p>
            <p>We set to 0 because we start again to recompute statistics.</p>
            <h4 class="unnumbered" data-number="" id="performance-analysis"><strong>Performance Analysis</strong></h4>
            <p><span class="math inline">\(p\)</span> : probability that an example passed through DT to level <span class="math inline">\(i\)</span> will fall into a leaf at that point</p>
            <p>The expected disagreement between the tree produced by Hoeffding tree algorithm and that produced using infinite examples at each node is <strong>no greater than</strong> <span class="math inline">\(\delta/p\)</span>, for the Hoeffding bound.</p>
            <p><strong>Required memory</strong>: <span class="math inline">\(O(leaves * attributes * values * classes)\)</span></p>
            <h3 data-number="13.6.2" id="cvfdt-concept-adapting-very-fast-decision-tree-learner"><span class="header-section-number">13.6.2</span> <strong>CVFDT</strong> (Concept adapting Very Fast Decision Tree learner)</h3>
            <p>It <strong>extends VFDT but maintain VFDT’s speed and accuracy also dealing with the concept drift</strong>, not managed by VFDT because we decide to split a node if the number of instances guarantees that the decision we are taking is a good approximation of the one using all instances but nothing talks about concept drift, which is managed but alternative subtrees created when we receive instances.</p>
            <p>It <strong>detects and responds to changes</strong> in the example generating process</p>
            <h4 class="unnumbered" data-number="" id="observations"><strong>Observations</strong></h4>
            <ul>
            <li><p>With a <strong>time changing concept</strong>, the current splitting attribute of some nodes <strong>may not be the best anymore</strong>.</p></li>
            <li><p>An outdated subtree <strong>may still be better than the best single leaf</strong>, particularly if it is near the root.</p>
            <ul>
            <li>Grow an alternative subtree with the new best attribute at its root, when the old attribute seems out of date.</li>
            </ul></li>
            <li><p>Periodically <strong>use a bunch of samples to evaluate qualities of trees</strong>.</p>
            <ul>
            <li>Replace the old subtree when the alternate one becomes more accurate.</li>
            </ul></li>
            </ul>
            <h1 data-number="14" id="mapreduce-and-hadoop"><span class="header-section-number">14</span> MapReduce and Hadoop</h1>
            <p>With large datasets we need to store data in different servers.</p>
            <p>We have these frameworks that can help us to do it.</p>
            <h2 data-number="14.1" id="mapreduce"><span class="header-section-number">14.1</span> MapReduce</h2>
            <p><em>MapReduce</em> is a programming model Google has used to process big data.</p>
            <p>There’s the problem to distribute the filesystem and to perform distributed computations.</p>
            <p>In this paradigm:</p>
            <ul>
            <li><p><strong>Users specify the computation</strong> in terms of a <strong>map</strong> and a <strong>reduce</strong> function, mappers and reducers are executed <strong>in parallel</strong>.</p></li>
            <li><p><strong>Underlying runtime system</strong> automatically <strong>parallelizes the computation</strong> across large-scale clusters of machines, and</p></li>
            <li><p>Underlying system also <strong>handles machine failures</strong>, <strong>efficient communications</strong>, and <strong>performance issues</strong>.</p></li>
            </ul>
            <p>Consider a large data collection:</p>
            <p>{web, weed, green, sun, moon, land, part, web, green,…}</p>
            <p>Problem: Count the occurrences of the different words in the collection.</p>
            <p>The <em>parse()</em> analyzes the collection and the <em>count()</em> counts the occurrences of words in the collection.</p>
            <table>
            <tbody>
            <tr class="odd">
            <td><img src="../media/image736.png" /></td>
            <td><img src="../media/image737.png" /></td>
            <td><img src="../media/image738.png" /></td>
            </tr>
            </tbody>
            </table>
            <p>We can think they work in sequence, but we can speed-up the process with a <strong>multi-thread solution</strong>.</p>
            <p>If we just maintain a unique result table, we can write only in a lock state.</p>
            <p>To solve this, we can create <strong>multiple parser and counters</strong>, each one having a <strong>private table</strong>. We don’t have need for lock and at the end we will <strong>merge the tables</strong>.</p>
            <p>If we have a very large data collection, we have that this data cannot be maintained in one single machine and we cannot exploit multi-threading. To solve this problem, each machine will elaborate its own data collection private to the single machie.</p>
            <p>The file system has to be fault-tolerant, Hard disks can fail.</p>
            <p>We exploit as solution the replication on several servers and using <strong>checksum</strong>.</p>
            <p>If we have a “fire” in close servers, we may lose the data if replicas are all in that “building”. So it’s important that the replicas are in different places.</p>
            <p>This introduces other problems: <strong>using replicas in different locations brings us to deal with synchronization</strong>, but data transfer bandwidth is critical (location of data).</p>
            <p><strong>Critical aspects</strong>: fault tolerance + replication + load balancing, monitoring</p>
            <p>We have to take care of load balancing because we have also computation on different servers but we need a balance on this computations, considering fast/slow tasks in fast/slow servers.</p>
            <p>We want to <strong>exploit parallelism</strong> afforded by spitting parsing and counting.</p>
            <p><strong>We need an automatic solution</strong>.</p>
            <p>In every server we will have a data collection, a parser and a counter.</p>
            <p>Each server manages the private data collection, and we have to synchronize the tables to produce the final result.</p>
            <p>The real advantage is that data are characterized with the <strong>Write Once Read Many</strong> (<em>WORM</em>) characteristics.</p>
            <p>This is the case, because we receive data collections, and we just have to read them.</p>
            <p>Data with WORM characteristics <strong>yields to parallel processing</strong>.</p>
            <p><strong>Write-once-read-many</strong>: a file once created, written and closed need not be changed –this assumption simplifies coherency</p>
            <p>If we change the content of the file, we need to handle the consistency updating replicas.</p>
            <p>This file system works very well if we have one write and multiple reads.</p>
            <p>Data without dependencies <strong>yields to out of order processing</strong> because we can think to process concurrently and to have only in some phases some synchronization.</p>
            <h3 data-number="14.1.1" id="divide-and-conquer-1"><span class="header-section-number">14.1.1</span> Divide and Conquer</h3>
            <p>Parse and count can work in parallel.</p>
            <p>Our <strong>parse</strong> is a mapping operation:</p>
            <p><strong>MAP</strong>: input <span class="math inline">\(\rightarrow\)</span> &lt;key, value&gt; pairs with value equal to 1</p>
            <p>Our <strong>count</strong> is a reduce operation:</p>
            <p><strong>REDUCE</strong> receives as input &lt;key, value&gt; pairs and <strong>reduce it increasing the counter for a key</strong> producing in output &lt;key,value&gt; cumulative.</p>
            <p>Runtime adds distribution + fault tolerance + replication + monitoring + load balancing to your base application</p>
            <p><img src="../media/image739.png" /></p>
            <p>In MAP we <strong>split the data to supply multiple processors</strong>, each map <strong>reads the split of the data collection</strong> and <strong>analyzes if the word is present</strong> and output &lt;word, 1&gt;.</p>
            <p><img src="../media/image740.png" /></p>
            <p>It goes in input to the reducers that increase the counter.</p>
            <p><em>combine</em> combines results of executions in the same server</p>
            <h3 data-number="14.1.2" id="programming-model"><span class="header-section-number">14.1.2</span> <strong>Programming Model</strong></h3>
            <p>Considering the MapReduce Programming Model, we have to:</p>
            <ul>
            <li><p><strong>determine if the problem is parallelizable</strong> and solvable using MapReduce (ex: Is the data WORM?, large data set).</p></li>
            <li><p><strong>design and implement solutions</strong> as Mapper classes and Reducer class.</p></li>
            <li><p><strong>compile</strong> the source code with Hadoopcore.</p></li>
            <li><p><strong>package</strong> the code as jar executable.</p></li>
            <li><p><strong>configure the application</strong> (job) to the number of mappers and reducers (tasks), input and output streams</p></li>
            <li><p><strong>load the data</strong> (or use it on previously available data)</p></li>
            </ul>
            <p>Mappers can be executed in parallel, reducers can be executed in parallel and <strong>the only synchronization is between mappers and reducers</strong>.</p>
            <p>Map and Reduce are the main operations: <strong>simple code</strong>.</p>
            <p>There are other supporting operations such as combine and partition.</p>
            <p>All the maps should be completed before the reduce operations start.</p>
            <p>Operations are computed where we have the data.</p>
            <p>We have special distributed file system.</p>
            <p>For example: HadoopDistributed File System and HadoopRuntime.</p>
            <p><strong>Hadoop is the framework and MapReduce is the paradigm</strong>.</p>
            <p>It is <strong>scalable</strong>, <strong>flexible</strong> in accepting all data formats, <strong>efficient</strong> for fault-tolerance and <strong>can use commodity inexpensive hardware</strong>.</p>
            <p>Automatic parallelization &amp; distribution and fault-tolerance &amp; automatic recovery are hidden from the end user that just provides two functions.</p>
            <h2 data-number="14.2" id="hadoop"><span class="header-section-number">14.2</span> <strong>Hadoop</strong></h2>
            <p><strong>Hadoop</strong> is a software framework for distributed processing of large datasetsacross large clusters of computers.</p>
            <ul>
            <li><p>Large datasets <span class="math inline">\(\rightarrow\)</span> Terabytes or petabytes of data</p></li>
            <li><p>Large clusters <span class="math inline">\(\rightarrow\)</span> Hundreds or thousands of nodes</p></li>
            </ul>
            <p>Hadoop is open-source implementation for Google MapReduce.</p>
            <p>Hadoop is based on a simple programming model called MapReduce.</p>
            <p>Hadoop is based on a simple data model, any data will fit.</p>
            <h3 data-number="14.2.1" id="hadoop-distributed-file-system"><span class="header-section-number">14.2.1</span> <strong>Hadoop Distributed File System</strong></h3>
            <p>It has a <strong>master-slave shared-nothing architecture</strong>, used for the distributed file systems and distributed processing.</p>
            <p><img src="../media/image741.png" /></p>
            <p>In the <strong>Master</strong> we have the <strong>filesystem</strong> and, in the <strong>slaves</strong>, we have the <strong>actual storage of data</strong>.</p>
            <p>In the slaves we have the <strong>allocation for tasks</strong>, the real executions of tasks.</p>
            <p>The master has a <strong>jobTracker</strong> and acts as a sort of synchronizer. It works well if slaves do not need to share anything.</p>
            <p><img src="../media/image742.png" /> Hadoop framework consists of two main layers</p>
            <ul>
            <li><p><strong>Distributed File System</strong> (HDFS)</p></li>
            <li><p><strong>Execution engine</strong> (MapReduce), for the execution of tasks</p></li>
            </ul>
            <p>The environment is the following:</p>
            <p><img src="../media/image743.png" /></p>
            <p>While in local file systems we manage blocks of 2K size, in HDFS we manage 128M blocks.</p>
            <p>This allows us to have a <strong>limited blockmap</strong>, we reduce the number of blocks to manage in the master node that contains mapping of each block with the place in which it is stored.</p>
            <p>Each block is stored in different slaves.</p>
            <p>HDFS is a distributed file system <strong>optimized for high throughput</strong>.</p>
            <p>It leverages unusually large (for a file system) block sizes and use data locality optimizations to reduce network input/output (I/O).</p>
            <p>Works very well when reading and writing large files (gigabytes and larger), because we mode large blocks.</p>
            <p>It is <strong>scalable</strong> and <strong>reliable</strong>.</p>
            <p>It replicates files for a configured number of times, it is tolerant of both software and hardware failures and it automatically re-replicates data blocks on nodes that have failed.</p>
            <p><strong>Failure is the norm rather than exception</strong>.</p>
            <p>A HDFS instance may consist of thousands of server machines, each storing part of the file system’s data.</p>
            <p>Since we have huge number of components and that each component has non-trivial probability of failure means that <strong>there is always some component that is non-functional</strong>.</p>
            <p><strong>Detection of faults</strong> and <strong>quick, automatic recovery</strong> from them is a core architectural goal of HDFS.</p>
            <h4 class="unnumbered" data-number="" id="masterslave-architecture"><strong>Master/slave architecture</strong></h4>
            <p>HDFS cluster consists of a <strong>single Namenode</strong> (master), a master server that <strong>manages the file system namespace</strong> and <strong>regulates access to files by clients</strong>.</p>
            <p>There are <strong>multiple DataNodes</strong> (slaves) usually <strong>one per node</strong> in a cluster. The DataNodes <strong>manage storage</strong> attached to the nodes that they run on.</p>
            <p>HDFS exposes a file system namespace and allows user data to be stored in files.</p>
            <p>A file is <strong>split into one or more blocks</strong> and <strong>set of blocks are stored in DataNodes</strong>.</p>
            <p>DataNodes: <strong>serves read</strong>, <strong>write requests</strong>, <strong>performs block creation, deletion, and replication</strong> upon instruction from Namenode. Real operations are done in DataNodes but <strong>commands come from Namenodes</strong>.</p>
            <h4 class="unnumbered" data-number="" id="hdfs-architecture"><strong>HDFS Architecture</strong></h4>
            <p><img src="../media/image744.png" /></p>
            <p>For each file, we have the <strong>list of blocks</strong> and where they are stored.</p>
            <p>The client <strong>interacts directly with the NameNode</strong>, receives where blocks are present and interacts with DataNodes to retrieve the block.</p>
            <p>When we manage the transfer from DataNodes we don’t need the interaction with the NameNode.</p>
            <p>By default, the number of identical copies of each block is 3 but we can increment it if we want to increase the fault tolerance.</p>
            <p>We have an <strong>hierarchical file system</strong> with directories and files.</p>
            <p>The Namenode <strong>maintains the file system</strong>.</p>
            <p>Any meta information changes to the file system recorded by the Namenode.</p>
            <p>An application <strong>can specify the number of replicas of the file needed</strong>: replication factor of the file. This information is stored in the Namenode.</p>
            <p>HDFS is designed to store very large files across machines in a large cluster.</p>
            <p>Each file is a <strong>sequence of blocks</strong>. All blocks in the file except the last (depending on the size of the file) are of the <strong>same size</strong>. Blocks are replicated for fault tolerance. Block size and replicas are <strong>configurable per file</strong>.</p>
            <p>The Namenode receives a <strong>Heartbeat</strong> and a <strong>BlockReport</strong> from each DataNode in the cluster. BlockReport contains <strong>all the blocks on a DataNode</strong>.</p>
            <p>The <strong>placement of the replicas</strong> is <strong>critical</strong> to HDFS reliability and performance. If a node fails, we will have to find the replica from another node and give it to another new node. <strong>Optimizing replica placement</strong> distinguishes HDFS from other distributed file systems.</p>
            <p><strong>Rack-aware replica placement</strong>:</p>
            <p>The goal is to improve reliability, availability and network bandwidth utilization.</p>
            <p>Where to store replicas?</p>
            <p>Having them in one server is easy and fast to maintain, but we have a <strong>Single Point Of Failure</strong>. Typically we have replicas in <strong>different servers in the same rack</strong> and other replicas in other locations. We also have many racks, communication between racks is <strong>through switches</strong>.</p>
            <p>If something happens in the same location we lose a lot of replicas.</p>
            <p>Network bandwidth between machines on the same rack is greater than those in different racks.</p>
            <p>The Namenode determines the rack id for each DataNode.</p>
            <p>Replicas are typically placed on unique racks. This is <strong>simple but non-optimal</strong>: writes are expensive and replication factor is 3.</p>
            <p>Replicas are placed: <strong>one</strong> on a node in a <strong>local rack</strong>, <strong>one</strong> on a <strong>different node in the local rack</strong> and <strong>one</strong> on a node in a <strong>different rack</strong> (possibly in a different location). 1/3 of the replica on a node, 2/3 on a rack and 1/3 distributed evenly across remaining racks.</p>
            <p><strong>Replica selection for READ operation</strong>: HDFS tries to minimize the bandwidth consumption and latency. When we perform a read operation, we exploit the closer block. We execute the code in the server which has the <strong>closest block</strong>, we don’t need to move data. If there is a replica on the Reader node then that is preferred.</p>
            <p>HDFS cluster may <strong>span multiple data centers</strong>: replica in the local data center is preferred over the remote one.</p>
            <h4 class="unnumbered" data-number="" id="namenode"><strong>NameNode</strong></h4>
            <p>The HDFS namespace is stored by Namenode.</p>
            <p>Namenode uses a <strong>transaction log</strong> called the <strong>EditLog</strong> to record every change that occurs to the filesystem meta data. EditLog is <strong>stored in the Namenode’s local filesystem</strong>.</p>
            <p>Furthermore, the entire filesystem namespace including mapping of blocks to files and file system properties is stored in a file <strong>FsImage</strong> (File System Image), stored in Namenode’s local filesystem.</p>
            <p>The Namenode <strong>keeps the image of the entire file system namespace</strong> and <strong>file Blockmap in memory</strong>. 4GB of local RAM is sufficient to support the above data structures that represent the huge number of files and directories.</p>
            <p>We use large blocksize to reduce the occupation in the main memory for the NameNode.</p>
            <p>When the Namenode starts up it <strong>gets the FsImage and Editlog</strong> from its local file system, <strong>update FsImage with EditLog Information</strong> and then stores a copy of the FsImage on the filesystem as a <strong>checkpoint</strong>.</p>
            <p>Periodic checkpointing is done, so that the system can recover back to the last checkpointed state in case of a crash. Periodically <strong>FsImage is updated</strong> using the EditLog.</p>
            <p>Primary objective of HDFS is to store data reliably in the presence of failures.</p>
            <h4 class="unnumbered" data-number="" id="failures"><strong>Failures</strong></h4>
            <p>Three common failures are: <strong>Namenode failure</strong>, <strong>Datanode failure</strong> and <strong>network partition</strong>.</p>
            <p>A <strong>network partition</strong> can cause a subset of Datanodes to lose connectivity with the Namenode. Namenode detects this condition by the <strong>absence of a Heartbeat message</strong>, sent from slaves to the master.</p>
            <p>Namenode marks Datanodes without Hearbeat and <strong>does not send any I/O requests to them</strong>. Any data registered to the failed Datanode is not available to the HDFS. Also, the death of a Datanode may cause replication factor of some of the blocks to <strong>fall below their specified value</strong>.</p>
            <p><strong>Metadata Disk Failure</strong></p>
            <p>FsImage and EditLog are central data structures of HDFS. A corruption of these files can cause a HDFS instance to be non-functional.</p>
            <p>For this reason, a Namenode can be configured to <strong>maintain multiple copies of the FsImage and EditLog</strong>. Multiple copies of the FsImage and EditLog files are <strong>updated synchronously</strong>.</p>
            <p><strong>Meta-data is not data-intensive</strong>. The Namenode could be single point failure: automatic failover is NOT supported!</p>
            <p>HDFS support write-once-read-many with reads at streaming speeds.</p>
            <p>A typical block size is 64MB, a file is chopped into 64MB chunks and stored. We move from datanodes to client and to namenode, blocks of 64MB.</p>
            <h4 class="unnumbered" data-number="" id="staging"><strong>Staging</strong></h4>
            <p>A client request to create a file <strong>does not reach Namenode immediately</strong>.</p>
            <p>HDFS client caches the data into a temporary file. When the data reached a HDFS block size the client contacts the Namenode.</p>
            <p>Namenode <strong>inserts the filename into its hierarchy</strong> and <strong>allocates a data block for it</strong>.</p>
            <p>The Namenode responds to the client with <strong>the identity of the Datanode</strong> and the <strong>destination of the replicas</strong> (Datanodes) for the block.</p>
            <p>Then the client flushes it <strong>from its local memory directly to the Datanode</strong>.</p>
            <p>The client sends a message that the file is closed.</p>
            <p>Namenode proceeds to <strong>commit the file</strong> for creation operation into the persistent store.</p>
            <p>If the Namenode dies before file is closed, <strong>the file is lost</strong>, we don’t have this information in the EditLog.</p>
            <p>This client side caching is required to avoid network congestion, because it’s useless just to open the communication and send few bytes, we use the caching and when we receive the size of a block we start to send it.</p>
            <h4 class="unnumbered" data-number="" id="replication-pipelining"><strong>Replication Pipelining</strong></h4>
            <p>When the client receives response from Namenode, it flushes its block in small pieces (4K) to the first replica, that in turn copies it to the next replica and so on.</p>
            <p>Thus data is pipelined from DataNode to the next.</p>
            <p><img src="../media/image745.png" /></p>
            <p>The client communicates with the namenode for the creation of the file.</p>
            <p>The namenode allocate the file and block names and return the datanode in which it will be stored.</p>
            <h4 class="unnumbered" data-number="" id="application-programming-interface-api"><strong>Application Programming Interface</strong> (API)</h4>
            <p>HDFS provides Java API for application to use, and a http browser can be used to browse the files of a HDFS instance.</p>
            <h4 class="unnumbered" data-number="" id="fs-shell-admin-and-browser-interface"><strong>FS Shell, Admin and Browser Interface</strong></h4>
            <p>HDFS organizes its data in files and directories.</p>
            <p>It provides a command line interface called the <strong>FS shell</strong> that lets the user interact with data in the HDFS.</p>
            <p>The <strong>syntax</strong> of the commands is <strong>similar to bash and csh</strong>.</p>
            <blockquote>
            <p>Example: to create a directory /foodir</p>
            <p>/bin/hadoop dfs –mkdir /foodir</p>
            </blockquote>
            <p>There is also <strong>DFSAdmin interface</strong> available</p>
            <h4 class="unnumbered" data-number="" id="space-reclamation"><strong>Space Reclamation</strong></h4>
            <p>When a file is deleted by a client, HDFS renames file to a file in be the /trash directory for a configurable amount of time.</p>
            <p>A client <strong>can request for an undelete in this allowed time</strong>.</p>
            <p>After the specified time the file is deleted and the space reclaimed.</p>
            <p>When the replication factor is reduced, the Namenode selects excess replicas that can be deleted.</p>
            <p>Next heartbeat(?) transfers this information to the Datanode that clears the blocks for use.</p>
            <h3 data-number="14.2.2" id="core-hadoop-components-mapreduce"><span class="header-section-number">14.2.2</span> <strong>Core Hadoop Components: MapReduce</strong></h3>
            <p>In MapReduce we have mappers and reducers, and they work very well when we have a batch-based, distributed computing framework.</p>
            <p>It allows to parallelize work over a large amount of raw data.</p>
            <p>Simplifies parallel processing by abstracting away the complexities involved in working with distributed systems (computational parallelization, work distribution, unreliable software, and hardware, etc.)</p>
            <p>It decomposes work submitted by a client into small, parallelized map and reduce workers.</p>
            <p>It uses a <strong>shared-nothing model</strong>: remove any parallel execution interdependencies that could add unwanted synchronization points or state sharing. They could limit our parallel execution.</p>
            <p><img src="../media/image746.png" /></p>
            <p>The MapReduce master <strong>divides the job in parts</strong>, some for mappers and some for reducers. They schedule them for remote execution on the slave node.</p>
            <p>The role of the programmer is to <strong>define map</strong> and <strong>reduce</strong> functions.</p>
            <p><strong>Map functions</strong>: output key/value tuples</p>
            <p><strong>Reduce functions</strong>: process the key/value tuples to produce the final output</p>
            <p><img src="../media/image747.png" /></p>
            <p>It <strong>elaborates the original file/table</strong> and <strong>process key/value pairs</strong>.</p>
            <p>The reducer <strong>elaborates this list of key/value pairs</strong> and <strong>produces a list of other key/value pairs</strong>.</p>
            <p>The power of MapReduce occurs in <strong>between the map output and the reduce input</strong>, in the <strong>shuffle and sort phases</strong>.</p>
            <p><img src="../media/image748.png" /></p>
            <p>The <strong>shuffle+sort</strong> is managed using the map/reduce support and have the primary duties specified, determine the reducer for the output of the mappers and ensure that for a given reducer the input keys are sorted.</p>
            <p>The shuffle knows that the keyword cat is elaborated by <em>Reducer 1</em> and so all outputs are organized to provide this keyword as input to <em>Reducer 1</em>, also merging possible values for the specific key(sort phase).</p>
            <p><img src="../media/image749.png" /></p>
            <p>Shuffle and sort in MapReduce:</p>
            <p><img src="../media/image750.png" /></p>
            <p>Map tasks and reducers tasks are <strong>typically executed in different servers</strong>.</p>
            <p>We need to have <strong>similar mapper executions</strong>, otherwise we have to wait for the longest one.</p>
            <p><img src="../media/image751.png" /></p>
            <h3 data-number="14.2.3" id="physical-architecture"><span class="header-section-number">14.2.3</span> <strong>Physical Architecture</strong></h3>
            <p><strong>RAID is discouraged on the DataNodes</strong> because HDFS has already replication, but it is <strong>strongly recommended on the NameNode</strong>.</p>
            <p>From a network topology perspective with regards to switches and firewalls, <strong>all of the master and slave nodes must be able to open connections to each other</strong>.</p>
            <p>For <strong>small clusters</strong>, all the hosts would run 1 GB network cards connected to a single, good-quality switch.</p>
            <p>For <strong>larger clusters</strong> look at 10 GB top-of-rack switches that have at least multiple 1 GB uplinks to dual-central switches.</p>
            <p>We can use <strong>commodity hardware</strong> but <strong>fast communication network</strong>.</p>
            <h3 data-number="14.2.4" id="hadoop-limitations"><span class="header-section-number">14.2.4</span> <strong>Hadoop Limitations</strong></h3>
            <h4 class="unnumbered" data-number="" id="hdfs"><strong>HDFS</strong></h4>
            <p>Lack of high availability (High Availability requires shared storage for NameNode metadata, which may require expensive HA storage), inefficient handling of small files and lack of transparent compression.</p>
            <h4 class="unnumbered" data-number="" id="mapreduce-1"><strong>MapReduce</strong></h4>
            <p>Batch-based architecture –does not lend itself to use cases that need real-time data access</p>
            <h4 class="unnumbered" data-number="" id="security"><strong>Security</strong></h4>
            <p>Hadoop offers a security model, but by default it is disabled. Hadoop can be configured to run with <em>Kerberos</em> (a network authentication protocol, which requires Hadoop daemons to authenticate clients)</p>
            <h3 data-number="14.2.5" id="mapreduce-execution"><span class="header-section-number">14.2.5</span> <strong>MapReduce execution</strong></h3>
            <p>We need to have mappers and reducers working in parallel or we lose all our advantages we can have.</p>
            <p><strong>One or more chunks</strong> from a distributed file systems are given to a Map task.</p>
            <p>Map tasks turn the chunk into a <strong>sequence of key-value pairs</strong> (the way is determined by the code of the Map function).</p>
            <p>The key-value pairs from each Map task are <strong>collected by a master controller and sorted by key</strong>.</p>
            <p>The keys are <strong>divided among all the Reduce tasks</strong>: all key-value pairs with the <strong>same key</strong> wind up at the <strong>same Reduce task</strong>.</p>
            <p>The Reduce tasks work on <strong>one key at a time</strong> and combine all the values associated with that key in the way defined by the code written in the Reduce function.</p>
            <p><img src="../media/image752.png" /></p>
            <h4 class="unnumbered" data-number="" id="map-tasks"><strong>Map Tasks</strong></h4>
            <p>The input files for a Map task consists of <strong>elements</strong> (a tuple or a document) <strong>gathered in chunks</strong>. No element is stored across two chunks.</p>
            <p>The map function takes an input element as its argument and produces zero or more key-value pairs.</p>
            <p><strong>Keys do not have to be unique</strong>: rather Map task can produce several key-value pairs with the same key.</p>
            <p>Let’s see the map task.</p>
            <p>Example: counting the number of occurrences for each word in a collection of documents</p>
            <p>Input file: repository of documents and each document is an element</p>
            <p>Key: strings</p>
            <p>Value: integer</p>
            <p>The Map Task reads the document and breaks it into its sequence of words <span class="math inline">\(w_1, w_2, ..., w_n\)</span>.</p>
            <p>The output of the Map task is:</p>
            <p><span class="math display">\[
                \large{(w_1, 1), (w_2, 1), ..., (w_n, 1)}
            \]</span></p>
            <p>We will have a pair <strong>(word, 1)</strong>.</p>
            <p>Note that a <strong>single Map</strong> task will typically <strong>process many documents</strong> –all the documents in one or more chunks. Thus, its output will be more than the sequence for the one document suggested above.</p>
            <p>The key-value pairs are <strong>grouped by key</strong> and the values associated with each key are formed into a list of values.</p>
            <p>Master controller process knows how many reduce tasks there will be (say <span class="math inline">\(r\)</span>), picks a hash function that applies to keys and produces a bucket number from <span class="math inline">\(0\)</span> to <span class="math inline">\(r-1\)</span>.</p>
            <p>Each key-value pair output from a Map task is put in one of the <span class="math inline">\(r\)</span> local files. Each file is destined for one of the Reduce tasks.</p>
            <p>Merges the files from each Map task and feeds the merged file to that process as a sequence of key-list-of-value pairs <span class="math inline">\((k, [v_1, v_2, ..., v_n])\)</span></p>
            <p>where <span class="math inline">\((k, v_1), (k, v_2), ..., (k, v_n)\)</span> are all the key-value pairs with key <span class="math inline">\(k\)</span> coming from all the Map tasks.</p>
            <p>We have as output the key and the set of values corresponding to that key.</p>
            <h4 class="unnumbered" data-number="" id="reduce-task"><strong>Reduce Task</strong></h4>
            <p>Considering the Reduce task, we have:</p>
            <p><strong>Input</strong>: key and its list of associated values</p>
            <p><strong>Output</strong>: sequence of zero or more key-value pairs</p>
            <p>The Reduce task elaborates the keys and their lists of associated values generated by the grouping process</p>
            <p>Example word count: The Reduce function <strong>simply adds up all the values</strong>.</p>
            <p>The <strong>output</strong> of a reducer consists of the <strong>word and the sum</strong>.</p>
            <p>The output of all the Reduce tasks is a sequence of <span class="math inline">\((w,m)\)</span> pairs, where <span class="math inline">\(w\)</span> is a word that appears at least once among all those documents and <span class="math inline">\(m\)</span> is the total number of occurrences of <span class="math inline">\(w\)</span> among all those documents.</p>
            <p>We have a stage of combiners in which we merge the output of the single words to count how many occurrences we have.</p>
            <h4 class="unnumbered" data-number="" id="combiners"><strong>Combiners</strong></h4>
            <p>Sometimes, a Reduce function is <strong>associative and commutative</strong>.</p>
            <p>The reduce function can be applied within the Map tasks.</p>
            <p>Instead of producing <span class="math inline">\((w,1) (w,1) ...\)</span> the Map task could produce <span class="math inline">\((w,m)\)</span></p>
            <p>Anyway, it is <strong>still necessary to do grouping and aggregation and to pass the result to the Reduce tasks</strong></p>
            <p>We have different chunks of the document for each mapper, we produce in output the words with the counter, we combine values considering the same word and with the partitioner and shuffle &amp; sort, the single key is in input to the single reducer.</p>
            <p><img src="../media/image756.png" /></p>
            <p>The combiner helps us to <strong>reduce the communication</strong>, we combine output with the same key to count the number of occurrences so we can transmit to the reducer the key but already with a combination of values.</p>
            <p>The communication happens when we deal with reducers.</p>
            <p>When we implement this application we want the maximum parallelism, we have to reduce each possible interaction between reducer and mappers.</p>
            <p>Reduce task to execute each reducer, i.e., a single key and its associated value list.</p>
            <p>Reduce task at a different compute node.</p>
            <p><strong>Problems</strong>:</p>
            <ul>
            <li><p><strong>Overhead</strong> with each task we create</p></li>
            <li><p><strong>More keys than</strong> there are <strong>compute nodes available</strong></p></li>
            <li><p>We can have <strong>significant variation in the lengths of the value lists</strong> for different keys (skew –a significant difference in the amount of time each takes)</p></li>
            </ul>
            <p>We should be careful to balance the workload for each node, otherwise we have to synchronize and wait for the longest execution of the task.</p>
            <h3 data-number="14.2.6" id="parallel-k-means"><span class="header-section-number">14.2.6</span> <strong>Parallel K-Means</strong></h3>
            <h4 class="unnumbered" data-number="" id="standard-k-means"><strong>Standard K-Means</strong></h4>
            <ol type="1">
            <li><p>The algorithm arbitrarily selects k points as the initial cluster centers (“means”).</p></li>
            <li><p>Each point in the dataset is assigned to the closed cluster, based upon the Euclidean distance between each point and each cluster center.</p></li>
            <li><p>Each cluster center is recomputed as the average of the points in that cluster.</p></li>
            <li><p>Steps <strong>2</strong> and <strong>3</strong> repeat until the clusters converge.</p></li>
            </ol>
            <p>Convergence may be defined differently depending upon the implementation, but it normally means that either no observations change clusters when steps 2 and 3 are repeated or that the changes do not make a material difference in the definition of the clusters.</p>
            <p>Each node will have a chunk and we have to think at the execution in terms of map/reduce function being careful to the parallelism.</p>
            <p>The most intensive calculation to occur is the calculation of distances. In each iteration, it would require a total of <span class="math inline">\((nk)\)</span> distance computations where <span class="math inline">\(n\)</span> is the number of objects and <span class="math inline">\(k\)</span> is the number of clusters being created. Distance computations between different objects with centers can be concurrently executed. We work in the chunk, computing the distance in parallel, but we need the centers in the previous iteration.</p>
            <p><strong>Map function</strong>: performs the procedure of assigning each sample to the closest center</p>
            <p><strong>Reduce function</strong>: the reduce function performs the procedure of updating the new centers, exploiting what it knows from the Map function</p>
            <h4 class="unnumbered" data-number="" id="map-function"><strong>Map function</strong></h4>
            <p><strong>Input dataset</strong>: sequence file of &lt;key, value&gt; pairs, each of which represents a record in the dataset.</p>
            <p>The <strong>key</strong> is the <strong>offset in bytes of this record to the start point of the data file</strong>, and the value is a string of the content of this record (features of the object).</p>
            <p>The dataset is <strong>split and globally broadcast to all mappers</strong>. Consequently, the distance computations are concurrently executed.</p>
            <p>For each map task, <em>PKMeans</em> constructs a global variant centers which is an array containing the information about centers of the clusters.</p>
            <p>Given the information, a mapper can <strong>compute the closest center point for each sample</strong>.</p>
            <p>The intermediate values are then composed of two parts: the <strong>index</strong> of the closest center point and the <strong>sample information</strong>.</p>
            <p><img src="../media/image757.png" /></p>
            <p>The global variable centers, offset key and sample value are given in input.</p>
            <p>In output we have &lt;cluster, information relating to the single object belonging to this cluster&gt;.</p>
            <p><span class="math inline">\(key&#39; =\)</span> <strong>index of ther closest cluster</strong></p>
            <p><span class="math inline">\(value&#39; =\)</span> <strong>list of values corresponding to each feature of the object</strong>.</p>
            <h4 class="unnumbered" data-number="" id="combine-function"><strong>Combine function</strong></h4>
            <p>After each map task, we apply a combiner to combine the intermediate data of the same map task.</p>
            <p>The function combines the intermediate data stored in local disk of the host.</p>
            <p>The update of the center is obtained <strong>averaging the points of a cluster</strong>.</p>
            <p>We <strong>sum the values</strong> of the points assigned to the same cluster and <strong>their number</strong>.</p>
            <p>It takes the output of the map function and make this aggregation for all values belonging to the same cluster, before sending that information to the reducer.</p>
            <p>The procedure <strong>does not require any communication</strong>.</p>
            <p><img src="../media/image758.png" /></p>
            <p>Feature by feature we sum all values of objs belonging to the same cluster and we transmit also the number of objects we consider.</p>
            <p>In the reducer we aggregate this information summing all <span class="math inline">\(V_i\)</span> of the same cluster and dividing for the #objs.</p>
            <h4 class="unnumbered" data-number="" id="reduce-function"><strong>Reduce Function</strong></h4>
            <p>The input of the reduce function is the data obtained from the combine function of each host.</p>
            <p>The data includes partial sum of the samples in the same cluster and the sample number.</p>
            <p>The reduce function:</p>
            <ul>
            <li><p><strong>sums all the samples</strong> and <strong>computes the total number of samples assigned to the same cluster</strong></p></li>
            <li><p><strong>computes the new centers</strong> which are used for the next iteration, dividing for the number of objects in the cluster</p></li>
            </ul>
            <p><img src="../media/image759.png" /></p>
            <p>The output is the <strong>list of centers of the clusters</strong>.</p>
            <h4 class="unnumbered" data-number="" id="performance-evaluation"><strong>Performance Evaluation</strong></h4>
            <p>All mappers can be executed in parallel because we just compute the distance of the objects in the chunks and centers.</p>
            <p>We can execute combiners in parallel, after waiting for the end of the mappers.</p>
            <p>Reducers can be executed in parallel because related to different clusters.</p>
            <p>The <strong>real effort is in the computation of the distance of objs and centers</strong>, mappers spend a lot of time while reducers are very fast.</p>
            <p>We have the <strong>same results as a sequential k-means</strong>.</p>
            <p>But we have <strong>advantages</strong> in terms of <strong>computational time</strong>.</p>
            <p><strong>Speedup</strong>: measures how much a parallel algorithm is faster than a corresponding sequential algorithm.</p>
            <p><img src="../media/image760.png" /></p>
            <p>With the increase of the number of nodes we are able to increase the speed-up, obtaining a <strong>faster execution</strong> than the sequential one.</p>
            <p>We expect theoretically to have a linear increase, but we have <strong>overhead due to the transmission of data, activation of mappers/reducers, reallocation of data structures</strong>.</p>
            <p>But we can appreciate to be close to the optimal solution, with 8GB of datasets especially.</p>
            <p>When coping with large datasets we have a good improvement.</p>
            <p>If the dataset is small the overhead weights more in terms of speed-up, because we need a short time to process the dataset and the overhead affects a lot the performance.</p>
            <p>Another metric is the <strong>Scaleup</strong>: holds the number of computers in the system constant and grows the size of the datasets by the factor <span class="math inline">\(m\)</span>. Sizeup measures <strong>how much longer it takes on a given system, when the dataset size is <span class="math inline">\(m\)</span>-times larger than the original dataset</strong>.</p>
            <p><img src="../media/image761.png" /></p>
            <p>We have a reduction in time when we increase the size of the dataset, and we have available a higher number of nodes.</p>
            <h3 data-number="14.2.7" id="parallel-fp-growth-pfp"><span class="header-section-number">14.2.7</span> <strong>Parallel FP-Growth</strong> (PFP)</h3>
            <p>It shards a large-scale mining task into independent, parallel tasks. PFP then <strong>uses MapReduce to take advantage of its recovery model</strong>. Empirical study shows that PFP achieves <strong>near-linear speedup</strong>.</p>
            <p>FP-Growth first <strong>computes a list of frequent items sorted by frequency in descending order</strong> (<strong>F-List</strong>) during its first database scan.</p>
            <p>In its second scan, the database is <strong>compressed into a FP-tree</strong>. Then FP-Growth starts to <strong>mine the FP-tree for each item whose support is larger than minsup</strong> by recursively building its conditional FP-tree.</p>
            <p>The algorithm performs <strong>mining recursively on FP-tree</strong>.</p>
            <p>The problem of finding frequent itemsets is converted to <strong>constructing and searching trees recursively</strong>.</p>
            <p>We use map/reduce tasks in several steps.</p>
            <p><img src="../media/image762.png" /></p>
            <p><img src="../media/image763.png" /></p>
            <h4 class="unnumbered" data-number="" id="parrallel-counting"><strong>Parrallel Counting</strong></h4>
            <ol type="1">
            <li><p><strong>Sharding</strong>:</p>
            <p>Dividing DB into successive parts and storing the parts on P different computers. Each part is called a shard.</p></li>
            <li><p><strong>Parallel Counting</strong>:</p>
            <p>Doing a MapReduce pass to count the support values of all items that appear in DB. Each mapper inputs one shard of DB. This step implicitly discovers the items’ vocabulary I, which is usually unknown for a huge DB.</p>
            <p>The result is stored in F-list.</p>
            <p><img src="../media/image764.png" /></p></li>
            <li><p><strong>Grouping Items</strong>:</p>
            <p>Dividing all the |I| items on F-list into Q groups. The list of groups is called group list (G-list), where each group is given a unique group-id (gid). As F-list and G-list are both small and the time complexity is <span class="math inline">\(O(|I|)\)</span>, this step can complete on a single computer in few seconds.</p></li>
            <li><p><strong>Parallel FP-Growth</strong>:</p>
            <ul>
            <li><p><strong>Mapper</strong> – Generating group-dependent transactions: Each mapper instance is fed with a shard of DB generated in Step 1. Before it processes transactions in the shard one by one, it reads the G-list.</p>
            <p>With the mapper algorithm, it outputs one or more key-value pairs, where each key is a group-id and its corresponding value is a generated group-dependent transaction.</p>
            <p><img src="../media/image765.png" /></p></li>
            <li><p><strong>Reducer</strong> – FP-Growth on group-dependent shards: When all mapper instances have finished their work, for each group-id, the MapReduce infrastructure automatically groups all corresponding group-dependent transactions into a shard of group-dependent transactions.</p>
            <p>Each reducer instance is assigned to process one or more group-dependent shard one by one. For each shard, the reducer instance builds a local FP-tree and growth its conditional FP-trees recursively. During the recursive process, it may output discovered patterns.</p>
            <p><img src="../media/image766.png" /></p></li>
            </ul></li>
            <li><p><strong>Aggregating</strong></p>
            <p><img src="../media/image767.png" /></p></li>
            </ol>
        </div>
        
            </div>

    <!-- mine -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</body>
</html>
